{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My learning notes","title":"Home"},{"location":"#my-learning-notes","text":"","title":"My learning notes"},{"location":"artificial-intelligence/","text":"Artificial Intelligence","title":"Artificial Intelligence"},{"location":"artificial-intelligence/#artificial-intelligence","text":"","title":"Artificial Intelligence"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/","text":"Statistical Learning Based on the youngest brother of The Elements of Statistical Learning (ESL) : An Introduction to Statistical Learning Statistical learning refers to a set of tools for modeling and understanding complex datasets . It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. Tools classification: supervised unsupervised Supervised statistical learning (predict an output variable) Involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. Unsupervised statistical learning In this case there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. It involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a clustering problem. Wage Data The Wage data involves predicting a continuous or quantitative output value. This is often referred to as a regression problem. Stock Market Data In certain cases we may instead wish to predict a non-numerical value\u2014that is, a categorical or qualitative output. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves\u2014for exmple\u2014predicting whether a given day\u2019s stock market performance will fall into the Up bucket or the Down bucket. This is known as a classification problem. Gene Expression Data Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.","title":"Introduction"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#statistical-learning","text":"Based on the youngest brother of The Elements of Statistical Learning (ESL) : An Introduction to Statistical Learning Statistical learning refers to a set of tools for modeling and understanding complex datasets . It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. Tools classification: supervised unsupervised","title":"Statistical Learning"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#supervised-statistical-learning-predict-an-output-variable","text":"Involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy.","title":"Supervised statistical learning (predict an output variable)"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#unsupervised-statistical-learning","text":"In this case there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. It involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a clustering problem.","title":"Unsupervised statistical learning"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#wage-data","text":"The Wage data involves predicting a continuous or quantitative output value. This is often referred to as a regression problem.","title":"Wage Data"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#stock-market-data","text":"In certain cases we may instead wish to predict a non-numerical value\u2014that is, a categorical or qualitative output. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves\u2014for exmple\u2014predicting whether a given day\u2019s stock market performance will fall into the Up bucket or the Down bucket. This is known as a classification problem.","title":"Stock Market Data"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#gene-expression-data","text":"Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.","title":"Gene Expression Data"},{"location":"artificial-intelligence/machine-learning/","text":"Machine Learning","title":"Machine Learning"},{"location":"artificial-intelligence/machine-learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/","text":"Introduction to Machine Learning It is a science of getting computers to learn without being explicitly programmed. Many scientists think the best way to make progress on this is through learning algorithms called neural networks, which mimic how the human brain works, and I'll teach you about that, too About machine Learning : Had grown out of the field of AI Is a new capability for computers Examples: Database mining Large datasets from growth of automation/web click-streams, medical records, biology, engineering Apps can\u2019t program by hand Autonomous helicopter, handwriting recognition, natural language processing (NLP), computer vision Understanding language Understanding images What is Machine Learning Even among machine learning practitioners, there isn't a well accepted definition of what is and what isn't machine learning. Arthur Samuel (1950) defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measure by P , improves with experience E . Types of learning algorithms Supervised learning : The idea is to teach the computer how to do something Unsupervised learning : In this case the computer is going to learn by itself There are also other types of algorithms such as reinforcement learning and recommender systems but the two most use types of learning algorithms are probably supervised learning and unsupervised learning. About the course: Why do we have to use Matlab or Octave? Why not Clojure, Julia, Python, R or [Insert favourite language here] ? A: As Prof. Ng explained in the 1st video of the Octave tutorial, he has tried teaching Machine Learning in a variety of languages, and found that students come up to speed faster with Matlab/Octave. Therefore the course was designed using Octave/Matlab, and the automatic submission grader uses those program interfaces. Octave and Matlab are optimized for rapid vectorized calculations, which is very useful in Machine Learning. R is a nice tool, but: It is a bit too high level. This course shows how to actually implement the algorithms of machine learning, while R already has them implemented. Since the focus of this course is to show you what happens in ML algorithms under the hood, you need to use Octave This course offers some starter code in Octave/Matlab, which will really save you tons of time solving the tasks.","title":"Intro"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#introduction-to-machine-learning","text":"It is a science of getting computers to learn without being explicitly programmed. Many scientists think the best way to make progress on this is through learning algorithms called neural networks, which mimic how the human brain works, and I'll teach you about that, too About machine Learning : Had grown out of the field of AI Is a new capability for computers Examples: Database mining Large datasets from growth of automation/web click-streams, medical records, biology, engineering Apps can\u2019t program by hand Autonomous helicopter, handwriting recognition, natural language processing (NLP), computer vision Understanding language Understanding images","title":"Introduction to Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#what-is-machine-learning","text":"Even among machine learning practitioners, there isn't a well accepted definition of what is and what isn't machine learning. Arthur Samuel (1950) defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measure by P , improves with experience E .","title":"What is Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#types-of-learning-algorithms","text":"Supervised learning : The idea is to teach the computer how to do something Unsupervised learning : In this case the computer is going to learn by itself There are also other types of algorithms such as reinforcement learning and recommender systems but the two most use types of learning algorithms are probably supervised learning and unsupervised learning.","title":"Types of learning algorithms"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#about-the-course","text":"Why do we have to use Matlab or Octave? Why not Clojure, Julia, Python, R or [Insert favourite language here] ? A: As Prof. Ng explained in the 1st video of the Octave tutorial, he has tried teaching Machine Learning in a variety of languages, and found that students come up to speed faster with Matlab/Octave. Therefore the course was designed using Octave/Matlab, and the automatic submission grader uses those program interfaces. Octave and Matlab are optimized for rapid vectorized calculations, which is very useful in Machine Learning. R is a nice tool, but: It is a bit too high level. This course shows how to actually implement the algorithms of machine learning, while R already has them implemented. Since the focus of this course is to show you what happens in ML algorithms under the hood, you need to use Octave This course offers some starter code in Octave/Matlab, which will really save you tons of time solving the tasks.","title":"About the course:"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/","text":"Supervised Learning It is probably the most common type of ML problem In Supervised Learning , we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised Learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Supervised Learning problems: Regression problems Classification problems Example 1: Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2: (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem (continuous value output), I mean we're trying to predict a continuous valued output. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignant, which is one, or zero or not malignant or benign. This is an example of a classification problem (discrete value output). The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that.","title":"Supervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#supervised-learning","text":"It is probably the most common type of ML problem In Supervised Learning , we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised Learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Supervised Learning problems: Regression problems Classification problems","title":"Supervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#example-1","text":"Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories.","title":"Example 1:"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#example-2","text":"(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem (continuous value output), I mean we're trying to predict a continuous valued output. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignant, which is one, or zero or not malignant or benign. This is an example of a classification problem (discrete value output). The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that.","title":"Example 2:"},{"location":"artificial-intelligence/machine-learning/introduction/unsupervised-learning/","text":"Unsupervised Learning Unsupervised Learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With Unsupervised Learning there is no feedback based on the prediction results. Example: Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Based on above we classify: Unsupervised Learning problems : Clustering Non-clustering Personal note: Check its relationship with logical programming (predicate based programming)","title":"Unsupervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/unsupervised-learning/#unsupervised-learning","text":"Unsupervised Learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With Unsupervised Learning there is no feedback based on the prediction results. Example: Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Based on above we classify: Unsupervised Learning problems : Clustering Non-clustering Personal note: Check its relationship with logical programming (predicate based programming)","title":"Unsupervised Learning"},{"location":"artificial-intelligence/octave-lang/","text":"Octave Lang","title":"Octave Lang"},{"location":"artificial-intelligence/octave-lang/#octave-lang","text":"","title":"Octave Lang"},{"location":"git/useful-commands/stash/","text":"Git stash git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on. Stashing is handy if you need to quickly switch context and work on something else, but you're mid-way through a code change and aren't quite ready to commit. The git stash command takes your uncommitted changes (both staged and unstaged), saves them away for later use, and then reverts them from your working copy. git stash --include-untracked \"Message to identify stash\" git stash --all \"Message to identify stash\" git stash list git stash apply [id] git stash pop [id] Popping your stash reapplies those changes to your working copy and removes them from your stash git stash drop [id]","title":"Stash"},{"location":"git/useful-commands/stash/#git-stash","text":"git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on. Stashing is handy if you need to quickly switch context and work on something else, but you're mid-way through a code change and aren't quite ready to commit. The git stash command takes your uncommitted changes (both staged and unstaged), saves them away for later use, and then reverts them from your working copy.","title":"Git stash"},{"location":"git/useful-commands/stash/#git-stash-include-untracked-message-to-identify-stash","text":"","title":"git stash --include-untracked \"Message to identify stash\""},{"location":"git/useful-commands/stash/#git-stash-all-message-to-identify-stash","text":"","title":"git stash --all \"Message to identify stash\""},{"location":"git/useful-commands/stash/#git-stash-list","text":"","title":"git stash list"},{"location":"git/useful-commands/stash/#git-stash-apply-id","text":"","title":"git stash apply [id]"},{"location":"git/useful-commands/stash/#git-stash-pop-id","text":"Popping your stash reapplies those changes to your working copy and removes them from your stash","title":"git stash pop [id]"},{"location":"git/useful-commands/stash/#git-stash-drop-id","text":"","title":"git stash drop [id]"},{"location":"java/concurrency/fundamentals/c02-thread-safety/","text":"2. Thread safety Surprisingly, concurrent programming isn\u2019t so much about threads or locks, these are just mechanisms\u2014means to an end. Writing thread-safe code is, at its core, about managing access to state , and in particular to shared, mutable state . Informally, an object\u2019s state is its data, stored in state variables such as instance or static fields. An object\u2019s state encompasses any data that can affect its externally visible behavior. By shared , we mean that a variable could be accessed by multiple threads; by mutable , we mean that its value could change during its lifetime. What we are really trying to do is protect data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads . This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state ; failing to do so could result in data corruption and other undesirable consequences. The primary mechanism for synchronization in Java is the synchronized keyword, which pro- vides exclusive locking, but the term \u201csynchronization\u201d also includes the use of volatile variables, explicit locks, and atomic variables. If multiple threads access the same mutable state variable without appro- priate synchronization, your program is broken . There are three ways to fix it: Don\u2019t share the state variable across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable. It is far easier to design a class to be thread-safe than to retrofit it for thread safety later. When designing thread-safe classes, good object-oriented techniques\u2014encapsulation, immutability, and clear specification of invariants\u2014are your best friends. It is always a good practice first to make your code right, and then make it fast. Even then, pursue optimization only if your performance measurements and requirements tell you that you must, and if those same measurements tell you that your optimizations actually made a difference under realistic conditions. Is a thread-safe program one that is constructed entirely of thread-safe classes? Not necessarily\u2014a program that consists entirely of thread-safe classes may not be thread-safe, and a thread-safe program may contain classes that are not thread-safe. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state. Thread safety may be a term that is applied to code, but it is about state, and it can only be applied to the entire body of code that encapsulates its state, which may be an object or an entire program. What is thread safety? Formal definition A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations\u2014calls to public methods and reads or writes of public fields\u2014should be able to violate any of its invariants or postconditions. No set of operations performed sequentially or con- currently on instances of a thread-safe class can cause an instance to be in an invalid state. Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own. Example: A stateless servlet @ThreadSafe public class StatelessFactorizer implements Servlet { public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } The transient state for a particular computation exists solely in local variables that are stored on the thread\u2019s stack and are accessible only to the executing thread. One thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer ; because the two threads do not share state, it is as if they were accessing different instances. Since the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe. Stateless objects are always thread-safe. Atomicity Atomic means something that executes as a single, indivisible operation. What happens when we add one element of state to what was a stateless object? Suppose we want to add a \u201chit counter\u201d that measures the number of requests processed. The obvious approach is to add a long field to the servlet and increment it on each request: @NotThreadSafe public class UnsafeCountingFactorizer implements Servlet { private long count = 0 ; public long getCount () { return count ; } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } While the increment operation, ++count, may look like a single action because of its compact syntax, it is not atomic, which means that it does not execute as a single, indivisible operation. Instead, it is a shorthand for a sequence of three discrete operations: fetch the current value, add one to it, and write the new value back. This is an example of a read-modify-write operation, in which the resulting state is derived from the previous state. The possibility of incorrect results in the presence of unlucky timing is so important in concurrent programming that it has a name: a race condition . Race conditions A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime; in other words, when getting the right answer relies on lucky timing. The most common type of race condition is check-then-act , where a potentially stale observation is used to make a decision on what to do next. You observe something to be true (file X doesn\u2019t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption). Example: race conditions in lazy initialization A common idiom that uses check-then-act is lazy initialization. The goal of lazy initialization is to defer initializing an object until it is actually needed while at the same time ensuring that it is initialized only once. @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null ; public ExpensiveObject getInstance () { if ( instance == null ) { instance = new ExpensiveObject (); return instance ; } } } Now we have identified two sorts of race condition operations: Read-modify-write: like incrementing a counter (i.e.: count++; ), define a transformation of an object\u2019s state in terms of its previous state. Check-then-act: Check for a condition (i.e.: if (instance == null) ) and then act (i.e.: {instance = new ExpensiveObject(); ...} ) Like most concurrency errors, race conditions don\u2019t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems. Compound actions We refer collectively to check-then-act and read-modify-write sequences as compound actions: sequences of operations that must be executed atomically in order to remain thread-safe. Fixing UnsafeCountingFactorizer @ThreadSafe public class CountingFactorizer implements Servlet { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); count . incrementAndGet (); encodeIntoResponse ( resp , factors ); } } The java.util.concurrent.atomic package contains atomic variable classes for effecting atomic state transitions on numbers and object references. By replacing the long counter with an AtomicLong, we ensure that all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe, our servlet is once again thread-safe. Locking To preserve state consistency, update related state variables in a single atomic operation. Intrinsic locks Java provides a built-in locking mechanism for enforcing atomicity: the synchronized block. A synchronized block has two parts: a reference to an object that will serve as the lock, and a block of code to be guarded by that lock. A synchronized method is a shorthand for a synchronized block that spans an entire method body, and whose lock is the object on which the method is being invoked. (Static synchronized methods use the Class object for the lock.) synchronized ( lock ) { // Access or modify shared state guarded by lock } Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. The only way to acquire an intrinsic lock is to enter a synchronized block or method guarded by that lock. Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. No thread executing a synchronized block can observe another thread to be in the middle of a synchronized block guarded by the same lock. That is, the synchronized blocks guarded by the same lock execute atomically with respect to one another. In the context of concurrency, atomicity means the same thing as it does in transactional applications\u2014that a group of statements appear to execute as a single, indivisible unit. Reentrancy When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy facilitates encapsulation of locking behavior, and thus simplifies the development of object-oriented concurrent code. public class Widget { public synchronized void doSomething () { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething () { System . out . println ( toString () + \": calling doSomething\" ); super . doSomething (); } } Without reentrant locks, the very natural-looking code above, in which a subclass overrides a synchronized method and then calls the superclass method, would deadlock. Because the doSomething methods in Widget and LoggingWidget are both synchronized, each tries to acquire the lock on the Widget before proceeding. But if intrinsic locks were not reentrant, the call to super.doSomething would never be able to acquire the lock because it would be considered already held, and the thread would permanently stall waiting for a lock it can never acquire. Reentrancy saves us from deadlock in situations like this. Guarding state with locks Because locks enable serialized 1 access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following these protocols consistently can ensure state consistency. Compound actions on shared state, such as incrementing a hit counter ( read-modify-write ) or lazy initialization ( check-then-act ), must be made atomic to avoid race conditions. Holding a lock for the entire duration of a compound action can make that compound action atomic. However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed. It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Acquiring the lock associated with an object does not prevent other threads from accessing that object\u2014the only thing that acquiring a lock prevents any other thread from doing is acquiring that same lock. The fact that every object has a built-in lock is just a convenience so that you needn\u2019t explicitly create lock objects 2 . Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is. A common locking convention is to encapsulate all mutable state within an object and to protect it from concurrent access by synchronizing any code path that accesses mutable state using the object\u2019s intrinsic lock. Not all data needs to be guarded by locks\u2014only mutable data that will be accessed from multiple threads. When a variable is guarded by a lock\u2014meaning that every access to that variable is performed with that lock held\u2014you\u2019ve ensured that only one thread at a time can access that variable. When a class has invariants that involve more than one state variable, there is an additional requirement: each variable participating in the invariant must be guarded by the same lock. This allows you to access or update them in a single atomic operation, preserving the invariant. For every invariant that involves more than one variable, all the variables involved in that invariant must be guarded by the same lock. If synchronization is the cure for race conditions, why not just declare ev- ery method synchronized? if (! vector . contains ( element )) vector . add ( element ); This attempt at a put-if-absent operation has a race condition, even though both contains and add are atomic. While synchronized methods can make individual operations atomic, additional locking is required when multiple operations are combined into a compound action. At the same time, synchronizing every method can lead to liveness or performance problems. Liveness and performance Deciding how big or small to make synchronized blocks may require tradeoffs among competing design forces, including safety (which must not be compromised), simplicity, and performance. Sometimes simplicity and performance are at odds with each other, a reasonable balance can usually be found. Whenever you use locking, you should be aware of what the code in the block is doing and how likely it is to take a long time to execute. Holding a lock for a long time, either because you are doing something compute-intensive or because you execute a potentially blocking operation, introduces the risk of liveness or performance problems. Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O. Serializing access to an object has nothing to do with object serialization (turning an object into a byte stream); serializing access means that threads take turns accessing the object exclusively, rather than doing so concurrently. \u21a9 In retrospect, this design decision was probably a bad one: not only can it be confusing, but it forces JVM implementors to make tradeoffs between object size and locking performance. \u21a9","title":"Thread safety"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#2-thread-safety","text":"Surprisingly, concurrent programming isn\u2019t so much about threads or locks, these are just mechanisms\u2014means to an end. Writing thread-safe code is, at its core, about managing access to state , and in particular to shared, mutable state . Informally, an object\u2019s state is its data, stored in state variables such as instance or static fields. An object\u2019s state encompasses any data that can affect its externally visible behavior. By shared , we mean that a variable could be accessed by multiple threads; by mutable , we mean that its value could change during its lifetime. What we are really trying to do is protect data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads . This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state ; failing to do so could result in data corruption and other undesirable consequences. The primary mechanism for synchronization in Java is the synchronized keyword, which pro- vides exclusive locking, but the term \u201csynchronization\u201d also includes the use of volatile variables, explicit locks, and atomic variables. If multiple threads access the same mutable state variable without appro- priate synchronization, your program is broken . There are three ways to fix it: Don\u2019t share the state variable across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable. It is far easier to design a class to be thread-safe than to retrofit it for thread safety later. When designing thread-safe classes, good object-oriented techniques\u2014encapsulation, immutability, and clear specification of invariants\u2014are your best friends. It is always a good practice first to make your code right, and then make it fast. Even then, pursue optimization only if your performance measurements and requirements tell you that you must, and if those same measurements tell you that your optimizations actually made a difference under realistic conditions. Is a thread-safe program one that is constructed entirely of thread-safe classes? Not necessarily\u2014a program that consists entirely of thread-safe classes may not be thread-safe, and a thread-safe program may contain classes that are not thread-safe. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state. Thread safety may be a term that is applied to code, but it is about state, and it can only be applied to the entire body of code that encapsulates its state, which may be an object or an entire program.","title":"2. Thread safety"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#what-is-thread-safety","text":"","title":"What is thread safety?"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#formal-definition","text":"A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations\u2014calls to public methods and reads or writes of public fields\u2014should be able to violate any of its invariants or postconditions. No set of operations performed sequentially or con- currently on instances of a thread-safe class can cause an instance to be in an invalid state. Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own.","title":"Formal definition"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#example-a-stateless-servlet","text":"@ThreadSafe public class StatelessFactorizer implements Servlet { public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } The transient state for a particular computation exists solely in local variables that are stored on the thread\u2019s stack and are accessible only to the executing thread. One thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer ; because the two threads do not share state, it is as if they were accessing different instances. Since the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe. Stateless objects are always thread-safe.","title":"Example: A stateless servlet"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#atomicity","text":"Atomic means something that executes as a single, indivisible operation. What happens when we add one element of state to what was a stateless object? Suppose we want to add a \u201chit counter\u201d that measures the number of requests processed. The obvious approach is to add a long field to the servlet and increment it on each request: @NotThreadSafe public class UnsafeCountingFactorizer implements Servlet { private long count = 0 ; public long getCount () { return count ; } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } While the increment operation, ++count, may look like a single action because of its compact syntax, it is not atomic, which means that it does not execute as a single, indivisible operation. Instead, it is a shorthand for a sequence of three discrete operations: fetch the current value, add one to it, and write the new value back. This is an example of a read-modify-write operation, in which the resulting state is derived from the previous state. The possibility of incorrect results in the presence of unlucky timing is so important in concurrent programming that it has a name: a race condition .","title":"Atomicity"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#race-conditions","text":"A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime; in other words, when getting the right answer relies on lucky timing. The most common type of race condition is check-then-act , where a potentially stale observation is used to make a decision on what to do next. You observe something to be true (file X doesn\u2019t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption).","title":"Race conditions"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#example-race-conditions-in-lazy-initialization","text":"A common idiom that uses check-then-act is lazy initialization. The goal of lazy initialization is to defer initializing an object until it is actually needed while at the same time ensuring that it is initialized only once. @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null ; public ExpensiveObject getInstance () { if ( instance == null ) { instance = new ExpensiveObject (); return instance ; } } } Now we have identified two sorts of race condition operations: Read-modify-write: like incrementing a counter (i.e.: count++; ), define a transformation of an object\u2019s state in terms of its previous state. Check-then-act: Check for a condition (i.e.: if (instance == null) ) and then act (i.e.: {instance = new ExpensiveObject(); ...} ) Like most concurrency errors, race conditions don\u2019t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems.","title":"Example: race conditions in lazy initialization"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#compound-actions","text":"We refer collectively to check-then-act and read-modify-write sequences as compound actions: sequences of operations that must be executed atomically in order to remain thread-safe.","title":"Compound actions"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#fixing-unsafecountingfactorizer","text":"@ThreadSafe public class CountingFactorizer implements Servlet { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); count . incrementAndGet (); encodeIntoResponse ( resp , factors ); } } The java.util.concurrent.atomic package contains atomic variable classes for effecting atomic state transitions on numbers and object references. By replacing the long counter with an AtomicLong, we ensure that all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe, our servlet is once again thread-safe.","title":"Fixing UnsafeCountingFactorizer"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#locking","text":"To preserve state consistency, update related state variables in a single atomic operation.","title":"Locking"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#intrinsic-locks","text":"Java provides a built-in locking mechanism for enforcing atomicity: the synchronized block. A synchronized block has two parts: a reference to an object that will serve as the lock, and a block of code to be guarded by that lock. A synchronized method is a shorthand for a synchronized block that spans an entire method body, and whose lock is the object on which the method is being invoked. (Static synchronized methods use the Class object for the lock.) synchronized ( lock ) { // Access or modify shared state guarded by lock } Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. The only way to acquire an intrinsic lock is to enter a synchronized block or method guarded by that lock. Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. No thread executing a synchronized block can observe another thread to be in the middle of a synchronized block guarded by the same lock. That is, the synchronized blocks guarded by the same lock execute atomically with respect to one another. In the context of concurrency, atomicity means the same thing as it does in transactional applications\u2014that a group of statements appear to execute as a single, indivisible unit.","title":"Intrinsic locks"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#reentrancy","text":"When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy facilitates encapsulation of locking behavior, and thus simplifies the development of object-oriented concurrent code. public class Widget { public synchronized void doSomething () { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething () { System . out . println ( toString () + \": calling doSomething\" ); super . doSomething (); } } Without reentrant locks, the very natural-looking code above, in which a subclass overrides a synchronized method and then calls the superclass method, would deadlock. Because the doSomething methods in Widget and LoggingWidget are both synchronized, each tries to acquire the lock on the Widget before proceeding. But if intrinsic locks were not reentrant, the call to super.doSomething would never be able to acquire the lock because it would be considered already held, and the thread would permanently stall waiting for a lock it can never acquire. Reentrancy saves us from deadlock in situations like this.","title":"Reentrancy"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#guarding-state-with-locks","text":"Because locks enable serialized 1 access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following these protocols consistently can ensure state consistency. Compound actions on shared state, such as incrementing a hit counter ( read-modify-write ) or lazy initialization ( check-then-act ), must be made atomic to avoid race conditions. Holding a lock for the entire duration of a compound action can make that compound action atomic. However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed. It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Acquiring the lock associated with an object does not prevent other threads from accessing that object\u2014the only thing that acquiring a lock prevents any other thread from doing is acquiring that same lock. The fact that every object has a built-in lock is just a convenience so that you needn\u2019t explicitly create lock objects 2 . Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is. A common locking convention is to encapsulate all mutable state within an object and to protect it from concurrent access by synchronizing any code path that accesses mutable state using the object\u2019s intrinsic lock. Not all data needs to be guarded by locks\u2014only mutable data that will be accessed from multiple threads. When a variable is guarded by a lock\u2014meaning that every access to that variable is performed with that lock held\u2014you\u2019ve ensured that only one thread at a time can access that variable. When a class has invariants that involve more than one state variable, there is an additional requirement: each variable participating in the invariant must be guarded by the same lock. This allows you to access or update them in a single atomic operation, preserving the invariant. For every invariant that involves more than one variable, all the variables involved in that invariant must be guarded by the same lock. If synchronization is the cure for race conditions, why not just declare ev- ery method synchronized? if (! vector . contains ( element )) vector . add ( element ); This attempt at a put-if-absent operation has a race condition, even though both contains and add are atomic. While synchronized methods can make individual operations atomic, additional locking is required when multiple operations are combined into a compound action. At the same time, synchronizing every method can lead to liveness or performance problems.","title":"Guarding state with locks"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#liveness-and-performance","text":"Deciding how big or small to make synchronized blocks may require tradeoffs among competing design forces, including safety (which must not be compromised), simplicity, and performance. Sometimes simplicity and performance are at odds with each other, a reasonable balance can usually be found. Whenever you use locking, you should be aware of what the code in the block is doing and how likely it is to take a long time to execute. Holding a lock for a long time, either because you are doing something compute-intensive or because you execute a potentially blocking operation, introduces the risk of liveness or performance problems. Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O. Serializing access to an object has nothing to do with object serialization (turning an object into a byte stream); serializing access means that threads take turns accessing the object exclusively, rather than doing so concurrently. \u21a9 In retrospect, this design decision was probably a bad one: not only can it be confusing, but it forces JVM implementors to make tradeoffs between object size and locking performance. \u21a9","title":"Liveness and performance"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/","text":"3. Sharing objects","title":"Sharing objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#3-sharing-objects","text":"","title":"3. Sharing objects"},{"location":"java/concurrency/introduction/c01-intro/","text":"1. Java Concurrency The language provides low-level mechanisms such as synchronization and condition waits , but these mechanisms must be used consistently to implement application-level protocols or policies. Without such policies, it is all too easy to create programs that compile and appear to work but are nevertheless broken. Processes Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously: Resource utilization . Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run. Fairness . Multiple users and programs may have equal claims on the machine\u2019s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start an- other. Convenience . It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks. A daily life sample The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence\u2014mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions\u2014open the cupboard, select a flavor of tea, measure some tea into the pot, see if there\u2019s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step\u2014waiting for the water to boil\u2014also involves a degree of asynchrony. While the water is heating, you have a choice of what to do\u2014just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people\u2014and the same is true of programs . Threads The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of threads. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs. Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms . But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results. Benefits of threads When used properly, threads can reduce development and maintenance costs and improve the performance of complex applications. Threads make it easier to model how humans work and interact, by turning asynchronous workflows into mostly sequential ones. They can also turn otherwise convoluted code into straight-line code that is easier to write, read, and maintain. Exploiting multiple processors Since the basic unit of scheduling is the thread, a program with only one thread can run on at most one processor at a time. On a two-processor system, a single-threaded program is giving up access to half the available CPU resources; on a 100-processor system, it is giving up access to 99%. On the other hand, programs with multiple active threads can execute simultaneously on multiple processors. When properly designed, multithreaded programs can improve throughput by utilizing available processor resources more effectively. Using multiple threads can also help achieve better throughput on single-processor systems. If a program is single-threaded, the processor remains idle while it waits for a synchronous I/O operation to complete. In a multithreaded program, another thread can still run while the first thread is waiting for the I/O to complete, allowing the application to still make progress during the blocking I/O. (This is like reading the newspaper while waiting for the water to boil, rather than waiting for the water to boil before starting to read.) Simplicity of modeling It is often easier to manage your time when you have only one type of task to perform (fix these twelve bugs) than when you have several (fix the bugs, interview replacement candidates for the system administrator, complete your team\u2019s performance evaluations, and create the slides for your presentation next week). When you have only one type of task to do, you can start at the top of the pile and keep working until the pile is exhausted (or you are); you don\u2019t have to spend any mental energy figuring out what to work on next. On the other hand, managing multiple priorities and deadlines and switching from task to task usually carries some overhead. The same is true for software: a program that processes one type of task sequentially is simpler to write, less error-prone, and easier to test than one managing multiple different types of tasks at once. Assigning a thread to each type of task or to each element in a simulation affords the illusion of sequentiality and insulates domain logic from the details of scheduling, interleaved operations, asynchronous I/O, and resource waits. A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points. Simplified handling of asynchronous events A server application that accepts socket connections from multiple remote clients may be easier to develop when each connection is allocated its own thread and allowed to use synchronous I/O. If an application goes to read from a socket when no data is available, read blocks until some data is available. In a single-threaded application, this means that not only does processing the corresponding request stall, but processing of all requests stalls while the single thread is blocked. To avoid this problem, single-threaded server applications are forced to use nonblocking I/O, which is far more complicated and error-prone than synchronous I/O. However, if each request has its own thread, then blocking does not affect the processing of other requests. Historically, operating systems placed relatively low limits on the number of threads that a process could create, as few as several hundred (or even less). As a result, operating systems developed efficient facilities for multiplexed I/O, such as the Unix select and poll system calls, and to access these facilities, the Java class libraries acquired a set of packages (java.nio) for nonblocking I/O. However, operating system support for larger numbers of threads has improved significantly, making the thread-per-client model practical even for large numbers of clients on some platforms. Risks of threads Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising. Safety hazards Because threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. This is a tremendous convenience, because it makes data sharing much easier than would other inter-thread communications mechanisms. But it is also a significant risk: threads can be confused by having data change unexpectedly. Allowing multiple threads to access and modify the same variables introduces an element of nonsequentiality into an otherwise sequential programming model, which can be confusing and difficult to reason about. For a multithreaded program\u2019s behavior to be predictable, access to shared variables must be properly coordinated so that threads do not interfere with one another. Liveness hazards A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. One form of liveness failure that can occur in sequential programs is an inadvertent infinite loop, where the code that follows the loop never gets executed. The use of threads introduces additional liveness risks. For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever. Like most concurrency bugs, bugs that cause liveness failures can be elusive because they depend on the relative timing of events in different threads, and therefore do not always manifest themselves in development or testing. Performance hazards Performance issues subsume a broad range of problems, including poor service time, responsiveness, throughput, resource consumption, or scalability. Just as with safety and liveness, multithreaded programs are subject to all the performance hazards of single-threaded programs, and to others as well that are introduced by the use of threads. In well designed concurrent applications the use of threads is a net performance gain, but threads nevertheless carry some degree of runtime overhead. Context switches\u2014when the scheduler suspends the active thread temporarily so another thread can run\u2014are more frequent in applications with many threads, and have significant costs: saving and restoring execution context, loss of locality, and CPU time spent scheduling threads instead of running them. When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus.","title":"Introduction"},{"location":"java/concurrency/introduction/c01-intro/#1-java-concurrency","text":"The language provides low-level mechanisms such as synchronization and condition waits , but these mechanisms must be used consistently to implement application-level protocols or policies. Without such policies, it is all too easy to create programs that compile and appear to work but are nevertheless broken.","title":"1. Java Concurrency"},{"location":"java/concurrency/introduction/c01-intro/#processes","text":"Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously: Resource utilization . Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run. Fairness . Multiple users and programs may have equal claims on the machine\u2019s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start an- other. Convenience . It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks.","title":"Processes"},{"location":"java/concurrency/introduction/c01-intro/#a-daily-life-sample","text":"The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence\u2014mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions\u2014open the cupboard, select a flavor of tea, measure some tea into the pot, see if there\u2019s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step\u2014waiting for the water to boil\u2014also involves a degree of asynchrony. While the water is heating, you have a choice of what to do\u2014just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people\u2014and the same is true of programs .","title":"A daily life sample"},{"location":"java/concurrency/introduction/c01-intro/#threads","text":"The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of threads. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs. Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms . But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results.","title":"Threads"},{"location":"java/concurrency/introduction/c01-intro/#benefits-of-threads","text":"When used properly, threads can reduce development and maintenance costs and improve the performance of complex applications. Threads make it easier to model how humans work and interact, by turning asynchronous workflows into mostly sequential ones. They can also turn otherwise convoluted code into straight-line code that is easier to write, read, and maintain.","title":"Benefits of threads"},{"location":"java/concurrency/introduction/c01-intro/#exploiting-multiple-processors","text":"Since the basic unit of scheduling is the thread, a program with only one thread can run on at most one processor at a time. On a two-processor system, a single-threaded program is giving up access to half the available CPU resources; on a 100-processor system, it is giving up access to 99%. On the other hand, programs with multiple active threads can execute simultaneously on multiple processors. When properly designed, multithreaded programs can improve throughput by utilizing available processor resources more effectively. Using multiple threads can also help achieve better throughput on single-processor systems. If a program is single-threaded, the processor remains idle while it waits for a synchronous I/O operation to complete. In a multithreaded program, another thread can still run while the first thread is waiting for the I/O to complete, allowing the application to still make progress during the blocking I/O. (This is like reading the newspaper while waiting for the water to boil, rather than waiting for the water to boil before starting to read.)","title":"Exploiting multiple processors"},{"location":"java/concurrency/introduction/c01-intro/#simplicity-of-modeling","text":"It is often easier to manage your time when you have only one type of task to perform (fix these twelve bugs) than when you have several (fix the bugs, interview replacement candidates for the system administrator, complete your team\u2019s performance evaluations, and create the slides for your presentation next week). When you have only one type of task to do, you can start at the top of the pile and keep working until the pile is exhausted (or you are); you don\u2019t have to spend any mental energy figuring out what to work on next. On the other hand, managing multiple priorities and deadlines and switching from task to task usually carries some overhead. The same is true for software: a program that processes one type of task sequentially is simpler to write, less error-prone, and easier to test than one managing multiple different types of tasks at once. Assigning a thread to each type of task or to each element in a simulation affords the illusion of sequentiality and insulates domain logic from the details of scheduling, interleaved operations, asynchronous I/O, and resource waits. A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points.","title":"Simplicity of modeling"},{"location":"java/concurrency/introduction/c01-intro/#simplified-handling-of-asynchronous-events","text":"A server application that accepts socket connections from multiple remote clients may be easier to develop when each connection is allocated its own thread and allowed to use synchronous I/O. If an application goes to read from a socket when no data is available, read blocks until some data is available. In a single-threaded application, this means that not only does processing the corresponding request stall, but processing of all requests stalls while the single thread is blocked. To avoid this problem, single-threaded server applications are forced to use nonblocking I/O, which is far more complicated and error-prone than synchronous I/O. However, if each request has its own thread, then blocking does not affect the processing of other requests. Historically, operating systems placed relatively low limits on the number of threads that a process could create, as few as several hundred (or even less). As a result, operating systems developed efficient facilities for multiplexed I/O, such as the Unix select and poll system calls, and to access these facilities, the Java class libraries acquired a set of packages (java.nio) for nonblocking I/O. However, operating system support for larger numbers of threads has improved significantly, making the thread-per-client model practical even for large numbers of clients on some platforms.","title":"Simplified handling of asynchronous events"},{"location":"java/concurrency/introduction/c01-intro/#risks-of-threads","text":"Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising.","title":"Risks of threads"},{"location":"java/concurrency/introduction/c01-intro/#safety-hazards","text":"Because threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. This is a tremendous convenience, because it makes data sharing much easier than would other inter-thread communications mechanisms. But it is also a significant risk: threads can be confused by having data change unexpectedly. Allowing multiple threads to access and modify the same variables introduces an element of nonsequentiality into an otherwise sequential programming model, which can be confusing and difficult to reason about. For a multithreaded program\u2019s behavior to be predictable, access to shared variables must be properly coordinated so that threads do not interfere with one another.","title":"Safety hazards"},{"location":"java/concurrency/introduction/c01-intro/#liveness-hazards","text":"A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. One form of liveness failure that can occur in sequential programs is an inadvertent infinite loop, where the code that follows the loop never gets executed. The use of threads introduces additional liveness risks. For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever. Like most concurrency bugs, bugs that cause liveness failures can be elusive because they depend on the relative timing of events in different threads, and therefore do not always manifest themselves in development or testing.","title":"Liveness hazards"},{"location":"java/concurrency/introduction/c01-intro/#performance-hazards","text":"Performance issues subsume a broad range of problems, including poor service time, responsiveness, throughput, resource consumption, or scalability. Just as with safety and liveness, multithreaded programs are subject to all the performance hazards of single-threaded programs, and to others as well that are introduced by the use of threads. In well designed concurrent applications the use of threads is a net performance gain, but threads nevertheless carry some degree of runtime overhead. Context switches\u2014when the scheduler suspends the active thread temporarily so another thread can run\u2014are more frequent in applications with many threads, and have significant costs: saving and restoring execution context, loss of locality, and CPU time spent scheduling threads instead of running them. When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus.","title":"Performance hazards"}]}