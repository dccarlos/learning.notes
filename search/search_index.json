{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My learning notes","title":"Home"},{"location":"#my-learning-notes","text":"","title":"My learning notes"},{"location":"artificial-intelligence/","text":"Artificial Intelligence","title":"Artificial Intelligence"},{"location":"artificial-intelligence/#artificial-intelligence","text":"","title":"Artificial Intelligence"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/","text":"Statistical Learning Based on the youngest brother of The Elements of Statistical Learning (ESL) : An Introduction to Statistical Learning Statistical learning refers to a set of tools for modeling and understanding complex datasets . It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. Tools classification: supervised unsupervised Supervised statistical learning (predict an output variable) Involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. Unsupervised statistical learning In this case there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. It involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a clustering problem. Wage Data The Wage data involves predicting a continuous or quantitative output value. This is often referred to as a regression problem. Stock Market Data In certain cases we may instead wish to predict a non-numerical value\u2014that is, a categorical or qualitative output. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves\u2014for exmple\u2014predicting whether a given day\u2019s stock market performance will fall into the Up bucket or the Down bucket. This is known as a classification problem. Gene Expression Data Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.","title":"Introduction"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#statistical-learning","text":"Based on the youngest brother of The Elements of Statistical Learning (ESL) : An Introduction to Statistical Learning Statistical learning refers to a set of tools for modeling and understanding complex datasets . It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. Tools classification: supervised unsupervised","title":"Statistical Learning"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#supervised-statistical-learning-predict-an-output-variable","text":"Involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy.","title":"Supervised statistical learning (predict an output variable)"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#unsupervised-statistical-learning","text":"In this case there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. It involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a clustering problem.","title":"Unsupervised statistical learning"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#wage-data","text":"The Wage data involves predicting a continuous or quantitative output value. This is often referred to as a regression problem.","title":"Wage Data"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#stock-market-data","text":"In certain cases we may instead wish to predict a non-numerical value\u2014that is, a categorical or qualitative output. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves\u2014for exmple\u2014predicting whether a given day\u2019s stock market performance will fall into the Up bucket or the Down bucket. This is known as a classification problem.","title":"Stock Market Data"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#gene-expression-data","text":"Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.","title":"Gene Expression Data"},{"location":"artificial-intelligence/machine-learning/","text":"Machine Learning","title":"Machine Learning"},{"location":"artificial-intelligence/machine-learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/","text":"Introduction to Machine Learning It is a science of getting computers to learn without being explicitly programmed. Many scientists think the best way to make progress on this is through learning algorithms called neural networks, which mimic how the human brain works, and I'll teach you about that, too About machine Learning : Had grown out of the field of AI Is a new capability for computers Examples: Database mining Large datasets from growth of automation/web click-streams, medical records, biology, engineering Apps can\u2019t program by hand Autonomous helicopter, handwriting recognition, natural language processing (NLP), computer vision Understanding language Understanding images What is Machine Learning Even among machine learning practitioners, there isn't a well accepted definition of what is and what isn't machine learning. Arthur Samuel (1950) defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measure by P , improves with experience E . Types of learning algorithms Supervised learning : The idea is to teach the computer how to do something Unsupervised learning : In this case the computer is going to learn by itself There are also other types of algorithms such as reinforcement learning and recommender systems but the two most use types of learning algorithms are probably supervised learning and unsupervised learning. About the course: Why do we have to use Matlab or Octave? Why not Clojure, Julia, Python, R or [Insert favourite language here] ? A: As Prof. Ng explained in the 1st video of the Octave tutorial, he has tried teaching Machine Learning in a variety of languages, and found that students come up to speed faster with Matlab/Octave. Therefore the course was designed using Octave/Matlab, and the automatic submission grader uses those program interfaces. Octave and Matlab are optimized for rapid vectorized calculations, which is very useful in Machine Learning. R is a nice tool, but: It is a bit too high level. This course shows how to actually implement the algorithms of machine learning, while R already has them implemented. Since the focus of this course is to show you what happens in ML algorithms under the hood, you need to use Octave This course offers some starter code in Octave/Matlab, which will really save you tons of time solving the tasks.","title":"Intro"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#introduction-to-machine-learning","text":"It is a science of getting computers to learn without being explicitly programmed. Many scientists think the best way to make progress on this is through learning algorithms called neural networks, which mimic how the human brain works, and I'll teach you about that, too About machine Learning : Had grown out of the field of AI Is a new capability for computers Examples: Database mining Large datasets from growth of automation/web click-streams, medical records, biology, engineering Apps can\u2019t program by hand Autonomous helicopter, handwriting recognition, natural language processing (NLP), computer vision Understanding language Understanding images","title":"Introduction to Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#what-is-machine-learning","text":"Even among machine learning practitioners, there isn't a well accepted definition of what is and what isn't machine learning. Arthur Samuel (1950) defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measure by P , improves with experience E .","title":"What is Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#types-of-learning-algorithms","text":"Supervised learning : The idea is to teach the computer how to do something Unsupervised learning : In this case the computer is going to learn by itself There are also other types of algorithms such as reinforcement learning and recommender systems but the two most use types of learning algorithms are probably supervised learning and unsupervised learning.","title":"Types of learning algorithms"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#about-the-course","text":"Why do we have to use Matlab or Octave? Why not Clojure, Julia, Python, R or [Insert favourite language here] ? A: As Prof. Ng explained in the 1st video of the Octave tutorial, he has tried teaching Machine Learning in a variety of languages, and found that students come up to speed faster with Matlab/Octave. Therefore the course was designed using Octave/Matlab, and the automatic submission grader uses those program interfaces. Octave and Matlab are optimized for rapid vectorized calculations, which is very useful in Machine Learning. R is a nice tool, but: It is a bit too high level. This course shows how to actually implement the algorithms of machine learning, while R already has them implemented. Since the focus of this course is to show you what happens in ML algorithms under the hood, you need to use Octave This course offers some starter code in Octave/Matlab, which will really save you tons of time solving the tasks.","title":"About the course:"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/","text":"Supervised Learning It is probably the most common type of ML problem In Supervised Learning , we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised Learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Supervised Learning problems: Regression problems Classification problems Example 1: Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2: (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem (continuous value output), I mean we're trying to predict a continuous valued output. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignant, which is one, or zero or not malignant or benign. This is an example of a classification problem (discrete value output). The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that.","title":"Supervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#supervised-learning","text":"It is probably the most common type of ML problem In Supervised Learning , we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised Learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Supervised Learning problems: Regression problems Classification problems","title":"Supervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#example-1","text":"Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories.","title":"Example 1:"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#example-2","text":"(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem (continuous value output), I mean we're trying to predict a continuous valued output. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignant, which is one, or zero or not malignant or benign. This is an example of a classification problem (discrete value output). The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that.","title":"Example 2:"},{"location":"artificial-intelligence/machine-learning/introduction/unsupervised-learning/","text":"Unsupervised Learning Unsupervised Learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With Unsupervised Learning there is no feedback based on the prediction results. Example: Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Based on above we classify: Unsupervised Learning problems : Clustering Non-clustering Personal note: Check its relationship with logical programming (predicate based programming)","title":"Unsupervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/unsupervised-learning/#unsupervised-learning","text":"Unsupervised Learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With Unsupervised Learning there is no feedback based on the prediction results. Example: Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Based on above we classify: Unsupervised Learning problems : Clustering Non-clustering Personal note: Check its relationship with logical programming (predicate based programming)","title":"Unsupervised Learning"},{"location":"artificial-intelligence/octave-lang/","text":"Octave Lang","title":"Octave Lang"},{"location":"artificial-intelligence/octave-lang/#octave-lang","text":"","title":"Octave Lang"},{"location":"git/useful-commands/stash/","text":"Git stash git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on. Stashing is handy if you need to quickly switch context and work on something else, but you're mid-way through a code change and aren't quite ready to commit. The git stash command takes your uncommitted changes (both staged and unstaged), saves them away for later use, and then reverts them from your working copy. git stash --include-untracked \"Message to identify stash\" git stash --all \"Message to identify stash\" git stash list git stash apply [id] git stash pop [id] Popping your stash reapplies those changes to your working copy and removes them from your stash git stash drop [id]","title":"Stash"},{"location":"git/useful-commands/stash/#git-stash","text":"git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on. Stashing is handy if you need to quickly switch context and work on something else, but you're mid-way through a code change and aren't quite ready to commit. The git stash command takes your uncommitted changes (both staged and unstaged), saves them away for later use, and then reverts them from your working copy.","title":"Git stash"},{"location":"git/useful-commands/stash/#git-stash-include-untracked-message-to-identify-stash","text":"","title":"git stash --include-untracked \"Message to identify stash\""},{"location":"git/useful-commands/stash/#git-stash-all-message-to-identify-stash","text":"","title":"git stash --all \"Message to identify stash\""},{"location":"git/useful-commands/stash/#git-stash-list","text":"","title":"git stash list"},{"location":"git/useful-commands/stash/#git-stash-apply-id","text":"","title":"git stash apply [id]"},{"location":"git/useful-commands/stash/#git-stash-pop-id","text":"Popping your stash reapplies those changes to your working copy and removes them from your stash","title":"git stash pop [id]"},{"location":"git/useful-commands/stash/#git-stash-drop-id","text":"","title":"git stash drop [id]"},{"location":"java/concurrency/fundamentals/c02-thread-safety/","text":"2. Thread safety Surprisingly, concurrent programming isn\u2019t so much about threads or locks, these are just mechanisms\u2014means to an end. Writing thread-safe code is, at its core, about managing access to state , and in particular to shared, mutable state . Informally, an object\u2019s state is its data, stored in state variables such as instance or static fields. An object\u2019s state encompasses any data that can affect its externally visible behavior. By shared , we mean that a variable could be accessed by multiple threads; by mutable , we mean that its value could change during its lifetime. What we are really trying to do is protect data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads . This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state ; failing to do so could result in data corruption and other undesirable consequences. The primary mechanism for synchronization in Java is the synchronized keyword, which pro- vides exclusive locking, but the term \u201csynchronization\u201d also includes the use of volatile variables, explicit locks, and atomic variables. If multiple threads access the same mutable state variable without appro- priate synchronization, your program is broken . There are three ways to fix it: Don\u2019t share the state variable across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable. It is far easier to design a class to be thread-safe than to retrofit it for thread safety later. When designing thread-safe classes, good object-oriented techniques\u2014encapsulation, immutability, and clear specification of invariants\u2014are your best friends. It is always a good practice first to make your code right, and then make it fast. Even then, pursue optimization only if your performance measurements and requirements tell you that you must, and if those same measurements tell you that your optimizations actually made a difference under realistic conditions. Is a thread-safe program one that is constructed entirely of thread-safe classes? Not necessarily\u2014a program that consists entirely of thread-safe classes may not be thread-safe, and a thread-safe program may contain classes that are not thread-safe. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state. Thread safety may be a term that is applied to code, but it is about state, and it can only be applied to the entire body of code that encapsulates its state, which may be an object or an entire program. What is thread safety? Formal definition A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations\u2014calls to public methods and reads or writes of public fields\u2014should be able to violate any of its invariants or postconditions. No set of operations performed sequentially or con- currently on instances of a thread-safe class can cause an instance to be in an invalid state. Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own. Example: A stateless servlet @ThreadSafe public class StatelessFactorizer implements Servlet { public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } The transient state for a particular computation exists solely in local variables that are stored on the thread\u2019s stack and are accessible only to the executing thread. One thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer ; because the two threads do not share state, it is as if they were accessing different instances. Since the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe. Stateless objects are always thread-safe. Atomicity Atomic means something that executes as a single, indivisible operation. What happens when we add one element of state to what was a stateless object? Suppose we want to add a \u201chit counter\u201d that measures the number of requests processed. The obvious approach is to add a long field to the servlet and increment it on each request: @NotThreadSafe public class UnsafeCountingFactorizer implements Servlet { private long count = 0 ; public long getCount () { return count ; } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } While the increment operation, ++count, may look like a single action because of its compact syntax, it is not atomic, which means that it does not execute as a single, indivisible operation. Instead, it is a shorthand for a sequence of three discrete operations: fetch the current value, add one to it, and write the new value back. This is an example of a read-modify-write operation, in which the resulting state is derived from the previous state. The possibility of incorrect results in the presence of unlucky timing is so important in concurrent programming that it has a name: a race condition . Race conditions A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime; in other words, when getting the right answer relies on lucky timing. The most common type of race condition is check-then-act , where a potentially stale observation is used to make a decision on what to do next. You observe something to be true (file X doesn\u2019t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption). Example: race conditions in lazy initialization A common idiom that uses check-then-act is lazy initialization. The goal of lazy initialization is to defer initializing an object until it is actually needed while at the same time ensuring that it is initialized only once. @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null ; public ExpensiveObject getInstance () { if ( instance == null ) { instance = new ExpensiveObject (); return instance ; } } } Now we have identified two sorts of race condition operations: Read-modify-write: like incrementing a counter (i.e.: count++; ), define a transformation of an object\u2019s state in terms of its previous state. Check-then-act: Check for a condition (i.e.: if (instance == null) ) and then act (i.e.: {instance = new ExpensiveObject(); ...} ) Like most concurrency errors, race conditions don\u2019t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems. Compound actions We refer collectively to check-then-act and read-modify-write sequences as compound actions: sequences of operations that must be executed atomically in order to remain thread-safe. Fixing UnsafeCountingFactorizer @ThreadSafe public class CountingFactorizer implements Servlet { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); count . incrementAndGet (); encodeIntoResponse ( resp , factors ); } } The java.util.concurrent.atomic package contains atomic variable classes for effecting atomic state transitions on numbers and object references. By replacing the long counter with an AtomicLong, we ensure that all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe, our servlet is once again thread-safe. Locking To preserve state consistency, update related state variables in a single atomic operation. Intrinsic locks Java provides a built-in locking mechanism for enforcing atomicity: the synchronized block. A synchronized block has two parts: a reference to an object that will serve as the lock, and a block of code to be guarded by that lock. A synchronized method is a shorthand for a synchronized block that spans an entire method body, and whose lock is the object on which the method is being invoked. (Static synchronized methods use the Class object for the lock.) synchronized ( lock ) { // Access or modify shared state guarded by lock } Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. The only way to acquire an intrinsic lock is to enter a synchronized block or method guarded by that lock. Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. No thread executing a synchronized block can observe another thread to be in the middle of a synchronized block guarded by the same lock. That is, the synchronized blocks guarded by the same lock execute atomically with respect to one another. In the context of concurrency, atomicity means the same thing as it does in transactional applications\u2014that a group of statements appear to execute as a single, indivisible unit. Reentrancy When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy facilitates encapsulation of locking behavior, and thus simplifies the development of object-oriented concurrent code. public class Widget { public synchronized void doSomething () { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething () { System . out . println ( toString () + \": calling doSomething\" ); super . doSomething (); } } Without reentrant locks, the very natural-looking code above, in which a subclass overrides a synchronized method and then calls the superclass method, would deadlock. Because the doSomething methods in Widget and LoggingWidget are both synchronized, each tries to acquire the lock on the Widget before proceeding. But if intrinsic locks were not reentrant, the call to super.doSomething would never be able to acquire the lock because it would be considered already held, and the thread would permanently stall waiting for a lock it can never acquire. Reentrancy saves us from deadlock in situations like this. Guarding state with locks Because locks enable serialized 1 access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following these protocols consistently can ensure state consistency. Compound actions on shared state, such as incrementing a hit counter ( read-modify-write ) or lazy initialization ( check-then-act ), must be made atomic to avoid race conditions. Holding a lock for the entire duration of a compound action can make that compound action atomic. However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed. It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Acquiring the lock associated with an object does not prevent other threads from accessing that object\u2014the only thing that acquiring a lock prevents any other thread from doing is acquiring that same lock. The fact that every object has a built-in lock is just a convenience so that you needn\u2019t explicitly create lock objects 2 . Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is. A common locking convention is to encapsulate all mutable state within an object and to protect it from concurrent access by synchronizing any code path that accesses mutable state using the object\u2019s intrinsic lock. Not all data needs to be guarded by locks\u2014only mutable data that will be accessed from multiple threads. When a variable is guarded by a lock\u2014meaning that every access to that variable is performed with that lock held\u2014you\u2019ve ensured that only one thread at a time can access that variable. When a class has invariants that involve more than one state variable, there is an additional requirement: each variable participating in the invariant must be guarded by the same lock. This allows you to access or update them in a single atomic operation, preserving the invariant. For every invariant that involves more than one variable, all the variables involved in that invariant must be guarded by the same lock. If synchronization is the cure for race conditions, why not just declare ev- ery method synchronized? if (! vector . contains ( element )) vector . add ( element ); This attempt at a put-if-absent operation has a race condition, even though both contains and add are atomic. While synchronized methods can make individual operations atomic, additional locking is required when multiple operations are combined into a compound action. At the same time, synchronizing every method can lead to liveness or performance problems. Liveness and performance Deciding how big or small to make synchronized blocks may require tradeoffs among competing design forces, including safety (which must not be compromised), simplicity, and performance. Sometimes simplicity and performance are at odds with each other, a reasonable balance can usually be found. Whenever you use locking, you should be aware of what the code in the block is doing and how likely it is to take a long time to execute. Holding a lock for a long time, either because you are doing something compute-intensive or because you execute a potentially blocking operation, introduces the risk of liveness or performance problems. Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O. Serializing access to an object has nothing to do with object serialization (turning an object into a byte stream); serializing access means that threads take turns accessing the object exclusively, rather than doing so concurrently. \u21a9 In retrospect, this design decision was probably a bad one: not only can it be confusing, but it forces JVM implementors to make tradeoffs between object size and locking performance. \u21a9","title":"Thread safety"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#2-thread-safety","text":"Surprisingly, concurrent programming isn\u2019t so much about threads or locks, these are just mechanisms\u2014means to an end. Writing thread-safe code is, at its core, about managing access to state , and in particular to shared, mutable state . Informally, an object\u2019s state is its data, stored in state variables such as instance or static fields. An object\u2019s state encompasses any data that can affect its externally visible behavior. By shared , we mean that a variable could be accessed by multiple threads; by mutable , we mean that its value could change during its lifetime. What we are really trying to do is protect data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads . This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state ; failing to do so could result in data corruption and other undesirable consequences. The primary mechanism for synchronization in Java is the synchronized keyword, which pro- vides exclusive locking, but the term \u201csynchronization\u201d also includes the use of volatile variables, explicit locks, and atomic variables. If multiple threads access the same mutable state variable without appro- priate synchronization, your program is broken . There are three ways to fix it: Don\u2019t share the state variable across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable. It is far easier to design a class to be thread-safe than to retrofit it for thread safety later. When designing thread-safe classes, good object-oriented techniques\u2014encapsulation, immutability, and clear specification of invariants\u2014are your best friends. It is always a good practice first to make your code right, and then make it fast. Even then, pursue optimization only if your performance measurements and requirements tell you that you must, and if those same measurements tell you that your optimizations actually made a difference under realistic conditions. Is a thread-safe program one that is constructed entirely of thread-safe classes? Not necessarily\u2014a program that consists entirely of thread-safe classes may not be thread-safe, and a thread-safe program may contain classes that are not thread-safe. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state. Thread safety may be a term that is applied to code, but it is about state, and it can only be applied to the entire body of code that encapsulates its state, which may be an object or an entire program.","title":"2. Thread safety"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#what-is-thread-safety","text":"","title":"What is thread safety?"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#formal-definition","text":"A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations\u2014calls to public methods and reads or writes of public fields\u2014should be able to violate any of its invariants or postconditions. No set of operations performed sequentially or con- currently on instances of a thread-safe class can cause an instance to be in an invalid state. Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own.","title":"Formal definition"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#example-a-stateless-servlet","text":"@ThreadSafe public class StatelessFactorizer implements Servlet { public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } The transient state for a particular computation exists solely in local variables that are stored on the thread\u2019s stack and are accessible only to the executing thread. One thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer ; because the two threads do not share state, it is as if they were accessing different instances. Since the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe. Stateless objects are always thread-safe.","title":"Example: A stateless servlet"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#atomicity","text":"Atomic means something that executes as a single, indivisible operation. What happens when we add one element of state to what was a stateless object? Suppose we want to add a \u201chit counter\u201d that measures the number of requests processed. The obvious approach is to add a long field to the servlet and increment it on each request: @NotThreadSafe public class UnsafeCountingFactorizer implements Servlet { private long count = 0 ; public long getCount () { return count ; } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } While the increment operation, ++count, may look like a single action because of its compact syntax, it is not atomic, which means that it does not execute as a single, indivisible operation. Instead, it is a shorthand for a sequence of three discrete operations: fetch the current value, add one to it, and write the new value back. This is an example of a read-modify-write operation, in which the resulting state is derived from the previous state. The possibility of incorrect results in the presence of unlucky timing is so important in concurrent programming that it has a name: a race condition .","title":"Atomicity"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#race-conditions","text":"A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime; in other words, when getting the right answer relies on lucky timing. The most common type of race condition is check-then-act , where a potentially stale observation is used to make a decision on what to do next. You observe something to be true (file X doesn\u2019t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption).","title":"Race conditions"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#example-race-conditions-in-lazy-initialization","text":"A common idiom that uses check-then-act is lazy initialization. The goal of lazy initialization is to defer initializing an object until it is actually needed while at the same time ensuring that it is initialized only once. @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null ; public ExpensiveObject getInstance () { if ( instance == null ) { instance = new ExpensiveObject (); return instance ; } } } Now we have identified two sorts of race condition operations: Read-modify-write: like incrementing a counter (i.e.: count++; ), define a transformation of an object\u2019s state in terms of its previous state. Check-then-act: Check for a condition (i.e.: if (instance == null) ) and then act (i.e.: {instance = new ExpensiveObject(); ...} ) Like most concurrency errors, race conditions don\u2019t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems.","title":"Example: race conditions in lazy initialization"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#compound-actions","text":"We refer collectively to check-then-act and read-modify-write sequences as compound actions: sequences of operations that must be executed atomically in order to remain thread-safe.","title":"Compound actions"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#fixing-unsafecountingfactorizer","text":"@ThreadSafe public class CountingFactorizer implements Servlet { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); count . incrementAndGet (); encodeIntoResponse ( resp , factors ); } } The java.util.concurrent.atomic package contains atomic variable classes for effecting atomic state transitions on numbers and object references. By replacing the long counter with an AtomicLong, we ensure that all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe, our servlet is once again thread-safe.","title":"Fixing UnsafeCountingFactorizer"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#locking","text":"To preserve state consistency, update related state variables in a single atomic operation.","title":"Locking"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#intrinsic-locks","text":"Java provides a built-in locking mechanism for enforcing atomicity: the synchronized block. A synchronized block has two parts: a reference to an object that will serve as the lock, and a block of code to be guarded by that lock. A synchronized method is a shorthand for a synchronized block that spans an entire method body, and whose lock is the object on which the method is being invoked. (Static synchronized methods use the Class object for the lock.) synchronized ( lock ) { // Access or modify shared state guarded by lock } Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. The only way to acquire an intrinsic lock is to enter a synchronized block or method guarded by that lock. Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. No thread executing a synchronized block can observe another thread to be in the middle of a synchronized block guarded by the same lock. That is, the synchronized blocks guarded by the same lock execute atomically with respect to one another. In the context of concurrency, atomicity means the same thing as it does in transactional applications\u2014that a group of statements appear to execute as a single, indivisible unit.","title":"Intrinsic locks"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#reentrancy","text":"When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy facilitates encapsulation of locking behavior, and thus simplifies the development of object-oriented concurrent code. public class Widget { public synchronized void doSomething () { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething () { System . out . println ( toString () + \": calling doSomething\" ); super . doSomething (); } } Without reentrant locks, the very natural-looking code above, in which a subclass overrides a synchronized method and then calls the superclass method, would deadlock. Because the doSomething methods in Widget and LoggingWidget are both synchronized, each tries to acquire the lock on the Widget before proceeding. But if intrinsic locks were not reentrant, the call to super.doSomething would never be able to acquire the lock because it would be considered already held, and the thread would permanently stall waiting for a lock it can never acquire. Reentrancy saves us from deadlock in situations like this.","title":"Reentrancy"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#guarding-state-with-locks","text":"Because locks enable serialized 1 access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following these protocols consistently can ensure state consistency. Compound actions on shared state, such as incrementing a hit counter ( read-modify-write ) or lazy initialization ( check-then-act ), must be made atomic to avoid race conditions. Holding a lock for the entire duration of a compound action can make that compound action atomic. However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed. It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Acquiring the lock associated with an object does not prevent other threads from accessing that object\u2014the only thing that acquiring a lock prevents any other thread from doing is acquiring that same lock. The fact that every object has a built-in lock is just a convenience so that you needn\u2019t explicitly create lock objects 2 . Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is. A common locking convention is to encapsulate all mutable state within an object and to protect it from concurrent access by synchronizing any code path that accesses mutable state using the object\u2019s intrinsic lock. Not all data needs to be guarded by locks\u2014only mutable data that will be accessed from multiple threads. When a variable is guarded by a lock\u2014meaning that every access to that variable is performed with that lock held\u2014you\u2019ve ensured that only one thread at a time can access that variable. When a class has invariants that involve more than one state variable, there is an additional requirement: each variable participating in the invariant must be guarded by the same lock. This allows you to access or update them in a single atomic operation, preserving the invariant. For every invariant that involves more than one variable, all the variables involved in that invariant must be guarded by the same lock. If synchronization is the cure for race conditions, why not just declare ev- ery method synchronized? if (! vector . contains ( element )) vector . add ( element ); This attempt at a put-if-absent operation has a race condition, even though both contains and add are atomic. While synchronized methods can make individual operations atomic, additional locking is required when multiple operations are combined into a compound action. At the same time, synchronizing every method can lead to liveness or performance problems.","title":"Guarding state with locks"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#liveness-and-performance","text":"Deciding how big or small to make synchronized blocks may require tradeoffs among competing design forces, including safety (which must not be compromised), simplicity, and performance. Sometimes simplicity and performance are at odds with each other, a reasonable balance can usually be found. Whenever you use locking, you should be aware of what the code in the block is doing and how likely it is to take a long time to execute. Holding a lock for a long time, either because you are doing something compute-intensive or because you execute a potentially blocking operation, introduces the risk of liveness or performance problems. Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O. Serializing access to an object has nothing to do with object serialization (turning an object into a byte stream); serializing access means that threads take turns accessing the object exclusively, rather than doing so concurrently. \u21a9 In retrospect, this design decision was probably a bad one: not only can it be confusing, but it forces JVM implementors to make tradeoffs between object size and locking performance. \u21a9","title":"Liveness and performance"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/","text":"3. Sharing objects Visibility It is a common misconception that synchronized is only about atomicity or demarcating \u201ccritical sections\u201d. Synchronization also has another significant, and subtle, aspect: memory visibility. We want not only to prevent one thread from modifying the state of an object when another is using it, but also to ensure that when a thread modifies the state of an object, other threads can actually see the changes that were made. But without synchronization, this may not happen. In a single-threaded environment, if you write a value to a variable and later read that variable with no intervening writes, you can expect to get the same value back. This seems only natural. But when the reads and writes occur in different threads, this is simply not the case. In general, there is no guarantee that the reading thread will see a value written by another thread \u201cat the right time\u201d basis, or even at all. In order to ensure visibility of memory writes across threads, you must use synchronization. public class NoVisibility { private static boolean ready ; private static int number ; /* yield() provides a mechanism to inform the \u201cscheduler\u201d that the current thread is willing to relinquish its current use of processor but it'd like to be scheduled back soon as possible. */ private static class ReaderThread extends Thread { public void run () { while (! ready ) Thread . yield (); System . out . println ( number ); } } public static void main ( String [] args ) { new ReaderThread (). start (); number = 42 ; ready = true ; } } Two threads, the main thread and the reader thread, access the shared variables ready and number. The main thread starts the reader thread and then sets number to 42 and ready to true. The reader thread spins until it sees ready is true, and then prints out number. While it may seem obvious that NoVisibility will print 42, it is in fact possible that it will print zero, or never terminate at all! Because it does not use adequate synchronization, there is no guarantee that the values of ready and number written by the main thread will be visible to the reader thread. NoVisibility could loop forever because the value of ready might never become visible to the reader thread. Even more strangely, NoVisibility could print zero because the write to ready might be made visible to the reader thread before the write to number, a phenomenon known as reordering. There is no guarantee that operations in one thread will be performed in the order given by the program, as long as the reordering is not detectable from within that thread\u2014even if the reordering is apparent to other threads 1 . When the main thread writes first to number and then to ready without synchronization, the reader thread could see those writes happen in the opposite order\u2014or not at all. In the absence of synchronization, the compiler, processor, and runtime can do some downright weird things to the order in which operations appear to execute. Attempts to reason about the order in which memory actions \u201cmust\u201d happen in insufficiently synchronized multithreaded programs will almost certainly be incorrect. Always use the proper synchronization whenever data is shared across threads. Stale data NoVisibility demonstrated one of the ways that insufficiently synchronized programs can cause surprising results: stale data. When the reader thread examines ready, it may see an out-of-date value. Unless synchronization is used every time a variable is accessed , it is possible to see a stale value for that variable. Worse, staleness is not all-or-nothing: a thread can see an up-to-date value of one variable but a stale value of another variable that was written first. Stale values can cause serious safety or liveness failures. In NoVisibility, stale values could cause it to print the wrong value or prevent the program from terminating. Things can get even more complicated with stale values of object references, such as the link pointers in a linked list implementation. Stale data can cause serious and confusing failures such as unexpected exceptions, corrupted data structures, inaccurate computations, and infinite loops. In a not thread-safe program where a value field is accessed from both get and set without synchronization it is susceptible to stale values: if one thread calls set, other threads calling get may or may not see that update. It can become safe by synchronizing the getter and setter. Synchronizing only the setter would not be sufficient: threads calling get would still be able to see stale values. Nonatomic 64-bit operations When a thread reads a variable without synchronization, it may see a stale value, but at least it sees a value that was actually placed there by some thread rather than some random value. This safety guarantee is called out-of-thin-air safety. Out-of-thin-air safety applies to all variables, with one exception: 64-bit numeric variables (double and long) that are not declared volatile. The Java Memory Model requires fetch and store operations to be atomic, but for nonvolatile long and double variables, the JVM is permitted to treat a 64-bit read or write as two separate 32-bit operations. If the reads and writes occur in different threads, it is therefore possible to read a nonvolatile long and get back the high 32 bits of one value and the low 32 bits of another 2 . Thus, even if you don\u2019t care about stale values, it is not safe to use shared mutable long and double variables in multithreaded programs unless they are declared volatile or guarded by a lock. Locking and visibility (piggybacking?) When thread A executes a synchronized block, and subsequently thread B enters a synchronized block guarded by the same lock, the values of variables that were visible to A prior to releasing the lock are guaranteed to be visible to B upon acquiring the lock. In other words, everything A did in or prior to a synchronized block is visible to B when it executes a synchronized block guarded by the same lock. Without synchronization, there is no such guarantee. Locking is not just about mutual exclusion; it is also about memory visibility. To ensure that all threads see the most up-to-date values of shared mutable variables, the reading and writing threads must synchronize on a common lock. Volatile variables The Java language also provides an alternative, weaker form of synchronization, volatile variables, to ensure that updates to a variable are propagated predictably to other threads. When a field is declared volatile, the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. Yet accessing a volatile variable performs no locking and so cannot cause the executing thread to block, making volatile variables a lighter-weight synchronization mechanism than synchronized 3 . So from a memory visibility perspective, writing a volatile variable is like exiting a synchronized block and reading a volatile variable is like entering a synchronized block. Use volatile variables only when they simplify implementing and veri- fying your synchronization policy; Good uses of volatile variables include ensuring the visibility of their own state, that of the object they refer to, or indicating that an important life-cycle event (such as initialization or shutdown) has occurred. volatile boolean asleep ; ... while (! asleep ) countSomeSheep (); For this example to work, the asleep flag must be volatile. Otherwise, the thread might not notice when asleep has been set by another thread. We could instead have used locking to ensure visibility of changes to asleep, but that would have made the code more cumbersome. Volatile variables are convenient, but they have limitations. The most common use for volatile variables is as a completion, interruption, or status flag, such as the asleep flag in above sample. Volatile variables can be used for other kinds of state information, but more care is required when attempting this. For example, the semantics of volatile are not strong enough to make the increment operation ( count++ ) atomic, unless you can guarantee that the variable is written only from a single thread. (Atomic variables do provide atomic read-modify-write support and can often be used as \u201cbetter volatile variables\u201d). Locking can guarantee both visibility and atomicity; volatile variables can only guarantee visibility. You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value. The variable does not participate in invariants with other state variables. Locking is not required for any other reason while the variable is being accessed. More about volatile Overview In the absence of necessary synchronizations, the compiler, runtime, or processors may apply all sorts of optimizations. Even though these optimizations are beneficial most of the time, sometimes they can cause subtle issues. Caching and reordering are among those optimizations that may surprise us in concurrent contexts. Java and the JVM provide many ways to control memory order, and the volatile keyword is one of them. Shared Multiprocessor Architecture Processors are responsible for executing program instructions. Therefore, they need to retrieve both program instructions and required data from RAM. As CPUs are capable of carrying out a significant number of instructions per second, fetching from RAM is not that ideal for them. To improve this situation, processors are using tricks like Out of Order Execution, Branch Prediction, Speculative Execution, and, of course, Caching. This is where the following memory hierarchy comes into play: As different cores execute more instructions and manipulate more data, they fill up their caches with more relevant data and instructions. This will improve the overall performance at the expense of introducing cache coherence challenges. Put simply, we should think twice about what happens when one thread updates a cached value. Memory Visibility (more about NoVisibility ) In this simple example, we have two application threads: the main thread and the reader thread. Let's imagine a scenario in which the OS schedules those threads on two different CPU cores, where: The main thread has its copy of ready and number variables in its core cache The reader thread ends up with its copies, too The main thread updates the cached values On most modern processors, write requests won't be applied right away after they're issued. In fact, processors tend to queue those writes in a special write buffer. After a while, they will apply those writes to main memory all at once. With all that being said, when the main thread updates the number and ready variables, there is no guarantee about what the reader thread may see. In other words, the reader thread may see the updated value right away, or with some delay, or never at all! This memory visibility may cause liveness issues in programs that are relying on visibility. Reordering To make matters even worse, the reader thread may see those writes in any order other than the actual program order. For instance, since we first update the number variable: public static void main ( String [] args ) { new Reader (). start (); number = 42 ; ready = true ; } We may expect the reader thread prints 42. However, it's actually possible to see zero as the printed value! The reordering is an optimization technique for performance improvements. Interestingly, different components may apply this optimization: The processor may flush its write buffer in any order other than the program order The processor may apply out-of-order execution technique The JIT compiler may optimize via reordering To ensure that updates to variables propagate predictably to other threads, we should apply the volatile modifier to those variables. This way, we communicate with runtime and processor to not reorder any instruction involving the volatile variable. Also, processors understand that they should flush any updates to these variables right away. volatile and Thread Synchronization For multithreaded applications, we need to ensure a couple of rules for consistent behavior: Mutual Exclusion \u2013 only one thread executes a critical section at a time Visibility \u2013 changes made by one thread to the shared data are visible to other threads to maintain data consistency synchronized methods and blocks provide both of the above properties, at the cost of application performance. volatile is quite a useful keyword because it can help ensure the visibility aspect of the data change without, of course, providing mutual exclusion. Thus, it's useful in the places where we're ok with multiple threads executing a block of code in parallel, but we need to ensure the visibility property. Happens-Before Ordering The memory visibility effects of volatile variables extend beyond the volatile variables themselves. To make matters more concrete, let's suppose thread A writes to a volatile variable, and then thread B reads the same volatile variable. In such cases, the values that were visible to A before writing the volatile variable will be visible to B after reading the volatile variable. Piggybacking Because of the strength of the happens-before memory ordering, sometimes we can piggyback on the visibility properties of another volatile variable. For instance, in our particular example, we just need to mark the ready variable as volatile: public class TaskRunner { private static int number ; // not volatile private volatile static boolean ready ; // same as before } Anything prior to writing true to the ready variable is visible to anything after reading the ready variable. Therefore, the number variable piggybacks on the memory visibility enforced by the ready variable. Put simply, even though it's not a volatile variable, it is exhibiting a volatile behavior. By making use of these semantics, we can define only a few of the variables in our class as volatile and optimize the visibility guarantee. Publication and escape Publishing an object means making it available to code outside of its current scope, such as by storing a reference to it where other code can find it, returning it from a nonprivate method, or passing it to a method in another class. In many situations, we want to ensure that objects and their internals are not published. In other situations, we do want to publish an object for general use, but doing so in a thread-safe manner may require synchronization. Publishing internal state variables can compromise encapsulation and make it more difficult to preserve invariants; publishing objects before they are fully constructed can compromise thread safety. An object that is published when it should not have been is said to have escaped. The most blatant form of publication is to store a reference in a public static field, where any class and thread could see it: public static Set < Secret > knownSecrets ; public void initialize () { knownSecrets = new HashSet < Secret >(); } // Don't do this class UnsafeStates { private String [] states = new String [] { \"AK\" , \"AL\" ... }; public String [] getStates () { return states ; } } Publishing one object may indirectly publish others. If you add a Secret to the published knownSecrets set, you\u2019ve also published that Secret, because any code can iterate the Set and obtain a reference to the new Secret. Publishing states in UnsafeStates way is problematic because any caller can modify its contents. In this case, the states array has escaped its intended scope, because what was supposed to be private state has been effectively made public. Whether another thread actually does something with a published reference doesn\u2019t really matter, because the risk of misuse is still present. Once an object escapes, you have to assume that another class or thread may, maliciously or carelessly, misuse it. This is a compelling reason to use encapsulation: it makes it practical to analyze programs for correctness and harder to violate design constraints accidentally. A final mechanism by which an object or its internal state can be published is to publish an inner class instance: public class ThisEscape { // Don't do this public ThisEscape ( EventSource source ) { source . registerListener ( new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }); } } } When ThisEscape publishes the EventListener , it implicitly publishes the enclosing ThisEscape instance as well, because inner class instances contain a hidden reference to the enclosing instance. Safe construction practices ThisEscape illustrates an important special case of escape\u2014when the this references escapes during construction. When the inner EventListener instance is published, so is the enclosing ThisEscape instance. But an object is in a predictable, consistent state only after its constructor returns, so publishing an object from within its constructor can publish an incompletely constructed object. This is true even if the publication is the last statement in the constructor. If the this reference escapes during construction, the object is considered not properly constructed. Do not allow the this reference to escape during construction. A common mistake that can let the this reference escape during construction is to start a thread from a constructor. When an object creates a thread from its constructor, it almost always shares its this reference with the new thread, either explicitly (by passing it to the constructor) or implicitly (because the Thread or Runnable is an inner class of the owning object). The new thread might then be able to see the owning object before it is fully constructed. There\u2019s nothing wrong with creating a thread in a constructor, but it is best not to start the thread immediately. Instead, expose a start or initialize method that starts the owned thread. Calling an overrideable instance method (one that is neither private nor final) from the constructor can also allow the this reference to escape. If you are tempted to register an event listener or start a thread from a constructor, you can avoid the improper construction by using a private constructor and a public factory method. public class SafeListener { private final EventListener listener ; private SafeListener () { listener = new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }; } public static SafeListener newInstance ( EventSource source ) { SafeListener safe = new SafeListener (); source . registerListener ( safe . listener ); return safe ; } } Thread confinement Accessing shared, mutable data requires using synchronization; one way to avoid this requirement is to not share. If data is only accessed from a single thread, no synchronization is needed. This technique, thread confinement, is one of the simplest ways to achieve thread safety. When an object is confined to a thread, such usage is automatically thread-safe even if the confined object itself is not. Examples: Swing: The Swing visual components and data model objects are not thread safe; instead, safety is achieved by confining them to the Swing event dispatch thread. To use Swing properly, code running in threads other than the event thread should not access these objects. (To make this easier, Swing provides the invokeLater mechanism to schedule a Runnable for execution in the event thread.) Many concurrency errors in Swing applications stem from improper use of these confined objects from another thread. Pooled JDBC Connection objects: The JDBC specification does not require that Connection objects be thread-safe 4 . In typical server applications, a thread acquires a connection from the pool, uses it for processing a single request, and returns it. Since most requests, such as servlet requests or EJB (Enterprise JavaBeans) calls, are processed synchronously by a single thread, and the pool will not dispense the same connection to another thread until it has been returned, this pattern of connection management implicitly confines the Connection to that thread for the duration of the request. Just as the language has no mechanism for enforcing that a variable is guarded by a lock, it has no means of confining an object to a thread. Thread confinement is an element of your program\u2019s design that must be enforced by its implementation. The language and core libraries provide mechanisms that can help in maintaining thread confinement\u2014local variables and the ThreadLocal class\u2014but even with these, it is still the programmer\u2019s responsibility to ensure that thread-confined objects do not escape from their intended thread. Ad-hoc thread confinement Ad-hoc thread confinement describes when the responsibility for maintaining thread confinement falls entirely on the implementation. Ad-hoc thread confinement can be fragile because none of the language features, such as visibility modifiers or local variables, helps confine the object to the target thread. In fact, references to thread-confined objects such as visual components or data models in GUI applications are often held in public fields. The decision to use thread confinement is often a consequence of the decision to implement a particular subsystem, such as the GUI, as a single-threaded sub- system. Single-threaded subsystems can sometimes offer a simplicity benefit that outweighs the fragility of ad-hoc thread confinement 5 . A special case of thread confinement applies to volatile variables. It is safe to perform read-modify-write operations on shared volatile variables as long as you ensure that the volatile variable is only written from a single thread. In this case, you are confining the modification to a single thread to prevent race conditions, and the visibility guarantees for volatile variables ensure that other threads see the most up-to-date value. Because of its fragility, ad-hoc thread confinement should be used sparingly; if possible, use one of the stronger forms of thread confinment (stack confinement or ThreadLocal) instead. Stack confinement Stack confinement is a special case of thread confinement in which an object can only be reached through local variables. Just as encapsulation can make it easier to preserve invariants, local variables can make it easier to confine objects to a thread. Local variables are intrinsically confined to the executing thread; they exist on the executing thread\u2019s stack, which is not accessible to other threads. Stack confinement (also called within-thread or thread-local usage, but not to be confused with the ThreadLocal library class) is simpler to maintain and less fragile than ad-hoc thread confinement. For primitively typed local variables, such as numPairs in loadTheArk in below sample you cannot violate stack confinement even if you tried. There is no way to obtain a reference to a primitive variable, so the language semantics ensure that primitive local variables are always stack confined. public int loadTheArk ( Collection < Animal > candidates ) { SortedSet < Animal > animals ; int numPairs = 0 ; Animal candidate = null ; // animals confined to method, don\u2019t let them escape! animals = new TreeSet < Animal >( new SpeciesGenderComparator ()); animals . addAll ( candidates ); for ( Animal a : animals ) { if ( candidate == null || ! candidate . isPotentialMate ( a )) candidate = a ; else { ark . load ( new AnimalPair ( candidate , a )); ++ numPairs ; candidate = null ; } } return numPairs ; } In loadTheArk, we instantiate a TreeSet and store a reference to it in animals. At this point, there is exactly one reference to the Set, held in a local variable and therefore confined to the executing thread. However, if we were to publish a reference to the Set (or any of its internals), the confinement would be violated and the animals would escape. The design requirement that the object be confined to the executing thread, or the awareness that the confined object is not thread-safe, often exists only in the head of the developer when the code is written. If the assumption of within-thread usage is not clearly documented, future maintainers might mistakenly allow the object to escape. ThreadLocal (typically private static fields) A more formal means of maintaining thread confinement is ThreadLocal, which allows you to associate a per-thread value with a value-holding object. ThreadLocal provides get and set accessor methods that maintain a separate copy of the value for each thread that uses it, so a get returns the most recent value passed to set from the currently executing thread. Thread-local variables are often used to prevent sharing in designs based on mutable Singletons or global variables. For example, a single-threaded application might maintain a global database connection that is initialized at startup to avoid having to pass a Connection to every method. Since JDBC connections may not be thread-safe, a multithreaded application that uses a global connection without additional coordination is not thread-safe either. By using a ThreadLocal to store the JDBC connection, as in ConnectionHolder in below sample, each thread will have its own connection. private static ThreadLocal < Connection > connectionHolder = new ThreadLocal < Connection >() { public Connection initialValue () { return DriverManager . getConnection ( DB_URL ); } }; public static Connection getConnection () { return connectionHolder . get (); } When a thread calls ThreadLocal.get for the first time, initialValue is consulted to provide the initial value for that thread. The thread-specific values are stored in the Thread object itself; when the thread terminates, the thread-specific values can be garbage collected. If you are porting a single-threaded application to a multithreaded environment, you can preserve thread safety by converting shared global variables into ThreadLocals, if the semantics of the shared globals permits this; an application- wide cache would not be as useful if it were turned into a number of thread-local caches. It is easy to abuse ThreadLocal by treating its thread confinement property as a license to use global variables or as a means of creating \u201chidden\u201d method arguments. Like global variables, thread-local variables can detract from reusability and introduce hidden couplings among classes, and should therefore be used with care. Immutability If an object\u2019s state cannot be modified, these risks and complexities simply go away. An immutable object is one whose state cannot be changed after construction. Immutable objects are inherently thread-safe; their invariants are established by the constructor, and if their state cannot be changed, these invariants always hold. Immutable objects are always thread-safe. Immutable objects are simple. They can only be in one state, which is carefully controlled by the constructor. One of the most difficult elements of program design is reasoning about the possible states of complex objects. Reasoning about the state of immutable objects, on the other hand, is trivial. Immutable objects are also safer. Passing a mutable object to untrusted code, or otherwise publishing it where untrusted code could find it, is dangerous\u2014the untrusted code might modify its state, or, worse, retain a reference to it and modify its state later from another thread. On the other hand, immutable objects cannot be subverted in this manner by malicious or buggy code, so they are safe to share and publish freely without the need to make defensive copies. Neither the Java Language Specification nor the Java Memory Model formally defines immutability, but immutability is not equivalent to simply declaring all fields of an object final. An object whose fields are all final may still be mutable, since final fields can hold references to mutable objects. An object is immutable if: Its state cannot be modified after construction. All its fields are final. It is properly constructed (the this reference does not escape during construction). While the Set that stores the names is mutable, the design of ThreeStooges makes it impossible to modify that Set after construction. The stooges reference is final, so all object state is reached through a final field. The last requirement, proper construction, is easily met since the constructor does nothing that would cause the this reference to become accessible to code other than the constructor and its caller. @Immutable public final class ThreeStooges { private final Set < String > stooges = new HashSet < String >(); public ThreeStooges () { stooges . add ( \"Moe\" ); stooges . add ( \"Larry\" ); stooges . add ( \"Curly\" ); } public boolean isStooge ( String name ) { return stooges . contains ( name ); } } Final fields The final keyword, a more limited version of the const mechanism from C++, supports the construction of immutable objects. Final fields can\u2019t be modified (although the objects they refer to can be modified if they are mutable), but they also have special semantics under the Java Memory Model. It is the use of final fields that makes possible the guarantee of initialization safety that lets immutable objects be freely accessed and shared without synchronization. Even if an object is mutable, making some fields final can still simplify reasoning about its state, since limiting the mutability of an object restricts its set of possible states. An object that is \u201cmostly immutable\u201d but has one or two mutable state variables is still simpler than one that has many mutable variables. Declaring fields final also documents to maintainers that these fields are not expected to change. Just as it is a good practice to make all fields private unless they need greater visibility, it is a good practice to make all fields final unless they need to be mutable. Example: Using volatile to publish immutable objects Immutable objects can sometimes provide a weak form of atomicity. The factoring servlet performs two operations that must be atomic: updating the cached result and conditionally fetching the cached factors if the cached number matches the requested number. Whenever a group of related data items must be acted on atomically, consider creating an immutable holder class for them, such as OneValueCache 6 : @Immutable class OneValueCache { private final BigInteger lastNumber ; private final BigInteger [] lastFactors ; public OneValueCache ( BigInteger i , BigInteger [] factors ) { lastNumber = i ; lastFactors = Arrays . copyOf ( factors , factors . length ); } public BigInteger [] getFactors ( BigInteger i ) { if ( lastNumber == null || ! lastNumber . equals ( i )) return null ; else return Arrays . copyOf ( lastFactors , lastFactors . length ); } } Race conditions in accessing or updating multiple related variables can be eliminated by using an immutable object to hold all the variables. With a mutable holder object, you would have to use locking to ensure atomicity; with an im- mutable one, once a thread acquires a reference to it, it need never worry about another thread modifying its state. If the variables are to be updated, a new holder object is created, but any threads working with the previous holder still see it in a consistent state. VolatileCachedFactorizer uses a OneValueCache to store the cached number and factors. When a thread sets the volatile cache field to reference a new OneValueCache , the new cached data becomes immediately visible to other threads. The cache-related operations cannot interfere with each other because OneValueCache is immutable and the cache field is accessed only once in each of the relevant code paths. This combination of an immutable holder object for multiple state variables related by an invariant, and a volatile reference used to ensure its timely visibility, allows VolatileCachedFactorizer to be thread-safe even though it does no explicit locking. @ThreadSafe public class VolatileCachedFactorizer implements Servlet { private volatile OneValueCache cache = new OneValueCache ( null , null ); public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = cache . getFactors ( i ); if ( factors == null ) { factors = factor ( i ); cache = new OneValueCache ( i , factors ); } encodeIntoResponse ( resp , factors ); } } Safe publication Sometimes we do want to share objects across threads, and in this case we must do so safely. Unfortunately, simply storing a reference to an object into a public field, is not enough to publish that object safely. // Unsafe publication public Holder holder ; public void initialize () { holder = new Holder ( 42 ); } Because of visibility problems, the Holder could appear to another thread to be in an inconsistent state, even though its invariants were properly established by its constructor! This improper publication could allow another thread to observe a partially constructed object. Improper publication: when good objects go bad You cannot rely on the integrity of partially constructed objects. An observing thread could see the object in an inconsistent state, and then later see its state suddenly change, even though it has not been modified since publication 7 . Because synchronization was not used to make the Holder visible to other threads, we say the Holder was not properly published. Other threads could see a stale value for the holder field, and thus see a null reference or other older value even though a value has been placed in holder. But far worse, other threads could see an up-to-date value for the holder reference, but stale values for the state of the Holder. To make things even less predictable, a thread may see a stale value the first time it reads a field and then a more up-to-date value the next time. Immutable objects and initialization safety Because immutable objects are so important, the Java Memory Model offers a special guarantee of initialization safety for sharing immutable objects. As we\u2019ve seen, that an object reference becomes visible to another thread does not necessarily mean that the state of that object is visible to the consuming thread. In order to guarantee a consistent view of the object\u2019s state, synchronization is needed. Immutable objects, on the other hand, can be safely accessed even when synchronization is not used to publish the object reference. For this guarantee of initialization safety to hold, all of the requirements for immutability must be met: unmodifiable state, all fields are final, and proper construction. Immutable objects can be used safely by any thread without additional synchronization, even when synchronization is not used to publish them. This guarantee extends to the values of all final fields of properly constructed objects; final fields can be safely accessed without additional synchronization. However, if final fields refer to mutable objects, synchronization is still required to access the state of the objects they refer to. Safe publication idioms Objects that are not immutable must be safely published, which usually entails synchronization by both the publishing and the consuming thread. To publish an object safely, both the reference to the object and the object\u2019s state must be made visible to other threads at the same time. A properly constructed object can be safely published by: Initializing an object reference from a static initializer; Storing a reference to it into a volatile field or AtomicReference; Storing a reference to it into a final field of a properly constructed object; Storing a reference to it into a field that is properly guarded by a lock. The internal synchronization in thread-safe collections means that placing an object in a thread-safe collection, such as a Vector or synchronizedList, fulfills the last of these requirements. If thread A places object X in a thread-safe collection and thread B subsequently retrieves it, B is guaranteed to see the state of X as A left it, even though the application code that hands X off in this manner has no explicit synchronization. The thread-safe library collections offer the following safe publication guarantees, even if the Javadoc is less than clear on the subject: Placing a key or value in a Hashtable, synchronizedMap, or ConcurrentMap safely publishes it to any thread that retrieves it from the Map (whether directly or via an iterator); Placing an element in a Vector, CopyOnWriteArrayList, CopyOnWriteArraySet, synchronizedList, or synchronizedSet safely publishes it to any thread that retrieves it from the collection; Placing an element on a BlockingQueue or a ConcurrentLinkedQueue safely publishes it to any thread that retrieves it from the queue. Using a static initializer is often the easiest and safest way to publish objects that can be statically constructed: public static Holder holder = new Holder(42); Static initializers are executed by the JVM at class initialization time; because of internal synchronization in the JVM, this mechanism is guaranteed to safely publish any objects initialized in this way Effectively immutable objects The safe publication mechanisms all guarantee that the as-published state of an object is visible to all accessing threads as soon as the reference to it is visible, and if that state is not going to be changed again, this is sufficient to ensure that any access is safe. Objects that are not technically immutable, but whose state will not be modified after publication, are called effectively immutable. They do not need to meet the strict definition of immutability; they merely need to be treated by the program as if they were immutable after they are published. Using effectively immutable objects can simplify development and improve performance by reducing the need for synchronization. Safely published effectively immutable objects can be used safely by any thread without additional synchronization. Mutable objects If an object may be modified after construction, safe publication ensures only the visibility of the as-published state. Synchronization must be used not only to publish a mutable object, but also every time the object is accessed to ensure visibility of subsequent modifications. To share mutable objects safely, they must be safely published and be either thread-safe or guarded by a lock. The publication requirements for an object depend on its mutability: Immutable objects can be published through any mechanism. Effectively immutable objects must be safely published. Mutable objects must be safely published, and must be either thread-safe or guarded by a lock. Sharing objects safely Whenever you acquire a reference to an object, you should know what you are allowed to do with it. Do you need to acquire a lock before using it? Are you allowed to modify its state, or only to read it? Many concurrency errors stem from failing to understand these \u201crules of engagement\u201d for a shared object. When you publish an object, you should document how the object can be accessed. The most useful policies for using and sharing objects in a concurrent program are: Thread-confined. A thread-confined object is owned exclusively by and confined to one thread, and can be modified by its owning thread. Shared read-only. A shared read-only object can be accessed concurrently by multiple threads without additional synchronization, but cannot be modified by any thread. Shared read-only objects include immutable and effectively immutable objects. Shared thread-safe. A thread-safe object performs synchronization internally, so multiple threads can freely access it through its public interface without further synchronization. Guarded. A guarded object can be accessed only with a specific lock held. Guarded objects include those that are encapsulated within other thread-safe objects and published objects that are known to be guarded by a specific lock. This may seem like a broken design, but it is meant to allow JVMs to take full advantage of the performance of modern multiprocessor hardware. For example, in the absence of synchronization, the Java Memory Model permits the compiler to reorder operations and cache values in registers, and permits CPUs to reorder operations and cache values in processor-specific caches. \u21a9 When the Java Virtual Machine Specification was written, many widely used processor architectures could not efficiently provide atomic 64-bit arithmetic operations. \u21a9 Volatile reads are only slightly more expensive than nonvolatile reads on most current processor architectures. \u21a9 The connection pool implementations provided by application servers are thread-safe; connection pools are necessarily accessed from multiple threads, so a non-thread-safe implementation would not make sense. \u21a9 Another reason to make a subsystem single-threaded is deadlock avoidance; this is one of the primary reasons most GUI frameworks are single-threaded. \u21a9 OneValueCache wouldn\u2019t be immutable without the copyOf calls in the constructor and getter. Arrays.copyOf was added as a convenience in Java 6; clone would also work. \u21a9 The problem here is not the Holder class itself, but that the Holder is not properly published. However, Holder can be made immune to improper publication by declaring the n field to be final which would make Holder immutable. \u21a9","title":"Sharing objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#3-sharing-objects","text":"","title":"3. Sharing objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#visibility","text":"It is a common misconception that synchronized is only about atomicity or demarcating \u201ccritical sections\u201d. Synchronization also has another significant, and subtle, aspect: memory visibility. We want not only to prevent one thread from modifying the state of an object when another is using it, but also to ensure that when a thread modifies the state of an object, other threads can actually see the changes that were made. But without synchronization, this may not happen. In a single-threaded environment, if you write a value to a variable and later read that variable with no intervening writes, you can expect to get the same value back. This seems only natural. But when the reads and writes occur in different threads, this is simply not the case. In general, there is no guarantee that the reading thread will see a value written by another thread \u201cat the right time\u201d basis, or even at all. In order to ensure visibility of memory writes across threads, you must use synchronization. public class NoVisibility { private static boolean ready ; private static int number ; /* yield() provides a mechanism to inform the \u201cscheduler\u201d that the current thread is willing to relinquish its current use of processor but it'd like to be scheduled back soon as possible. */ private static class ReaderThread extends Thread { public void run () { while (! ready ) Thread . yield (); System . out . println ( number ); } } public static void main ( String [] args ) { new ReaderThread (). start (); number = 42 ; ready = true ; } } Two threads, the main thread and the reader thread, access the shared variables ready and number. The main thread starts the reader thread and then sets number to 42 and ready to true. The reader thread spins until it sees ready is true, and then prints out number. While it may seem obvious that NoVisibility will print 42, it is in fact possible that it will print zero, or never terminate at all! Because it does not use adequate synchronization, there is no guarantee that the values of ready and number written by the main thread will be visible to the reader thread. NoVisibility could loop forever because the value of ready might never become visible to the reader thread. Even more strangely, NoVisibility could print zero because the write to ready might be made visible to the reader thread before the write to number, a phenomenon known as reordering. There is no guarantee that operations in one thread will be performed in the order given by the program, as long as the reordering is not detectable from within that thread\u2014even if the reordering is apparent to other threads 1 . When the main thread writes first to number and then to ready without synchronization, the reader thread could see those writes happen in the opposite order\u2014or not at all. In the absence of synchronization, the compiler, processor, and runtime can do some downright weird things to the order in which operations appear to execute. Attempts to reason about the order in which memory actions \u201cmust\u201d happen in insufficiently synchronized multithreaded programs will almost certainly be incorrect. Always use the proper synchronization whenever data is shared across threads.","title":"Visibility"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#stale-data","text":"NoVisibility demonstrated one of the ways that insufficiently synchronized programs can cause surprising results: stale data. When the reader thread examines ready, it may see an out-of-date value. Unless synchronization is used every time a variable is accessed , it is possible to see a stale value for that variable. Worse, staleness is not all-or-nothing: a thread can see an up-to-date value of one variable but a stale value of another variable that was written first. Stale values can cause serious safety or liveness failures. In NoVisibility, stale values could cause it to print the wrong value or prevent the program from terminating. Things can get even more complicated with stale values of object references, such as the link pointers in a linked list implementation. Stale data can cause serious and confusing failures such as unexpected exceptions, corrupted data structures, inaccurate computations, and infinite loops. In a not thread-safe program where a value field is accessed from both get and set without synchronization it is susceptible to stale values: if one thread calls set, other threads calling get may or may not see that update. It can become safe by synchronizing the getter and setter. Synchronizing only the setter would not be sufficient: threads calling get would still be able to see stale values.","title":"Stale data"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#nonatomic-64-bit-operations","text":"When a thread reads a variable without synchronization, it may see a stale value, but at least it sees a value that was actually placed there by some thread rather than some random value. This safety guarantee is called out-of-thin-air safety. Out-of-thin-air safety applies to all variables, with one exception: 64-bit numeric variables (double and long) that are not declared volatile. The Java Memory Model requires fetch and store operations to be atomic, but for nonvolatile long and double variables, the JVM is permitted to treat a 64-bit read or write as two separate 32-bit operations. If the reads and writes occur in different threads, it is therefore possible to read a nonvolatile long and get back the high 32 bits of one value and the low 32 bits of another 2 . Thus, even if you don\u2019t care about stale values, it is not safe to use shared mutable long and double variables in multithreaded programs unless they are declared volatile or guarded by a lock.","title":"Nonatomic 64-bit operations"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#locking-and-visibility-piggybacking","text":"When thread A executes a synchronized block, and subsequently thread B enters a synchronized block guarded by the same lock, the values of variables that were visible to A prior to releasing the lock are guaranteed to be visible to B upon acquiring the lock. In other words, everything A did in or prior to a synchronized block is visible to B when it executes a synchronized block guarded by the same lock. Without synchronization, there is no such guarantee. Locking is not just about mutual exclusion; it is also about memory visibility. To ensure that all threads see the most up-to-date values of shared mutable variables, the reading and writing threads must synchronize on a common lock.","title":"Locking and visibility (piggybacking?)"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#volatile-variables","text":"The Java language also provides an alternative, weaker form of synchronization, volatile variables, to ensure that updates to a variable are propagated predictably to other threads. When a field is declared volatile, the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. Yet accessing a volatile variable performs no locking and so cannot cause the executing thread to block, making volatile variables a lighter-weight synchronization mechanism than synchronized 3 . So from a memory visibility perspective, writing a volatile variable is like exiting a synchronized block and reading a volatile variable is like entering a synchronized block. Use volatile variables only when they simplify implementing and veri- fying your synchronization policy; Good uses of volatile variables include ensuring the visibility of their own state, that of the object they refer to, or indicating that an important life-cycle event (such as initialization or shutdown) has occurred. volatile boolean asleep ; ... while (! asleep ) countSomeSheep (); For this example to work, the asleep flag must be volatile. Otherwise, the thread might not notice when asleep has been set by another thread. We could instead have used locking to ensure visibility of changes to asleep, but that would have made the code more cumbersome. Volatile variables are convenient, but they have limitations. The most common use for volatile variables is as a completion, interruption, or status flag, such as the asleep flag in above sample. Volatile variables can be used for other kinds of state information, but more care is required when attempting this. For example, the semantics of volatile are not strong enough to make the increment operation ( count++ ) atomic, unless you can guarantee that the variable is written only from a single thread. (Atomic variables do provide atomic read-modify-write support and can often be used as \u201cbetter volatile variables\u201d). Locking can guarantee both visibility and atomicity; volatile variables can only guarantee visibility. You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value. The variable does not participate in invariants with other state variables. Locking is not required for any other reason while the variable is being accessed.","title":"Volatile variables"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#more-about-volatile","text":"","title":"More about volatile"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#overview","text":"In the absence of necessary synchronizations, the compiler, runtime, or processors may apply all sorts of optimizations. Even though these optimizations are beneficial most of the time, sometimes they can cause subtle issues. Caching and reordering are among those optimizations that may surprise us in concurrent contexts. Java and the JVM provide many ways to control memory order, and the volatile keyword is one of them.","title":"Overview"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#shared-multiprocessor-architecture","text":"Processors are responsible for executing program instructions. Therefore, they need to retrieve both program instructions and required data from RAM. As CPUs are capable of carrying out a significant number of instructions per second, fetching from RAM is not that ideal for them. To improve this situation, processors are using tricks like Out of Order Execution, Branch Prediction, Speculative Execution, and, of course, Caching. This is where the following memory hierarchy comes into play: As different cores execute more instructions and manipulate more data, they fill up their caches with more relevant data and instructions. This will improve the overall performance at the expense of introducing cache coherence challenges. Put simply, we should think twice about what happens when one thread updates a cached value.","title":"Shared Multiprocessor Architecture"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#memory-visibility-more-about-novisibility","text":"In this simple example, we have two application threads: the main thread and the reader thread. Let's imagine a scenario in which the OS schedules those threads on two different CPU cores, where: The main thread has its copy of ready and number variables in its core cache The reader thread ends up with its copies, too The main thread updates the cached values On most modern processors, write requests won't be applied right away after they're issued. In fact, processors tend to queue those writes in a special write buffer. After a while, they will apply those writes to main memory all at once. With all that being said, when the main thread updates the number and ready variables, there is no guarantee about what the reader thread may see. In other words, the reader thread may see the updated value right away, or with some delay, or never at all! This memory visibility may cause liveness issues in programs that are relying on visibility.","title":"Memory Visibility (more about NoVisibility)"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#reordering","text":"To make matters even worse, the reader thread may see those writes in any order other than the actual program order. For instance, since we first update the number variable: public static void main ( String [] args ) { new Reader (). start (); number = 42 ; ready = true ; } We may expect the reader thread prints 42. However, it's actually possible to see zero as the printed value! The reordering is an optimization technique for performance improvements. Interestingly, different components may apply this optimization: The processor may flush its write buffer in any order other than the program order The processor may apply out-of-order execution technique The JIT compiler may optimize via reordering To ensure that updates to variables propagate predictably to other threads, we should apply the volatile modifier to those variables. This way, we communicate with runtime and processor to not reorder any instruction involving the volatile variable. Also, processors understand that they should flush any updates to these variables right away.","title":"Reordering"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#volatile-and-thread-synchronization","text":"For multithreaded applications, we need to ensure a couple of rules for consistent behavior: Mutual Exclusion \u2013 only one thread executes a critical section at a time Visibility \u2013 changes made by one thread to the shared data are visible to other threads to maintain data consistency synchronized methods and blocks provide both of the above properties, at the cost of application performance. volatile is quite a useful keyword because it can help ensure the visibility aspect of the data change without, of course, providing mutual exclusion. Thus, it's useful in the places where we're ok with multiple threads executing a block of code in parallel, but we need to ensure the visibility property.","title":"volatile and Thread Synchronization"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#happens-before-ordering","text":"The memory visibility effects of volatile variables extend beyond the volatile variables themselves. To make matters more concrete, let's suppose thread A writes to a volatile variable, and then thread B reads the same volatile variable. In such cases, the values that were visible to A before writing the volatile variable will be visible to B after reading the volatile variable.","title":"Happens-Before Ordering"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#piggybacking","text":"Because of the strength of the happens-before memory ordering, sometimes we can piggyback on the visibility properties of another volatile variable. For instance, in our particular example, we just need to mark the ready variable as volatile: public class TaskRunner { private static int number ; // not volatile private volatile static boolean ready ; // same as before } Anything prior to writing true to the ready variable is visible to anything after reading the ready variable. Therefore, the number variable piggybacks on the memory visibility enforced by the ready variable. Put simply, even though it's not a volatile variable, it is exhibiting a volatile behavior. By making use of these semantics, we can define only a few of the variables in our class as volatile and optimize the visibility guarantee.","title":"Piggybacking"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#publication-and-escape","text":"Publishing an object means making it available to code outside of its current scope, such as by storing a reference to it where other code can find it, returning it from a nonprivate method, or passing it to a method in another class. In many situations, we want to ensure that objects and their internals are not published. In other situations, we do want to publish an object for general use, but doing so in a thread-safe manner may require synchronization. Publishing internal state variables can compromise encapsulation and make it more difficult to preserve invariants; publishing objects before they are fully constructed can compromise thread safety. An object that is published when it should not have been is said to have escaped. The most blatant form of publication is to store a reference in a public static field, where any class and thread could see it: public static Set < Secret > knownSecrets ; public void initialize () { knownSecrets = new HashSet < Secret >(); } // Don't do this class UnsafeStates { private String [] states = new String [] { \"AK\" , \"AL\" ... }; public String [] getStates () { return states ; } } Publishing one object may indirectly publish others. If you add a Secret to the published knownSecrets set, you\u2019ve also published that Secret, because any code can iterate the Set and obtain a reference to the new Secret. Publishing states in UnsafeStates way is problematic because any caller can modify its contents. In this case, the states array has escaped its intended scope, because what was supposed to be private state has been effectively made public. Whether another thread actually does something with a published reference doesn\u2019t really matter, because the risk of misuse is still present. Once an object escapes, you have to assume that another class or thread may, maliciously or carelessly, misuse it. This is a compelling reason to use encapsulation: it makes it practical to analyze programs for correctness and harder to violate design constraints accidentally. A final mechanism by which an object or its internal state can be published is to publish an inner class instance: public class ThisEscape { // Don't do this public ThisEscape ( EventSource source ) { source . registerListener ( new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }); } } } When ThisEscape publishes the EventListener , it implicitly publishes the enclosing ThisEscape instance as well, because inner class instances contain a hidden reference to the enclosing instance.","title":"Publication and escape"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#safe-construction-practices","text":"ThisEscape illustrates an important special case of escape\u2014when the this references escapes during construction. When the inner EventListener instance is published, so is the enclosing ThisEscape instance. But an object is in a predictable, consistent state only after its constructor returns, so publishing an object from within its constructor can publish an incompletely constructed object. This is true even if the publication is the last statement in the constructor. If the this reference escapes during construction, the object is considered not properly constructed. Do not allow the this reference to escape during construction. A common mistake that can let the this reference escape during construction is to start a thread from a constructor. When an object creates a thread from its constructor, it almost always shares its this reference with the new thread, either explicitly (by passing it to the constructor) or implicitly (because the Thread or Runnable is an inner class of the owning object). The new thread might then be able to see the owning object before it is fully constructed. There\u2019s nothing wrong with creating a thread in a constructor, but it is best not to start the thread immediately. Instead, expose a start or initialize method that starts the owned thread. Calling an overrideable instance method (one that is neither private nor final) from the constructor can also allow the this reference to escape. If you are tempted to register an event listener or start a thread from a constructor, you can avoid the improper construction by using a private constructor and a public factory method. public class SafeListener { private final EventListener listener ; private SafeListener () { listener = new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }; } public static SafeListener newInstance ( EventSource source ) { SafeListener safe = new SafeListener (); source . registerListener ( safe . listener ); return safe ; } }","title":"Safe construction practices"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#thread-confinement","text":"Accessing shared, mutable data requires using synchronization; one way to avoid this requirement is to not share. If data is only accessed from a single thread, no synchronization is needed. This technique, thread confinement, is one of the simplest ways to achieve thread safety. When an object is confined to a thread, such usage is automatically thread-safe even if the confined object itself is not. Examples: Swing: The Swing visual components and data model objects are not thread safe; instead, safety is achieved by confining them to the Swing event dispatch thread. To use Swing properly, code running in threads other than the event thread should not access these objects. (To make this easier, Swing provides the invokeLater mechanism to schedule a Runnable for execution in the event thread.) Many concurrency errors in Swing applications stem from improper use of these confined objects from another thread. Pooled JDBC Connection objects: The JDBC specification does not require that Connection objects be thread-safe 4 . In typical server applications, a thread acquires a connection from the pool, uses it for processing a single request, and returns it. Since most requests, such as servlet requests or EJB (Enterprise JavaBeans) calls, are processed synchronously by a single thread, and the pool will not dispense the same connection to another thread until it has been returned, this pattern of connection management implicitly confines the Connection to that thread for the duration of the request. Just as the language has no mechanism for enforcing that a variable is guarded by a lock, it has no means of confining an object to a thread. Thread confinement is an element of your program\u2019s design that must be enforced by its implementation. The language and core libraries provide mechanisms that can help in maintaining thread confinement\u2014local variables and the ThreadLocal class\u2014but even with these, it is still the programmer\u2019s responsibility to ensure that thread-confined objects do not escape from their intended thread.","title":"Thread confinement"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#ad-hoc-thread-confinement","text":"Ad-hoc thread confinement describes when the responsibility for maintaining thread confinement falls entirely on the implementation. Ad-hoc thread confinement can be fragile because none of the language features, such as visibility modifiers or local variables, helps confine the object to the target thread. In fact, references to thread-confined objects such as visual components or data models in GUI applications are often held in public fields. The decision to use thread confinement is often a consequence of the decision to implement a particular subsystem, such as the GUI, as a single-threaded sub- system. Single-threaded subsystems can sometimes offer a simplicity benefit that outweighs the fragility of ad-hoc thread confinement 5 . A special case of thread confinement applies to volatile variables. It is safe to perform read-modify-write operations on shared volatile variables as long as you ensure that the volatile variable is only written from a single thread. In this case, you are confining the modification to a single thread to prevent race conditions, and the visibility guarantees for volatile variables ensure that other threads see the most up-to-date value. Because of its fragility, ad-hoc thread confinement should be used sparingly; if possible, use one of the stronger forms of thread confinment (stack confinement or ThreadLocal) instead.","title":"Ad-hoc thread confinement"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#stack-confinement","text":"Stack confinement is a special case of thread confinement in which an object can only be reached through local variables. Just as encapsulation can make it easier to preserve invariants, local variables can make it easier to confine objects to a thread. Local variables are intrinsically confined to the executing thread; they exist on the executing thread\u2019s stack, which is not accessible to other threads. Stack confinement (also called within-thread or thread-local usage, but not to be confused with the ThreadLocal library class) is simpler to maintain and less fragile than ad-hoc thread confinement. For primitively typed local variables, such as numPairs in loadTheArk in below sample you cannot violate stack confinement even if you tried. There is no way to obtain a reference to a primitive variable, so the language semantics ensure that primitive local variables are always stack confined. public int loadTheArk ( Collection < Animal > candidates ) { SortedSet < Animal > animals ; int numPairs = 0 ; Animal candidate = null ; // animals confined to method, don\u2019t let them escape! animals = new TreeSet < Animal >( new SpeciesGenderComparator ()); animals . addAll ( candidates ); for ( Animal a : animals ) { if ( candidate == null || ! candidate . isPotentialMate ( a )) candidate = a ; else { ark . load ( new AnimalPair ( candidate , a )); ++ numPairs ; candidate = null ; } } return numPairs ; } In loadTheArk, we instantiate a TreeSet and store a reference to it in animals. At this point, there is exactly one reference to the Set, held in a local variable and therefore confined to the executing thread. However, if we were to publish a reference to the Set (or any of its internals), the confinement would be violated and the animals would escape. The design requirement that the object be confined to the executing thread, or the awareness that the confined object is not thread-safe, often exists only in the head of the developer when the code is written. If the assumption of within-thread usage is not clearly documented, future maintainers might mistakenly allow the object to escape.","title":"Stack confinement"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#threadlocal-typically-private-static-fields","text":"A more formal means of maintaining thread confinement is ThreadLocal, which allows you to associate a per-thread value with a value-holding object. ThreadLocal provides get and set accessor methods that maintain a separate copy of the value for each thread that uses it, so a get returns the most recent value passed to set from the currently executing thread. Thread-local variables are often used to prevent sharing in designs based on mutable Singletons or global variables. For example, a single-threaded application might maintain a global database connection that is initialized at startup to avoid having to pass a Connection to every method. Since JDBC connections may not be thread-safe, a multithreaded application that uses a global connection without additional coordination is not thread-safe either. By using a ThreadLocal to store the JDBC connection, as in ConnectionHolder in below sample, each thread will have its own connection. private static ThreadLocal < Connection > connectionHolder = new ThreadLocal < Connection >() { public Connection initialValue () { return DriverManager . getConnection ( DB_URL ); } }; public static Connection getConnection () { return connectionHolder . get (); } When a thread calls ThreadLocal.get for the first time, initialValue is consulted to provide the initial value for that thread. The thread-specific values are stored in the Thread object itself; when the thread terminates, the thread-specific values can be garbage collected. If you are porting a single-threaded application to a multithreaded environment, you can preserve thread safety by converting shared global variables into ThreadLocals, if the semantics of the shared globals permits this; an application- wide cache would not be as useful if it were turned into a number of thread-local caches. It is easy to abuse ThreadLocal by treating its thread confinement property as a license to use global variables or as a means of creating \u201chidden\u201d method arguments. Like global variables, thread-local variables can detract from reusability and introduce hidden couplings among classes, and should therefore be used with care.","title":"ThreadLocal (typically private static fields)"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#immutability","text":"If an object\u2019s state cannot be modified, these risks and complexities simply go away. An immutable object is one whose state cannot be changed after construction. Immutable objects are inherently thread-safe; their invariants are established by the constructor, and if their state cannot be changed, these invariants always hold. Immutable objects are always thread-safe. Immutable objects are simple. They can only be in one state, which is carefully controlled by the constructor. One of the most difficult elements of program design is reasoning about the possible states of complex objects. Reasoning about the state of immutable objects, on the other hand, is trivial. Immutable objects are also safer. Passing a mutable object to untrusted code, or otherwise publishing it where untrusted code could find it, is dangerous\u2014the untrusted code might modify its state, or, worse, retain a reference to it and modify its state later from another thread. On the other hand, immutable objects cannot be subverted in this manner by malicious or buggy code, so they are safe to share and publish freely without the need to make defensive copies. Neither the Java Language Specification nor the Java Memory Model formally defines immutability, but immutability is not equivalent to simply declaring all fields of an object final. An object whose fields are all final may still be mutable, since final fields can hold references to mutable objects. An object is immutable if: Its state cannot be modified after construction. All its fields are final. It is properly constructed (the this reference does not escape during construction). While the Set that stores the names is mutable, the design of ThreeStooges makes it impossible to modify that Set after construction. The stooges reference is final, so all object state is reached through a final field. The last requirement, proper construction, is easily met since the constructor does nothing that would cause the this reference to become accessible to code other than the constructor and its caller. @Immutable public final class ThreeStooges { private final Set < String > stooges = new HashSet < String >(); public ThreeStooges () { stooges . add ( \"Moe\" ); stooges . add ( \"Larry\" ); stooges . add ( \"Curly\" ); } public boolean isStooge ( String name ) { return stooges . contains ( name ); } }","title":"Immutability"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#final-fields","text":"The final keyword, a more limited version of the const mechanism from C++, supports the construction of immutable objects. Final fields can\u2019t be modified (although the objects they refer to can be modified if they are mutable), but they also have special semantics under the Java Memory Model. It is the use of final fields that makes possible the guarantee of initialization safety that lets immutable objects be freely accessed and shared without synchronization. Even if an object is mutable, making some fields final can still simplify reasoning about its state, since limiting the mutability of an object restricts its set of possible states. An object that is \u201cmostly immutable\u201d but has one or two mutable state variables is still simpler than one that has many mutable variables. Declaring fields final also documents to maintainers that these fields are not expected to change. Just as it is a good practice to make all fields private unless they need greater visibility, it is a good practice to make all fields final unless they need to be mutable.","title":"Final fields"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#example-using-volatile-to-publish-immutable-objects","text":"Immutable objects can sometimes provide a weak form of atomicity. The factoring servlet performs two operations that must be atomic: updating the cached result and conditionally fetching the cached factors if the cached number matches the requested number. Whenever a group of related data items must be acted on atomically, consider creating an immutable holder class for them, such as OneValueCache 6 : @Immutable class OneValueCache { private final BigInteger lastNumber ; private final BigInteger [] lastFactors ; public OneValueCache ( BigInteger i , BigInteger [] factors ) { lastNumber = i ; lastFactors = Arrays . copyOf ( factors , factors . length ); } public BigInteger [] getFactors ( BigInteger i ) { if ( lastNumber == null || ! lastNumber . equals ( i )) return null ; else return Arrays . copyOf ( lastFactors , lastFactors . length ); } } Race conditions in accessing or updating multiple related variables can be eliminated by using an immutable object to hold all the variables. With a mutable holder object, you would have to use locking to ensure atomicity; with an im- mutable one, once a thread acquires a reference to it, it need never worry about another thread modifying its state. If the variables are to be updated, a new holder object is created, but any threads working with the previous holder still see it in a consistent state. VolatileCachedFactorizer uses a OneValueCache to store the cached number and factors. When a thread sets the volatile cache field to reference a new OneValueCache , the new cached data becomes immediately visible to other threads. The cache-related operations cannot interfere with each other because OneValueCache is immutable and the cache field is accessed only once in each of the relevant code paths. This combination of an immutable holder object for multiple state variables related by an invariant, and a volatile reference used to ensure its timely visibility, allows VolatileCachedFactorizer to be thread-safe even though it does no explicit locking. @ThreadSafe public class VolatileCachedFactorizer implements Servlet { private volatile OneValueCache cache = new OneValueCache ( null , null ); public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = cache . getFactors ( i ); if ( factors == null ) { factors = factor ( i ); cache = new OneValueCache ( i , factors ); } encodeIntoResponse ( resp , factors ); } }","title":"Example: Using volatile to publish immutable objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#safe-publication","text":"Sometimes we do want to share objects across threads, and in this case we must do so safely. Unfortunately, simply storing a reference to an object into a public field, is not enough to publish that object safely. // Unsafe publication public Holder holder ; public void initialize () { holder = new Holder ( 42 ); } Because of visibility problems, the Holder could appear to another thread to be in an inconsistent state, even though its invariants were properly established by its constructor! This improper publication could allow another thread to observe a partially constructed object.","title":"Safe publication"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#improper-publication-when-good-objects-go-bad","text":"You cannot rely on the integrity of partially constructed objects. An observing thread could see the object in an inconsistent state, and then later see its state suddenly change, even though it has not been modified since publication 7 . Because synchronization was not used to make the Holder visible to other threads, we say the Holder was not properly published. Other threads could see a stale value for the holder field, and thus see a null reference or other older value even though a value has been placed in holder. But far worse, other threads could see an up-to-date value for the holder reference, but stale values for the state of the Holder. To make things even less predictable, a thread may see a stale value the first time it reads a field and then a more up-to-date value the next time.","title":"Improper publication: when good objects go bad"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#immutable-objects-and-initialization-safety","text":"Because immutable objects are so important, the Java Memory Model offers a special guarantee of initialization safety for sharing immutable objects. As we\u2019ve seen, that an object reference becomes visible to another thread does not necessarily mean that the state of that object is visible to the consuming thread. In order to guarantee a consistent view of the object\u2019s state, synchronization is needed. Immutable objects, on the other hand, can be safely accessed even when synchronization is not used to publish the object reference. For this guarantee of initialization safety to hold, all of the requirements for immutability must be met: unmodifiable state, all fields are final, and proper construction. Immutable objects can be used safely by any thread without additional synchronization, even when synchronization is not used to publish them. This guarantee extends to the values of all final fields of properly constructed objects; final fields can be safely accessed without additional synchronization. However, if final fields refer to mutable objects, synchronization is still required to access the state of the objects they refer to.","title":"Immutable objects and initialization safety"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#safe-publication-idioms","text":"Objects that are not immutable must be safely published, which usually entails synchronization by both the publishing and the consuming thread. To publish an object safely, both the reference to the object and the object\u2019s state must be made visible to other threads at the same time. A properly constructed object can be safely published by: Initializing an object reference from a static initializer; Storing a reference to it into a volatile field or AtomicReference; Storing a reference to it into a final field of a properly constructed object; Storing a reference to it into a field that is properly guarded by a lock. The internal synchronization in thread-safe collections means that placing an object in a thread-safe collection, such as a Vector or synchronizedList, fulfills the last of these requirements. If thread A places object X in a thread-safe collection and thread B subsequently retrieves it, B is guaranteed to see the state of X as A left it, even though the application code that hands X off in this manner has no explicit synchronization. The thread-safe library collections offer the following safe publication guarantees, even if the Javadoc is less than clear on the subject: Placing a key or value in a Hashtable, synchronizedMap, or ConcurrentMap safely publishes it to any thread that retrieves it from the Map (whether directly or via an iterator); Placing an element in a Vector, CopyOnWriteArrayList, CopyOnWriteArraySet, synchronizedList, or synchronizedSet safely publishes it to any thread that retrieves it from the collection; Placing an element on a BlockingQueue or a ConcurrentLinkedQueue safely publishes it to any thread that retrieves it from the queue. Using a static initializer is often the easiest and safest way to publish objects that can be statically constructed: public static Holder holder = new Holder(42); Static initializers are executed by the JVM at class initialization time; because of internal synchronization in the JVM, this mechanism is guaranteed to safely publish any objects initialized in this way","title":"Safe publication idioms"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#effectively-immutable-objects","text":"The safe publication mechanisms all guarantee that the as-published state of an object is visible to all accessing threads as soon as the reference to it is visible, and if that state is not going to be changed again, this is sufficient to ensure that any access is safe. Objects that are not technically immutable, but whose state will not be modified after publication, are called effectively immutable. They do not need to meet the strict definition of immutability; they merely need to be treated by the program as if they were immutable after they are published. Using effectively immutable objects can simplify development and improve performance by reducing the need for synchronization. Safely published effectively immutable objects can be used safely by any thread without additional synchronization.","title":"Effectively immutable objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#mutable-objects","text":"If an object may be modified after construction, safe publication ensures only the visibility of the as-published state. Synchronization must be used not only to publish a mutable object, but also every time the object is accessed to ensure visibility of subsequent modifications. To share mutable objects safely, they must be safely published and be either thread-safe or guarded by a lock. The publication requirements for an object depend on its mutability: Immutable objects can be published through any mechanism. Effectively immutable objects must be safely published. Mutable objects must be safely published, and must be either thread-safe or guarded by a lock.","title":"Mutable objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#sharing-objects-safely","text":"Whenever you acquire a reference to an object, you should know what you are allowed to do with it. Do you need to acquire a lock before using it? Are you allowed to modify its state, or only to read it? Many concurrency errors stem from failing to understand these \u201crules of engagement\u201d for a shared object. When you publish an object, you should document how the object can be accessed. The most useful policies for using and sharing objects in a concurrent program are: Thread-confined. A thread-confined object is owned exclusively by and confined to one thread, and can be modified by its owning thread. Shared read-only. A shared read-only object can be accessed concurrently by multiple threads without additional synchronization, but cannot be modified by any thread. Shared read-only objects include immutable and effectively immutable objects. Shared thread-safe. A thread-safe object performs synchronization internally, so multiple threads can freely access it through its public interface without further synchronization. Guarded. A guarded object can be accessed only with a specific lock held. Guarded objects include those that are encapsulated within other thread-safe objects and published objects that are known to be guarded by a specific lock. This may seem like a broken design, but it is meant to allow JVMs to take full advantage of the performance of modern multiprocessor hardware. For example, in the absence of synchronization, the Java Memory Model permits the compiler to reorder operations and cache values in registers, and permits CPUs to reorder operations and cache values in processor-specific caches. \u21a9 When the Java Virtual Machine Specification was written, many widely used processor architectures could not efficiently provide atomic 64-bit arithmetic operations. \u21a9 Volatile reads are only slightly more expensive than nonvolatile reads on most current processor architectures. \u21a9 The connection pool implementations provided by application servers are thread-safe; connection pools are necessarily accessed from multiple threads, so a non-thread-safe implementation would not make sense. \u21a9 Another reason to make a subsystem single-threaded is deadlock avoidance; this is one of the primary reasons most GUI frameworks are single-threaded. \u21a9 OneValueCache wouldn\u2019t be immutable without the copyOf calls in the constructor and getter. Arrays.copyOf was added as a convenience in Java 6; clone would also work. \u21a9 The problem here is not the Holder class itself, but that the Holder is not properly published. However, Holder can be made immune to improper publication by declaring the n field to be final which would make Holder immutable. \u21a9","title":"Sharing objects safely"},{"location":"java/concurrency/fundamentals/c04-composing-objects/","text":"4. Composing objects","title":"Composing objects"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#4-composing-objects","text":"","title":"4. Composing objects"},{"location":"java/concurrency/introduction/c01-intro/","text":"1. Java Concurrency The language provides low-level mechanisms such as synchronization and condition waits , but these mechanisms must be used consistently to implement application-level protocols or policies. Without such policies, it is all too easy to create programs that compile and appear to work but are nevertheless broken. Processes Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously: Resource utilization . Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run. Fairness . Multiple users and programs may have equal claims on the machine\u2019s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start an- other. Convenience . It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks. A daily life sample The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence\u2014mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions\u2014open the cupboard, select a flavor of tea, measure some tea into the pot, see if there\u2019s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step\u2014waiting for the water to boil\u2014also involves a degree of asynchrony. While the water is heating, you have a choice of what to do\u2014just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people\u2014and the same is true of programs . Threads The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of threads. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs. Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms . But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results. Benefits of threads When used properly, threads can reduce development and maintenance costs and improve the performance of complex applications. Threads make it easier to model how humans work and interact, by turning asynchronous workflows into mostly sequential ones. They can also turn otherwise convoluted code into straight-line code that is easier to write, read, and maintain. Exploiting multiple processors Since the basic unit of scheduling is the thread, a program with only one thread can run on at most one processor at a time. On a two-processor system, a single-threaded program is giving up access to half the available CPU resources; on a 100-processor system, it is giving up access to 99%. On the other hand, programs with multiple active threads can execute simultaneously on multiple processors. When properly designed, multithreaded programs can improve throughput by utilizing available processor resources more effectively. Using multiple threads can also help achieve better throughput on single-processor systems. If a program is single-threaded, the processor remains idle while it waits for a synchronous I/O operation to complete. In a multithreaded program, another thread can still run while the first thread is waiting for the I/O to complete, allowing the application to still make progress during the blocking I/O. (This is like reading the newspaper while waiting for the water to boil, rather than waiting for the water to boil before starting to read.) Simplicity of modeling It is often easier to manage your time when you have only one type of task to perform (fix these twelve bugs) than when you have several (fix the bugs, interview replacement candidates for the system administrator, complete your team\u2019s performance evaluations, and create the slides for your presentation next week). When you have only one type of task to do, you can start at the top of the pile and keep working until the pile is exhausted (or you are); you don\u2019t have to spend any mental energy figuring out what to work on next. On the other hand, managing multiple priorities and deadlines and switching from task to task usually carries some overhead. The same is true for software: a program that processes one type of task sequentially is simpler to write, less error-prone, and easier to test than one managing multiple different types of tasks at once. Assigning a thread to each type of task or to each element in a simulation affords the illusion of sequentiality and insulates domain logic from the details of scheduling, interleaved operations, asynchronous I/O, and resource waits. A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points. Simplified handling of asynchronous events A server application that accepts socket connections from multiple remote clients may be easier to develop when each connection is allocated its own thread and allowed to use synchronous I/O. If an application goes to read from a socket when no data is available, read blocks until some data is available. In a single-threaded application, this means that not only does processing the corresponding request stall, but processing of all requests stalls while the single thread is blocked. To avoid this problem, single-threaded server applications are forced to use nonblocking I/O, which is far more complicated and error-prone than synchronous I/O. However, if each request has its own thread, then blocking does not affect the processing of other requests. Historically, operating systems placed relatively low limits on the number of threads that a process could create, as few as several hundred (or even less). As a result, operating systems developed efficient facilities for multiplexed I/O, such as the Unix select and poll system calls, and to access these facilities, the Java class libraries acquired a set of packages (java.nio) for nonblocking I/O. However, operating system support for larger numbers of threads has improved significantly, making the thread-per-client model practical even for large numbers of clients on some platforms. Risks of threads Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising. Safety hazards Because threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. This is a tremendous convenience, because it makes data sharing much easier than would other inter-thread communications mechanisms. But it is also a significant risk: threads can be confused by having data change unexpectedly. Allowing multiple threads to access and modify the same variables introduces an element of nonsequentiality into an otherwise sequential programming model, which can be confusing and difficult to reason about. For a multithreaded program\u2019s behavior to be predictable, access to shared variables must be properly coordinated so that threads do not interfere with one another. Liveness hazards A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. One form of liveness failure that can occur in sequential programs is an inadvertent infinite loop, where the code that follows the loop never gets executed. The use of threads introduces additional liveness risks. For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever. Like most concurrency bugs, bugs that cause liveness failures can be elusive because they depend on the relative timing of events in different threads, and therefore do not always manifest themselves in development or testing. Performance hazards Performance issues subsume a broad range of problems, including poor service time, responsiveness, throughput, resource consumption, or scalability. Just as with safety and liveness, multithreaded programs are subject to all the performance hazards of single-threaded programs, and to others as well that are introduced by the use of threads. In well designed concurrent applications the use of threads is a net performance gain, but threads nevertheless carry some degree of runtime overhead. Context switches\u2014when the scheduler suspends the active thread temporarily so another thread can run\u2014are more frequent in applications with many threads, and have significant costs: saving and restoring execution context, loss of locality, and CPU time spent scheduling threads instead of running them. When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus.","title":"Introduction"},{"location":"java/concurrency/introduction/c01-intro/#1-java-concurrency","text":"The language provides low-level mechanisms such as synchronization and condition waits , but these mechanisms must be used consistently to implement application-level protocols or policies. Without such policies, it is all too easy to create programs that compile and appear to work but are nevertheless broken.","title":"1. Java Concurrency"},{"location":"java/concurrency/introduction/c01-intro/#processes","text":"Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously: Resource utilization . Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run. Fairness . Multiple users and programs may have equal claims on the machine\u2019s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start an- other. Convenience . It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks.","title":"Processes"},{"location":"java/concurrency/introduction/c01-intro/#a-daily-life-sample","text":"The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence\u2014mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions\u2014open the cupboard, select a flavor of tea, measure some tea into the pot, see if there\u2019s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step\u2014waiting for the water to boil\u2014also involves a degree of asynchrony. While the water is heating, you have a choice of what to do\u2014just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people\u2014and the same is true of programs .","title":"A daily life sample"},{"location":"java/concurrency/introduction/c01-intro/#threads","text":"The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of threads. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs. Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms . But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results.","title":"Threads"},{"location":"java/concurrency/introduction/c01-intro/#benefits-of-threads","text":"When used properly, threads can reduce development and maintenance costs and improve the performance of complex applications. Threads make it easier to model how humans work and interact, by turning asynchronous workflows into mostly sequential ones. They can also turn otherwise convoluted code into straight-line code that is easier to write, read, and maintain.","title":"Benefits of threads"},{"location":"java/concurrency/introduction/c01-intro/#exploiting-multiple-processors","text":"Since the basic unit of scheduling is the thread, a program with only one thread can run on at most one processor at a time. On a two-processor system, a single-threaded program is giving up access to half the available CPU resources; on a 100-processor system, it is giving up access to 99%. On the other hand, programs with multiple active threads can execute simultaneously on multiple processors. When properly designed, multithreaded programs can improve throughput by utilizing available processor resources more effectively. Using multiple threads can also help achieve better throughput on single-processor systems. If a program is single-threaded, the processor remains idle while it waits for a synchronous I/O operation to complete. In a multithreaded program, another thread can still run while the first thread is waiting for the I/O to complete, allowing the application to still make progress during the blocking I/O. (This is like reading the newspaper while waiting for the water to boil, rather than waiting for the water to boil before starting to read.)","title":"Exploiting multiple processors"},{"location":"java/concurrency/introduction/c01-intro/#simplicity-of-modeling","text":"It is often easier to manage your time when you have only one type of task to perform (fix these twelve bugs) than when you have several (fix the bugs, interview replacement candidates for the system administrator, complete your team\u2019s performance evaluations, and create the slides for your presentation next week). When you have only one type of task to do, you can start at the top of the pile and keep working until the pile is exhausted (or you are); you don\u2019t have to spend any mental energy figuring out what to work on next. On the other hand, managing multiple priorities and deadlines and switching from task to task usually carries some overhead. The same is true for software: a program that processes one type of task sequentially is simpler to write, less error-prone, and easier to test than one managing multiple different types of tasks at once. Assigning a thread to each type of task or to each element in a simulation affords the illusion of sequentiality and insulates domain logic from the details of scheduling, interleaved operations, asynchronous I/O, and resource waits. A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points.","title":"Simplicity of modeling"},{"location":"java/concurrency/introduction/c01-intro/#simplified-handling-of-asynchronous-events","text":"A server application that accepts socket connections from multiple remote clients may be easier to develop when each connection is allocated its own thread and allowed to use synchronous I/O. If an application goes to read from a socket when no data is available, read blocks until some data is available. In a single-threaded application, this means that not only does processing the corresponding request stall, but processing of all requests stalls while the single thread is blocked. To avoid this problem, single-threaded server applications are forced to use nonblocking I/O, which is far more complicated and error-prone than synchronous I/O. However, if each request has its own thread, then blocking does not affect the processing of other requests. Historically, operating systems placed relatively low limits on the number of threads that a process could create, as few as several hundred (or even less). As a result, operating systems developed efficient facilities for multiplexed I/O, such as the Unix select and poll system calls, and to access these facilities, the Java class libraries acquired a set of packages (java.nio) for nonblocking I/O. However, operating system support for larger numbers of threads has improved significantly, making the thread-per-client model practical even for large numbers of clients on some platforms.","title":"Simplified handling of asynchronous events"},{"location":"java/concurrency/introduction/c01-intro/#risks-of-threads","text":"Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising.","title":"Risks of threads"},{"location":"java/concurrency/introduction/c01-intro/#safety-hazards","text":"Because threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. This is a tremendous convenience, because it makes data sharing much easier than would other inter-thread communications mechanisms. But it is also a significant risk: threads can be confused by having data change unexpectedly. Allowing multiple threads to access and modify the same variables introduces an element of nonsequentiality into an otherwise sequential programming model, which can be confusing and difficult to reason about. For a multithreaded program\u2019s behavior to be predictable, access to shared variables must be properly coordinated so that threads do not interfere with one another.","title":"Safety hazards"},{"location":"java/concurrency/introduction/c01-intro/#liveness-hazards","text":"A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. One form of liveness failure that can occur in sequential programs is an inadvertent infinite loop, where the code that follows the loop never gets executed. The use of threads introduces additional liveness risks. For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever. Like most concurrency bugs, bugs that cause liveness failures can be elusive because they depend on the relative timing of events in different threads, and therefore do not always manifest themselves in development or testing.","title":"Liveness hazards"},{"location":"java/concurrency/introduction/c01-intro/#performance-hazards","text":"Performance issues subsume a broad range of problems, including poor service time, responsiveness, throughput, resource consumption, or scalability. Just as with safety and liveness, multithreaded programs are subject to all the performance hazards of single-threaded programs, and to others as well that are introduced by the use of threads. In well designed concurrent applications the use of threads is a net performance gain, but threads nevertheless carry some degree of runtime overhead. Context switches\u2014when the scheduler suspends the active thread temporarily so another thread can run\u2014are more frequent in applications with many threads, and have significant costs: saving and restoring execution context, loss of locality, and CPU time spent scheduling threads instead of running them. When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus.","title":"Performance hazards"}]}