{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My learning notes","title":"Home"},{"location":"#my-learning-notes","text":"","title":"My learning notes"},{"location":"architecture/microservices-patterns/escaping-monolithic-hell/","text":"Escaping monolitic hell Amazon had, for example, started to migrate away from the monolith around 2002 The new architecture replaced the monolith with a collection of loosely coupled services. Services are owned by what Amazon calls two-pizza teams\u2014teams small enough to be fed by two pizzas. Amazon had adopted this architecture to accelerate the rate of software development so that the company could innovate faster and compete more effectively. The results are impressive: Amazon reportedly deploys changes into production every 11.6 seconds! A pattern is a reusable solution to a problem that occurs in a particular context. The beauty of a pattern is that besides describing the benefits of the solution, it also describes the drawbacks and the issues you must address in order to successfully implement a solution. In my experience, this kind of objectivity when thinking about solutions leads to much better decision making.","title":"Escaping monolithic hell"},{"location":"architecture/microservices-patterns/escaping-monolithic-hell/#escaping-monolitic-hell","text":"Amazon had, for example, started to migrate away from the monolith around 2002 The new architecture replaced the monolith with a collection of loosely coupled services. Services are owned by what Amazon calls two-pizza teams\u2014teams small enough to be fed by two pizzas. Amazon had adopted this architecture to accelerate the rate of software development so that the company could innovate faster and compete more effectively. The results are impressive: Amazon reportedly deploys changes into production every 11.6 seconds! A pattern is a reusable solution to a problem that occurs in a particular context. The beauty of a pattern is that besides describing the benefits of the solution, it also describes the drawbacks and the issues you must address in order to successfully implement a solution. In my experience, this kind of objectivity when thinking about solutions leads to much better decision making.","title":"Escaping monolitic hell"},{"location":"artificial-intelligence/","text":"Artificial Intelligence","title":"Artificial Intelligence"},{"location":"artificial-intelligence/#artificial-intelligence","text":"","title":"Artificial Intelligence"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/","text":"Statistical Learning Based on the youngest brother of The Elements of Statistical Learning (ESL) : An Introduction to Statistical Learning Statistical learning refers to a set of tools for modeling and understanding complex datasets . It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. Tools classification: supervised unsupervised Supervised statistical learning (predict an output variable) Involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. Unsupervised statistical learning In this case there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. It involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a clustering problem. Wage Data The Wage data involves predicting a continuous or quantitative output value. This is often referred to as a regression problem. Stock Market Data In certain cases we may instead wish to predict a non-numerical value\u2014that is, a categorical or qualitative output. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves\u2014for exmple\u2014predicting whether a given day\u2019s stock market performance will fall into the Up bucket or the Down bucket. This is known as a classification problem. Gene Expression Data Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.","title":"Introduction"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#statistical-learning","text":"Based on the youngest brother of The Elements of Statistical Learning (ESL) : An Introduction to Statistical Learning Statistical learning refers to a set of tools for modeling and understanding complex datasets . It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. Tools classification: supervised unsupervised","title":"Statistical Learning"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#supervised-statistical-learning-predict-an-output-variable","text":"Involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy.","title":"Supervised statistical learning (predict an output variable)"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#unsupervised-statistical-learning","text":"In this case there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. It involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a clustering problem.","title":"Unsupervised statistical learning"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#wage-data","text":"The Wage data involves predicting a continuous or quantitative output value. This is often referred to as a regression problem.","title":"Wage Data"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#stock-market-data","text":"In certain cases we may instead wish to predict a non-numerical value\u2014that is, a categorical or qualitative output. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves\u2014for exmple\u2014predicting whether a given day\u2019s stock market performance will fall into the Up bucket or the Down bucket. This is known as a classification problem.","title":"Stock Market Data"},{"location":"artificial-intelligence/data-science/statistical-learning/introduction/#gene-expression-data","text":"Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.","title":"Gene Expression Data"},{"location":"artificial-intelligence/machine-learning/","text":"Machine Learning","title":"Machine Learning"},{"location":"artificial-intelligence/machine-learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/","text":"Introduction to Machine Learning It is a science of getting computers to learn without being explicitly programmed. Many scientists think the best way to make progress on this is through learning algorithms called neural networks, which mimic how the human brain works, and I'll teach you about that, too About machine Learning : Had grown out of the field of AI Is a new capability for computers Examples: Database mining Large datasets from growth of automation/web click-streams, medical records, biology, engineering Apps can\u2019t program by hand Autonomous helicopter, handwriting recognition, natural language processing (NLP), computer vision Understanding language Understanding images What is Machine Learning Even among machine learning practitioners, there isn't a well accepted definition of what is and what isn't machine learning. Arthur Samuel (1950) defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measure by P , improves with experience E . Types of learning algorithms Supervised learning : The idea is to teach the computer how to do something Unsupervised learning : In this case the computer is going to learn by itself There are also other types of algorithms such as reinforcement learning and recommender systems but the two most use types of learning algorithms are probably supervised learning and unsupervised learning. About the course: Why do we have to use Matlab or Octave? Why not Clojure, Julia, Python, R or [Insert favourite language here] ? A: As Prof. Ng explained in the 1st video of the Octave tutorial, he has tried teaching Machine Learning in a variety of languages, and found that students come up to speed faster with Matlab/Octave. Therefore the course was designed using Octave/Matlab, and the automatic submission grader uses those program interfaces. Octave and Matlab are optimized for rapid vectorized calculations, which is very useful in Machine Learning. R is a nice tool, but: It is a bit too high level. This course shows how to actually implement the algorithms of machine learning, while R already has them implemented. Since the focus of this course is to show you what happens in ML algorithms under the hood, you need to use Octave This course offers some starter code in Octave/Matlab, which will really save you tons of time solving the tasks.","title":"Intro"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#introduction-to-machine-learning","text":"It is a science of getting computers to learn without being explicitly programmed. Many scientists think the best way to make progress on this is through learning algorithms called neural networks, which mimic how the human brain works, and I'll teach you about that, too About machine Learning : Had grown out of the field of AI Is a new capability for computers Examples: Database mining Large datasets from growth of automation/web click-streams, medical records, biology, engineering Apps can\u2019t program by hand Autonomous helicopter, handwriting recognition, natural language processing (NLP), computer vision Understanding language Understanding images","title":"Introduction to Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#what-is-machine-learning","text":"Even among machine learning practitioners, there isn't a well accepted definition of what is and what isn't machine learning. Arthur Samuel (1950) defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measure by P , improves with experience E .","title":"What is Machine Learning"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#types-of-learning-algorithms","text":"Supervised learning : The idea is to teach the computer how to do something Unsupervised learning : In this case the computer is going to learn by itself There are also other types of algorithms such as reinforcement learning and recommender systems but the two most use types of learning algorithms are probably supervised learning and unsupervised learning.","title":"Types of learning algorithms"},{"location":"artificial-intelligence/machine-learning/introduction/intro/#about-the-course","text":"Why do we have to use Matlab or Octave? Why not Clojure, Julia, Python, R or [Insert favourite language here] ? A: As Prof. Ng explained in the 1st video of the Octave tutorial, he has tried teaching Machine Learning in a variety of languages, and found that students come up to speed faster with Matlab/Octave. Therefore the course was designed using Octave/Matlab, and the automatic submission grader uses those program interfaces. Octave and Matlab are optimized for rapid vectorized calculations, which is very useful in Machine Learning. R is a nice tool, but: It is a bit too high level. This course shows how to actually implement the algorithms of machine learning, while R already has them implemented. Since the focus of this course is to show you what happens in ML algorithms under the hood, you need to use Octave This course offers some starter code in Octave/Matlab, which will really save you tons of time solving the tasks.","title":"About the course:"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/","text":"Supervised Learning It is probably the most common type of ML problem In Supervised Learning , we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised Learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Supervised Learning problems: Regression problems Classification problems Example 1: Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2: (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem (continuous value output), I mean we're trying to predict a continuous valued output. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignant, which is one, or zero or not malignant or benign. This is an example of a classification problem (discrete value output). The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that.","title":"Supervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#supervised-learning","text":"It is probably the most common type of ML problem In Supervised Learning , we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised Learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Supervised Learning problems: Regression problems Classification problems","title":"Supervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#example-1","text":"Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories.","title":"Example 1:"},{"location":"artificial-intelligence/machine-learning/introduction/supervised-learning/#example-2","text":"(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, \"right answers\" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem (continuous value output), I mean we're trying to predict a continuous valued output. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignant, which is one, or zero or not malignant or benign. This is an example of a classification problem (discrete value output). The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that.","title":"Example 2:"},{"location":"artificial-intelligence/machine-learning/introduction/unsupervised-learning/","text":"Unsupervised Learning Unsupervised Learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With Unsupervised Learning there is no feedback based on the prediction results. Example: Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Based on above we classify: Unsupervised Learning problems : Clustering Non-clustering Personal note: Check its relationship with logical programming (predicate based programming)","title":"Unsupervised Learning"},{"location":"artificial-intelligence/machine-learning/introduction/unsupervised-learning/#unsupervised-learning","text":"Unsupervised Learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With Unsupervised Learning there is no feedback based on the prediction results. Example: Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Based on above we classify: Unsupervised Learning problems : Clustering Non-clustering Personal note: Check its relationship with logical programming (predicate based programming)","title":"Unsupervised Learning"},{"location":"artificial-intelligence/octave-lang/","text":"Octave Lang","title":"Octave Lang"},{"location":"artificial-intelligence/octave-lang/#octave-lang","text":"","title":"Octave Lang"},{"location":"consulting/pconchas/01_caso/","text":"Clear Mechanics Diagn\u00f3stico Aplicativo Basado en la entrevista, se analizaron dos aplicaciones que actualmente son ejecutadas por mas de 15 entidades\u2014una de ellas, y la otra solamente por una entidad. Ambas aplicaciones tienen requerimientos distintos de hardware y al parecer las aplicaciones comparten el sistema de almacenamiento (NoSQL/mongodb) en el mismo servidor . Es evidente que no existen instancias de la aplicaci\u00f3n por entidad (cliente) por lo que se trata de una configuraci\u00f3n monol\u00edtica. Los monolitos no son algo malo, presentan ciertos beneficios Simple de desarrollar (son una o dos aplicaciones) F\u00e1cil de hacer cambios a la aplicaci\u00f3n F\u00e1cil de probar F\u00e1cil de desplegar F\u00e1cil de escalar Pero tienen una desventaja cada vez que la aplicaci\u00f3n crece mas (se le conoce como el \"infierno monol\u00edtico\"), cada vez que la aplicaci\u00f3n se hace mas grande (se le ponen mas features) el desarrollo se hace mas lento. As\u00ed tambien el camino del commit al despliegue se hace mas largo, arduo y escalar la aplicacion se empieza a complicar. Por ejemplo ambas aplicaciones tienen distintos requerimientos de hardware, una de ellas hace uso extensivo de datos (memoria) y la otra de procesamiento; lo que hace mucho mas costoso y complejo elegir el hardware adecuado. Pipeline de desarrollo Actualmente se tiene un proceso basico de sprints en el que un programador se encarga del proceso de desarrollo, testing, despliegue y operatividad (manejo de incidencias) en ciclos de aproximadamente cada quince d\u00edas. Lo cual deja en una posici\u00f3n fr\u00e1gil al sistema pues es dependiente procesos gestionados por una persona. Sobre la consultor\u00eda en arquitectura La arquitectura tiene poco que ver con los requerimientos funcionales de una aplicaci\u00f3n (sobre que lo que la aplicaci\u00f3n tiene que hacer). Es normal que las nuevas aplicaciones exitosas, que comienzan a crecer se vuelvan una plasta con bolas de lodo. La arquitectura importa por como afecta los requerimientos de calidad del serivio \u2014es decir, los requerimientos no funcionales o atributos de calidad como lo son la mantenibilidad, extensibilidad y testeabilidad. Basado en el estatus actual del aplicativo y de la perspectiva tanto de escalamiento como de capacidad de desarrollo; he concluido estan en el momento adecuado de establecer dos importantes aspectos en la manera en la que desarrollan, testean, despliegan y mantienen la operaci\u00f3n del software que presentan. 1. Arquitectura basada en microservicios La arquitectura basada en microservicios desincorpora un sistema en un conjunto de servicios independientemente desplegables, cada uno con su propia base de datos. La arquitectura monol\u00edtica es una buena elecci\u00f3n para aplicaciones simples; pero cuando la arquitectua de microservicios cobra sentido cuando la aplicacion se vuelve grande y compleja. La arquitectura basada en microservicios acelera el ciclo de desarrollo haciendo posible que equipos peque\u00f1os y aut\u00f3nomos trabajen en paralelo; no es una bala de plata pues tiene desventajas como el aumento en complejidad. 2. CI/CD Pipeline Lamentablemente la arquitectura basada en microservicios no es suficiente para lograr las metas de escalabilidad; tambien se requiere DevOps una combinaci\u00f3n de metodolog\u00edas, pr\u00e1cticas y herramientas para evolucionar y mejorar la manera en la que se entrega el software. Se requiere de trabajo en equipo y cambios culturales para lograrlo.v","title":"Car Shop "},{"location":"consulting/pconchas/01_caso/#clear-mechanics","text":"","title":"Clear Mechanics"},{"location":"consulting/pconchas/01_caso/#diagnostico","text":"","title":"Diagn\u00f3stico"},{"location":"consulting/pconchas/01_caso/#aplicativo","text":"Basado en la entrevista, se analizaron dos aplicaciones que actualmente son ejecutadas por mas de 15 entidades\u2014una de ellas, y la otra solamente por una entidad. Ambas aplicaciones tienen requerimientos distintos de hardware y al parecer las aplicaciones comparten el sistema de almacenamiento (NoSQL/mongodb) en el mismo servidor . Es evidente que no existen instancias de la aplicaci\u00f3n por entidad (cliente) por lo que se trata de una configuraci\u00f3n monol\u00edtica. Los monolitos no son algo malo, presentan ciertos beneficios Simple de desarrollar (son una o dos aplicaciones) F\u00e1cil de hacer cambios a la aplicaci\u00f3n F\u00e1cil de probar F\u00e1cil de desplegar F\u00e1cil de escalar Pero tienen una desventaja cada vez que la aplicaci\u00f3n crece mas (se le conoce como el \"infierno monol\u00edtico\"), cada vez que la aplicaci\u00f3n se hace mas grande (se le ponen mas features) el desarrollo se hace mas lento. As\u00ed tambien el camino del commit al despliegue se hace mas largo, arduo y escalar la aplicacion se empieza a complicar. Por ejemplo ambas aplicaciones tienen distintos requerimientos de hardware, una de ellas hace uso extensivo de datos (memoria) y la otra de procesamiento; lo que hace mucho mas costoso y complejo elegir el hardware adecuado.","title":"Aplicativo"},{"location":"consulting/pconchas/01_caso/#pipeline-de-desarrollo","text":"Actualmente se tiene un proceso basico de sprints en el que un programador se encarga del proceso de desarrollo, testing, despliegue y operatividad (manejo de incidencias) en ciclos de aproximadamente cada quince d\u00edas. Lo cual deja en una posici\u00f3n fr\u00e1gil al sistema pues es dependiente procesos gestionados por una persona.","title":"Pipeline de desarrollo"},{"location":"consulting/pconchas/01_caso/#sobre-la-consultoria-en-arquitectura","text":"La arquitectura tiene poco que ver con los requerimientos funcionales de una aplicaci\u00f3n (sobre que lo que la aplicaci\u00f3n tiene que hacer). Es normal que las nuevas aplicaciones exitosas, que comienzan a crecer se vuelvan una plasta con bolas de lodo. La arquitectura importa por como afecta los requerimientos de calidad del serivio \u2014es decir, los requerimientos no funcionales o atributos de calidad como lo son la mantenibilidad, extensibilidad y testeabilidad.","title":"Sobre la consultor\u00eda en arquitectura"},{"location":"consulting/pconchas/01_caso/#_1","text":"Basado en el estatus actual del aplicativo y de la perspectiva tanto de escalamiento como de capacidad de desarrollo; he concluido estan en el momento adecuado de establecer dos importantes aspectos en la manera en la que desarrollan, testean, despliegan y mantienen la operaci\u00f3n del software que presentan.","title":""},{"location":"consulting/pconchas/01_caso/#1-arquitectura-basada-en-microservicios","text":"La arquitectura basada en microservicios desincorpora un sistema en un conjunto de servicios independientemente desplegables, cada uno con su propia base de datos. La arquitectura monol\u00edtica es una buena elecci\u00f3n para aplicaciones simples; pero cuando la arquitectua de microservicios cobra sentido cuando la aplicacion se vuelve grande y compleja. La arquitectura basada en microservicios acelera el ciclo de desarrollo haciendo posible que equipos peque\u00f1os y aut\u00f3nomos trabajen en paralelo; no es una bala de plata pues tiene desventajas como el aumento en complejidad.","title":"1. Arquitectura basada en microservicios"},{"location":"consulting/pconchas/01_caso/#2-cicd-pipeline","text":"Lamentablemente la arquitectura basada en microservicios no es suficiente para lograr las metas de escalabilidad; tambien se requiere DevOps una combinaci\u00f3n de metodolog\u00edas, pr\u00e1cticas y herramientas para evolucionar y mejorar la manera en la que se entrega el software. Se requiere de trabajo en equipo y cambios culturales para lograrlo.v","title":"2. CI/CD Pipeline"},{"location":"git/useful-commands/stash/","text":"Git stash git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on. Stashing is handy if you need to quickly switch context and work on something else, but you're mid-way through a code change and aren't quite ready to commit. The git stash command takes your uncommitted changes (both staged and unstaged), saves them away for later use, and then reverts them from your working copy. git stash --include-untracked \"Message to identify stash\" git stash --all \"Message to identify stash\" git stash list git stash apply [id] git stash pop [id] Popping your stash reapplies those changes to your working copy and removes them from your stash git stash drop [id]","title":"Stash"},{"location":"git/useful-commands/stash/#git-stash","text":"git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on. Stashing is handy if you need to quickly switch context and work on something else, but you're mid-way through a code change and aren't quite ready to commit. The git stash command takes your uncommitted changes (both staged and unstaged), saves them away for later use, and then reverts them from your working copy.","title":"Git stash"},{"location":"git/useful-commands/stash/#git-stash-include-untracked-message-to-identify-stash","text":"","title":"git stash --include-untracked \"Message to identify stash\""},{"location":"git/useful-commands/stash/#git-stash-all-message-to-identify-stash","text":"","title":"git stash --all \"Message to identify stash\""},{"location":"git/useful-commands/stash/#git-stash-list","text":"","title":"git stash list"},{"location":"git/useful-commands/stash/#git-stash-apply-id","text":"","title":"git stash apply [id]"},{"location":"git/useful-commands/stash/#git-stash-pop-id","text":"Popping your stash reapplies those changes to your working copy and removes them from your stash","title":"git stash pop [id]"},{"location":"git/useful-commands/stash/#git-stash-drop-id","text":"","title":"git stash drop [id]"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/","text":"04 Approaches to programming problems The problems used in interviews have specific requirements. They must be short enough to be explained and solved reasonably quickly, yet complex enough that not everyone can solve them. Therefore, it\u2019s unlikely that you\u2019ll be asked any real-world problems. Almost any worthy real-world problem would take too long to even explain, let alone solve. Instead, many of these problems require algorithmic tricks or uncommonly used features of a language. When you begin solving a problem, don\u2019t start writing code immediately. First, make sure you completely understand the problem. When you\u2019re convinced you have the right algorithm, explain it clearly. Writing the code should be one of your final steps. Solving the problems Basic steps Make sure you understand the problem. When you understand the question, try a simple example (???). Focus on the algorithm and data structures you will use to solve the problem. After you figure out your algorithm and how you can implement it, explain your solution to the interviewer. While you code, explain what you\u2019re doing. Ask questions when necessary. After you write the code for a problem, immediately verify that the code works by tracing through it with an example. Make sure you check your code for all error and special cases, especially boundary conditions. Getting stuck Go back to an example. Try a different data structure. Consider the less commonly used or more advanced aspects of a language. Analyzing your solution Big-O Analysis It is a form of runtime analysis that measures the efficiency of an algorithm in terms of the time it takes for the algorithm to run as a function of the input size. It\u2019s not a formal benchmark, just a simple way to classify algorithms by relative efficiency when dealing with very large input sizes. Big-O analysis, however, is concerned with the asymptotic running time: the limit of the running time as n gets very large. It\u2019s only when n become large that the differences between algorithms are noticeable. As n n approaches infinity, the difference between n and n + 2 is insignificant, so the constant term can be ignored. Similarly, for an algorithm running in n + n2 time, the difference between n2 and n + n2 is negligible for a very large n. \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Approaches-to-programming-problems"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/#04-approaches-to-programming-problems","text":"The problems used in interviews have specific requirements. They must be short enough to be explained and solved reasonably quickly, yet complex enough that not everyone can solve them. Therefore, it\u2019s unlikely that you\u2019ll be asked any real-world problems. Almost any worthy real-world problem would take too long to even explain, let alone solve. Instead, many of these problems require algorithmic tricks or uncommonly used features of a language. When you begin solving a problem, don\u2019t start writing code immediately. First, make sure you completely understand the problem. When you\u2019re convinced you have the right algorithm, explain it clearly. Writing the code should be one of your final steps.","title":"04 Approaches to programming problems"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/#solving-the-problems","text":"","title":"Solving the problems"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/#basic-steps","text":"Make sure you understand the problem. When you understand the question, try a simple example (???). Focus on the algorithm and data structures you will use to solve the problem. After you figure out your algorithm and how you can implement it, explain your solution to the interviewer. While you code, explain what you\u2019re doing. Ask questions when necessary. After you write the code for a problem, immediately verify that the code works by tracing through it with an example. Make sure you check your code for all error and special cases, especially boundary conditions.","title":"Basic steps"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/#getting-stuck","text":"Go back to an example. Try a different data structure. Consider the less commonly used or more advanced aspects of a language.","title":"Getting stuck"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/#analyzing-your-solution","text":"","title":"Analyzing your solution"},{"location":"interviews/programming-interviews-exposed/04-approaches-to-programming-problems/#big-o-analysis","text":"It is a form of runtime analysis that measures the efficiency of an algorithm in terms of the time it takes for the algorithm to run as a function of the input size. It\u2019s not a formal benchmark, just a simple way to classify algorithms by relative efficiency when dealing with very large input sizes. Big-O analysis, however, is concerned with the asymptotic running time: the limit of the running time as n gets very large. It\u2019s only when n become large that the differences between algorithms are noticeable. As n n approaches infinity, the difference between n and n + 2 is insignificant, so the constant term can be ignored. Similarly, for an algorithm running in n + n2 time, the difference between n2 and n + n2 is negligible for a very large n. \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Big-O Analysis"},{"location":"java/concurrency/fundamentals/c02-thread-safety/","text":"2. Thread safety Surprisingly, concurrent programming isn\u2019t so much about threads or locks, these are just mechanisms\u2014means to an end. Writing thread-safe code is, at its core, about managing access to state , and in particular to shared, mutable state . Informally, an object\u2019s state is its data, stored in state variables such as instance or static fields. An object\u2019s state encompasses any data that can affect its externally visible behavior. By shared , we mean that a variable could be accessed by multiple threads; by mutable , we mean that its value could change during its lifetime. What we are really trying to do is protect data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads . This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state ; failing to do so could result in data corruption and other undesirable consequences. The primary mechanism for synchronization in Java is the synchronized keyword, which provides exclusive locking, but the term \u201csynchronization\u201d also includes the use of volatile variables, explicit locks, and atomic variables. If multiple threads access the same mutable state variable without appro- priate synchronization, your program is broken . There are three ways to fix it: Don\u2019t share the state variable across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable. It is far easier to design a class to be thread-safe than to retrofit it for thread safety later. When designing thread-safe classes, good object-oriented techniques\u2014encapsulation, immutability, and clear specification of invariants\u2014are your best friends. It is always a good practice first to make your code right, and then make it fast. Even then, pursue optimization only if your performance measurements and requirements tell you that you must, and if those same measurements tell you that your optimizations actually made a difference under realistic conditions. Is a thread-safe program one that is constructed entirely of thread-safe classes? Not necessarily\u2014a program that consists entirely of thread-safe classes may not be thread-safe, and a thread-safe program may contain classes that are not thread-safe. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state. Thread safety may be a term that is applied to code, but it is about state, and it can only be applied to the entire body of code that encapsulates its state, which may be an object or an entire program. What is thread safety? Formal definition A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations\u2014calls to public methods and reads or writes of public fields\u2014should be able to violate any of its invariants or postconditions. No set of operations performed sequentially or con- currently on instances of a thread-safe class can cause an instance to be in an invalid state. Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own. Example: A stateless servlet @ThreadSafe public class StatelessFactorizer implements Servlet { public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } The transient state for a particular computation exists solely in local variables that are stored on the thread\u2019s stack and are accessible only to the executing thread. One thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer ; because the two threads do not share state, it is as if they were accessing different instances. Since the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe. Stateless objects are always thread-safe. Atomicity Atomic means something that executes as a single, indivisible operation. What happens when we add one element of state to what was a stateless object? Suppose we want to add a \u201chit counter\u201d that measures the number of requests processed. The obvious approach is to add a long field to the servlet and increment it on each request: @NotThreadSafe public class UnsafeCountingFactorizer implements Servlet { private long count = 0 ; public long getCount () { return count ; } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } While the increment operation, ++count, may look like a single action because of its compact syntax, it is not atomic, which means that it does not execute as a single, indivisible operation. Instead, it is a shorthand for a sequence of three discrete operations: fetch the current value, add one to it, and write the new value back. This is an example of a read-modify-write operation, in which the resulting state is derived from the previous state. The possibility of incorrect results in the presence of unlucky timing is so important in concurrent programming that it has a name: a race condition . Race conditions A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime; in other words, when getting the right answer relies on lucky timing. The most common type of race condition is check-then-act , where a potentially stale observation is used to make a decision on what to do next. You observe something to be true (file X doesn\u2019t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption). Example: race conditions in lazy initialization A common idiom that uses check-then-act is lazy initialization. The goal of lazy initialization is to defer initializing an object until it is actually needed while at the same time ensuring that it is initialized only once. @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null ; public ExpensiveObject getInstance () { if ( instance == null ) { instance = new ExpensiveObject (); return instance ; } } } Now we have identified two sorts of race condition operations: Read-modify-write: like incrementing a counter (i.e.: count++; ), define a transformation of an object\u2019s state in terms of its previous state. Check-then-act: Check for a condition (i.e.: if (instance == null) ) and then act (i.e.: {instance = new ExpensiveObject(); ...} ) Like most concurrency errors, race conditions don\u2019t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems. Compound actions We refer collectively to check-then-act and read-modify-write sequences as compound actions: sequences of operations that must be executed atomically in order to remain thread-safe. Fixing UnsafeCountingFactorizer @ThreadSafe public class CountingFactorizer implements Servlet { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); count . incrementAndGet (); encodeIntoResponse ( resp , factors ); } } The java.util.concurrent.atomic package contains atomic variable classes for effecting atomic state transitions on numbers and object references. By replacing the long counter with an AtomicLong, we ensure that all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe, our servlet is once again thread-safe. Locking To preserve state consistency, update related state variables in a single atomic operation. Intrinsic locks Java provides a built-in locking mechanism for enforcing atomicity: the synchronized block. A synchronized block has two parts: a reference to an object that will serve as the lock, and a block of code to be guarded by that lock. A synchronized method is a shorthand for a synchronized block that spans an entire method body, and whose lock is the object on which the method is being invoked. (Static synchronized methods use the Class object for the lock.) synchronized ( lock ) { // Access or modify shared state guarded by lock } Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. The only way to acquire an intrinsic lock is to enter a synchronized block or method guarded by that lock. Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. No thread executing a synchronized block can observe another thread to be in the middle of a synchronized block guarded by the same lock. That is, the synchronized blocks guarded by the same lock execute atomically with respect to one another. In the context of concurrency, atomicity means the same thing as it does in transactional applications\u2014that a group of statements appear to execute as a single, indivisible unit. Reentrancy When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy facilitates encapsulation of locking behavior, and thus simplifies the development of object-oriented concurrent code. public class Widget { public synchronized void doSomething () { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething () { System . out . println ( toString () + \": calling doSomething\" ); super . doSomething (); } } Without reentrant locks, the very natural-looking code above, in which a subclass overrides a synchronized method and then calls the superclass method, would deadlock. Because the doSomething methods in Widget and LoggingWidget are both synchronized, each tries to acquire the lock on the Widget before proceeding. But if intrinsic locks were not reentrant, the call to super.doSomething would never be able to acquire the lock because it would be considered already held, and the thread would permanently stall waiting for a lock it can never acquire. Reentrancy saves us from deadlock in situations like this. Guarding state with locks Because locks enable serialized 1 access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following these protocols consistently can ensure state consistency. Compound actions on shared state, such as incrementing a hit counter ( read-modify-write ) or lazy initialization ( check-then-act ), must be made atomic to avoid race conditions. Holding a lock for the entire duration of a compound action can make that compound action atomic. However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed. It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Acquiring the lock associated with an object does not prevent other threads from accessing that object\u2014the only thing that acquiring a lock prevents any other thread from doing is acquiring that same lock. The fact that every object has a built-in lock is just a convenience so that you needn\u2019t explicitly create lock objects 2 . Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is. A common locking convention is to encapsulate all mutable state within an object and to protect it from concurrent access by synchronizing any code path that accesses mutable state using the object\u2019s intrinsic lock. Not all data needs to be guarded by locks\u2014only mutable data that will be accessed from multiple threads. When a variable is guarded by a lock\u2014meaning that every access to that variable is performed with that lock held\u2014you\u2019ve ensured that only one thread at a time can access that variable. When a class has invariants that involve more than one state variable, there is an additional requirement: each variable participating in the invariant must be guarded by the same lock. This allows you to access or update them in a single atomic operation, preserving the invariant. For every invariant that involves more than one variable, all the variables involved in that invariant must be guarded by the same lock. If synchronization is the cure for race conditions, why not just declare ev- ery method synchronized? if (! vector . contains ( element )) vector . add ( element ); This attempt at a put-if-absent operation has a race condition, even though both contains and add are atomic. While synchronized methods can make individual operations atomic, additional locking is required when multiple operations are combined into a compound action. At the same time, synchronizing every method can lead to liveness or performance problems. Liveness and performance Deciding how big or small to make synchronized blocks may require tradeoffs among competing design forces, including safety (which must not be compromised), simplicity, and performance. Sometimes simplicity and performance are at odds with each other, a reasonable balance can usually be found. Whenever you use locking, you should be aware of what the code in the block is doing and how likely it is to take a long time to execute. Holding a lock for a long time, either because you are doing something compute-intensive or because you execute a potentially blocking operation, introduces the risk of liveness or performance problems. Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O. Serializing access to an object has nothing to do with object serialization (turning an object into a byte stream); serializing access means that threads take turns accessing the object exclusively, rather than doing so concurrently. \u21a9 In retrospect, this design decision was probably a bad one: not only can it be confusing, but it forces JVM implementors to make tradeoffs between object size and locking performance. \u21a9","title":"Thread safety"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#2-thread-safety","text":"Surprisingly, concurrent programming isn\u2019t so much about threads or locks, these are just mechanisms\u2014means to an end. Writing thread-safe code is, at its core, about managing access to state , and in particular to shared, mutable state . Informally, an object\u2019s state is its data, stored in state variables such as instance or static fields. An object\u2019s state encompasses any data that can affect its externally visible behavior. By shared , we mean that a variable could be accessed by multiple threads; by mutable , we mean that its value could change during its lifetime. What we are really trying to do is protect data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads . This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state ; failing to do so could result in data corruption and other undesirable consequences. The primary mechanism for synchronization in Java is the synchronized keyword, which provides exclusive locking, but the term \u201csynchronization\u201d also includes the use of volatile variables, explicit locks, and atomic variables. If multiple threads access the same mutable state variable without appro- priate synchronization, your program is broken . There are three ways to fix it: Don\u2019t share the state variable across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable. It is far easier to design a class to be thread-safe than to retrofit it for thread safety later. When designing thread-safe classes, good object-oriented techniques\u2014encapsulation, immutability, and clear specification of invariants\u2014are your best friends. It is always a good practice first to make your code right, and then make it fast. Even then, pursue optimization only if your performance measurements and requirements tell you that you must, and if those same measurements tell you that your optimizations actually made a difference under realistic conditions. Is a thread-safe program one that is constructed entirely of thread-safe classes? Not necessarily\u2014a program that consists entirely of thread-safe classes may not be thread-safe, and a thread-safe program may contain classes that are not thread-safe. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state. Thread safety may be a term that is applied to code, but it is about state, and it can only be applied to the entire body of code that encapsulates its state, which may be an object or an entire program.","title":"2. Thread safety"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#what-is-thread-safety","text":"","title":"What is thread safety?"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#formal-definition","text":"A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations\u2014calls to public methods and reads or writes of public fields\u2014should be able to violate any of its invariants or postconditions. No set of operations performed sequentially or con- currently on instances of a thread-safe class can cause an instance to be in an invalid state. Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own.","title":"Formal definition"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#example-a-stateless-servlet","text":"@ThreadSafe public class StatelessFactorizer implements Servlet { public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } The transient state for a particular computation exists solely in local variables that are stored on the thread\u2019s stack and are accessible only to the executing thread. One thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer ; because the two threads do not share state, it is as if they were accessing different instances. Since the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe. Stateless objects are always thread-safe.","title":"Example: A stateless servlet"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#atomicity","text":"Atomic means something that executes as a single, indivisible operation. What happens when we add one element of state to what was a stateless object? Suppose we want to add a \u201chit counter\u201d that measures the number of requests processed. The obvious approach is to add a long field to the servlet and increment it on each request: @NotThreadSafe public class UnsafeCountingFactorizer implements Servlet { private long count = 0 ; public long getCount () { return count ; } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); encodeIntoResponse ( resp , factors ); } } While the increment operation, ++count, may look like a single action because of its compact syntax, it is not atomic, which means that it does not execute as a single, indivisible operation. Instead, it is a shorthand for a sequence of three discrete operations: fetch the current value, add one to it, and write the new value back. This is an example of a read-modify-write operation, in which the resulting state is derived from the previous state. The possibility of incorrect results in the presence of unlucky timing is so important in concurrent programming that it has a name: a race condition .","title":"Atomicity"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#race-conditions","text":"A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime; in other words, when getting the right answer relies on lucky timing. The most common type of race condition is check-then-act , where a potentially stale observation is used to make a decision on what to do next. You observe something to be true (file X doesn\u2019t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption).","title":"Race conditions"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#example-race-conditions-in-lazy-initialization","text":"A common idiom that uses check-then-act is lazy initialization. The goal of lazy initialization is to defer initializing an object until it is actually needed while at the same time ensuring that it is initialized only once. @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null ; public ExpensiveObject getInstance () { if ( instance == null ) { instance = new ExpensiveObject (); return instance ; } } } Now we have identified two sorts of race condition operations: Read-modify-write: like incrementing a counter (i.e.: count++; ), define a transformation of an object\u2019s state in terms of its previous state. Check-then-act: Check for a condition (i.e.: if (instance == null) ) and then act (i.e.: {instance = new ExpensiveObject(); ...} ) Like most concurrency errors, race conditions don\u2019t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems.","title":"Example: race conditions in lazy initialization"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#compound-actions","text":"We refer collectively to check-then-act and read-modify-write sequences as compound actions: sequences of operations that must be executed atomically in order to remain thread-safe.","title":"Compound actions"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#fixing-unsafecountingfactorizer","text":"@ThreadSafe public class CountingFactorizer implements Servlet { private final AtomicLong count = new AtomicLong ( 0 ); public long getCount () { return count . get (); } public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = factor ( i ); count . incrementAndGet (); encodeIntoResponse ( resp , factors ); } } The java.util.concurrent.atomic package contains atomic variable classes for effecting atomic state transitions on numbers and object references. By replacing the long counter with an AtomicLong, we ensure that all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe, our servlet is once again thread-safe.","title":"Fixing UnsafeCountingFactorizer"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#locking","text":"To preserve state consistency, update related state variables in a single atomic operation.","title":"Locking"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#intrinsic-locks","text":"Java provides a built-in locking mechanism for enforcing atomicity: the synchronized block. A synchronized block has two parts: a reference to an object that will serve as the lock, and a block of code to be guarded by that lock. A synchronized method is a shorthand for a synchronized block that spans an entire method body, and whose lock is the object on which the method is being invoked. (Static synchronized methods use the Class object for the lock.) synchronized ( lock ) { // Access or modify shared state guarded by lock } Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block. The only way to acquire an intrinsic lock is to enter a synchronized block or method guarded by that lock. Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever. No thread executing a synchronized block can observe another thread to be in the middle of a synchronized block guarded by the same lock. That is, the synchronized blocks guarded by the same lock execute atomically with respect to one another. In the context of concurrency, atomicity means the same thing as it does in transactional applications\u2014that a group of statements appear to execute as a single, indivisible unit.","title":"Intrinsic locks"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#reentrancy","text":"When a thread requests a lock that is already held by another thread, the requesting thread blocks. But because intrinsic locks are reentrant, if a thread tries to acquire a lock that it already holds, the request succeeds. Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis. Reentrancy facilitates encapsulation of locking behavior, and thus simplifies the development of object-oriented concurrent code. public class Widget { public synchronized void doSomething () { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething () { System . out . println ( toString () + \": calling doSomething\" ); super . doSomething (); } } Without reentrant locks, the very natural-looking code above, in which a subclass overrides a synchronized method and then calls the superclass method, would deadlock. Because the doSomething methods in Widget and LoggingWidget are both synchronized, each tries to acquire the lock on the Widget before proceeding. But if intrinsic locks were not reentrant, the call to super.doSomething would never be able to acquire the lock because it would be considered already held, and the thread would permanently stall waiting for a lock it can never acquire. Reentrancy saves us from deadlock in situations like this.","title":"Reentrancy"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#guarding-state-with-locks","text":"Because locks enable serialized 1 access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following these protocols consistently can ensure state consistency. Compound actions on shared state, such as incrementing a hit counter ( read-modify-write ) or lazy initialization ( check-then-act ), must be made atomic to avoid race conditions. Holding a lock for the entire duration of a compound action can make that compound action atomic. However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed. It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Acquiring the lock associated with an object does not prevent other threads from accessing that object\u2014the only thing that acquiring a lock prevents any other thread from doing is acquiring that same lock. The fact that every object has a built-in lock is just a convenience so that you needn\u2019t explicitly create lock objects 2 . Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is. A common locking convention is to encapsulate all mutable state within an object and to protect it from concurrent access by synchronizing any code path that accesses mutable state using the object\u2019s intrinsic lock. Not all data needs to be guarded by locks\u2014only mutable data that will be accessed from multiple threads. When a variable is guarded by a lock\u2014meaning that every access to that variable is performed with that lock held\u2014you\u2019ve ensured that only one thread at a time can access that variable. When a class has invariants that involve more than one state variable, there is an additional requirement: each variable participating in the invariant must be guarded by the same lock. This allows you to access or update them in a single atomic operation, preserving the invariant. For every invariant that involves more than one variable, all the variables involved in that invariant must be guarded by the same lock. If synchronization is the cure for race conditions, why not just declare ev- ery method synchronized? if (! vector . contains ( element )) vector . add ( element ); This attempt at a put-if-absent operation has a race condition, even though both contains and add are atomic. While synchronized methods can make individual operations atomic, additional locking is required when multiple operations are combined into a compound action. At the same time, synchronizing every method can lead to liveness or performance problems.","title":"Guarding state with locks"},{"location":"java/concurrency/fundamentals/c02-thread-safety/#liveness-and-performance","text":"Deciding how big or small to make synchronized blocks may require tradeoffs among competing design forces, including safety (which must not be compromised), simplicity, and performance. Sometimes simplicity and performance are at odds with each other, a reasonable balance can usually be found. Whenever you use locking, you should be aware of what the code in the block is doing and how likely it is to take a long time to execute. Holding a lock for a long time, either because you are doing something compute-intensive or because you execute a potentially blocking operation, introduces the risk of liveness or performance problems. Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O. Serializing access to an object has nothing to do with object serialization (turning an object into a byte stream); serializing access means that threads take turns accessing the object exclusively, rather than doing so concurrently. \u21a9 In retrospect, this design decision was probably a bad one: not only can it be confusing, but it forces JVM implementors to make tradeoffs between object size and locking performance. \u21a9","title":"Liveness and performance"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/","text":"3. Sharing objects Visibility It is a common misconception that synchronized is only about atomicity or demarcating \u201ccritical sections\u201d. Synchronization also has another significant, and subtle, aspect: memory visibility. We want not only to prevent one thread from modifying the state of an object when another is using it, but also to ensure that when a thread modifies the state of an object, other threads can actually see the changes that were made. But without synchronization, this may not happen. In a single-threaded environment, if you write a value to a variable and later read that variable with no intervening writes, you can expect to get the same value back. This seems only natural. But when the reads and writes occur in different threads, this is simply not the case. In general, there is no guarantee that the reading thread will see a value written by another thread \u201cat the right time\u201d basis, or even at all. In order to ensure visibility of memory writes across threads, you must use synchronization. public class NoVisibility { private static boolean ready ; private static int number ; /* yield() provides a mechanism to inform the \u201cscheduler\u201d that the current thread is willing to relinquish its current use of processor but it'd like to be scheduled back soon as possible. */ private static class ReaderThread extends Thread { public void run () { while (! ready ) Thread . yield (); System . out . println ( number ); } } public static void main ( String [] args ) { new ReaderThread (). start (); number = 42 ; ready = true ; } } Two threads, the main thread and the reader thread, access the shared variables ready and number. The main thread starts the reader thread and then sets number to 42 and ready to true. The reader thread spins until it sees ready is true, and then prints out number. While it may seem obvious that NoVisibility will print 42, it is in fact possible that it will print zero, or never terminate at all! Because it does not use adequate synchronization, there is no guarantee that the values of ready and number written by the main thread will be visible to the reader thread. NoVisibility could loop forever because the value of ready might never become visible to the reader thread. Even more strangely, NoVisibility could print zero because the write to ready might be made visible to the reader thread before the write to number, a phenomenon known as reordering. There is no guarantee that operations in one thread will be performed in the order given by the program, as long as the reordering is not detectable from within that thread\u2014even if the reordering is apparent to other threads 1 . When the main thread writes first to number and then to ready without synchronization, the reader thread could see those writes happen in the opposite order\u2014or not at all. In the absence of synchronization, the compiler, processor, and runtime can do some downright weird things to the order in which operations appear to execute. Attempts to reason about the order in which memory actions \u201cmust\u201d happen in insufficiently synchronized multithreaded programs will almost certainly be incorrect. Always use the proper synchronization whenever data is shared across threads. Stale data NoVisibility demonstrated one of the ways that insufficiently synchronized programs can cause surprising results: stale data. When the reader thread examines ready, it may see an out-of-date value. Unless synchronization is used every time a variable is accessed , it is possible to see a stale value for that variable. Worse, staleness is not all-or-nothing: a thread can see an up-to-date value of one variable but a stale value of another variable that was written first. Stale values can cause serious safety or liveness failures. In NoVisibility, stale values could cause it to print the wrong value or prevent the program from terminating. Things can get even more complicated with stale values of object references, such as the link pointers in a linked list implementation. Stale data can cause serious and confusing failures such as unexpected exceptions, corrupted data structures, inaccurate computations, and infinite loops. In a not thread-safe program where a value field is accessed from both get and set without synchronization it is susceptible to stale values: if one thread calls set, other threads calling get may or may not see that update. It can become safe by synchronizing the getter and setter. Synchronizing only the setter would not be sufficient: threads calling get would still be able to see stale values. Nonatomic 64-bit operations When a thread reads a variable without synchronization, it may see a stale value, but at least it sees a value that was actually placed there by some thread rather than some random value. This safety guarantee is called out-of-thin-air safety. Out-of-thin-air safety applies to all variables, with one exception: 64-bit numeric variables (double and long) that are not declared volatile. The Java Memory Model requires fetch and store operations to be atomic, but for nonvolatile long and double variables, the JVM is permitted to treat a 64-bit read or write as two separate 32-bit operations. If the reads and writes occur in different threads, it is therefore possible to read a nonvolatile long and get back the high 32 bits of one value and the low 32 bits of another 2 . Thus, even if you don\u2019t care about stale values, it is not safe to use shared mutable long and double variables in multithreaded programs unless they are declared volatile or guarded by a lock. Locking and visibility (piggybacking?) When thread A executes a synchronized block, and subsequently thread B enters a synchronized block guarded by the same lock, the values of variables that were visible to A prior to releasing the lock are guaranteed to be visible to B upon acquiring the lock. In other words, everything A did in or prior to a synchronized block is visible to B when it executes a synchronized block guarded by the same lock. Without synchronization, there is no such guarantee. Locking is not just about mutual exclusion; it is also about memory visibility. To ensure that all threads see the most up-to-date values of shared mutable variables, the reading and writing threads must synchronize on a common lock. Volatile variables The Java language also provides an alternative, weaker form of synchronization, volatile variables, to ensure that updates to a variable are propagated predictably to other threads. When a field is declared volatile, the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. Yet accessing a volatile variable performs no locking and so cannot cause the executing thread to block, making volatile variables a lighter-weight synchronization mechanism than synchronized 3 . So from a memory visibility perspective, writing a volatile variable is like exiting a synchronized block and reading a volatile variable is like entering a synchronized block. Use volatile variables only when they simplify implementing and verifying your synchronization policy; Good uses of volatile variables include ensuring the visibility of their own state, that of the object they refer to, or indicating that an important life-cycle event (such as initialization or shutdown) has occurred. volatile boolean asleep ; ... while (! asleep ) countSomeSheep (); For this example to work, the asleep flag must be volatile. Otherwise, the thread might not notice when asleep has been set by another thread. We could instead have used locking to ensure visibility of changes to asleep, but that would have made the code more cumbersome. Volatile variables are convenient, but they have limitations. The most common use for volatile variables is as a completion, interruption, or status flag, such as the asleep flag in above sample. Volatile variables can be used for other kinds of state information, but more care is required when attempting this. For example, the semantics of volatile are not strong enough to make the increment operation ( count++ ) atomic, unless you can guarantee that the variable is written only from a single thread. (Atomic variables do provide atomic read-modify-write support and can often be used as \u201cbetter volatile variables\u201d). Locking can guarantee both visibility and atomicity; volatile variables can only guarantee visibility. You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value. The variable does not participate in invariants with other state variables. Locking is not required for any other reason while the variable is being accessed. More about volatile Overview In the absence of necessary synchronizations, the compiler, runtime, or processors may apply all sorts of optimizations. Even though these optimizations are beneficial most of the time, sometimes they can cause subtle issues. Caching and reordering are among those optimizations that may surprise us in concurrent contexts. Java and the JVM provide many ways to control memory order, and the volatile keyword is one of them. Shared Multiprocessor Architecture Processors are responsible for executing program instructions. Therefore, they need to retrieve both program instructions and required data from RAM. As CPUs are capable of carrying out a significant number of instructions per second, fetching from RAM is not that ideal for them. To improve this situation, processors are using tricks like Out of Order Execution, Branch Prediction, Speculative Execution, and, of course, Caching. This is where the following memory hierarchy comes into play: As different cores execute more instructions and manipulate more data, they fill up their caches with more relevant data and instructions. This will improve the overall performance at the expense of introducing cache coherence challenges. Put simply, we should think twice about what happens when one thread updates a cached value. Memory Visibility (more about NoVisibility ) In this simple example, we have two application threads: the main thread and the reader thread. Let's imagine a scenario in which the OS schedules those threads on two different CPU cores, where: The main thread has its copy of ready and number variables in its core cache The reader thread ends up with its copies, too The main thread updates the cached values On most modern processors, write requests won't be applied right away after they're issued. In fact, processors tend to queue those writes in a special write buffer. After a while, they will apply those writes to main memory all at once. With all that being said, when the main thread updates the number and ready variables, there is no guarantee about what the reader thread may see. In other words, the reader thread may see the updated value right away, or with some delay, or never at all! This memory visibility may cause liveness issues in programs that are relying on visibility. Reordering To make matters even worse, the reader thread may see those writes in any order other than the actual program order. For instance, since we first update the number variable: public static void main ( String [] args ) { new Reader (). start (); number = 42 ; ready = true ; } We may expect the reader thread prints 42. However, it's actually possible to see zero as the printed value! The reordering is an optimization technique for performance improvements. Interestingly, different components may apply this optimization: The processor may flush its write buffer in any order other than the program order The processor may apply out-of-order execution technique The JIT compiler may optimize via reordering To ensure that updates to variables propagate predictably to other threads, we should apply the volatile modifier to those variables. This way, we communicate with runtime and processor to not reorder any instruction involving the volatile variable. Also, processors understand that they should flush any updates to these variables right away. volatile and Thread Synchronization For multithreaded applications, we need to ensure a couple of rules for consistent behavior: Mutual Exclusion \u2013 only one thread executes a critical section at a time Visibility \u2013 changes made by one thread to the shared data are visible to other threads to maintain data consistency synchronized methods and blocks provide both of the above properties, at the cost of application performance. volatile is quite a useful keyword because it can help ensure the visibility aspect of the data change without, of course, providing mutual exclusion. Thus, it's useful in the places where we're ok with multiple threads executing a block of code in parallel, but we need to ensure the visibility property. Happens-Before Ordering The memory visibility effects of volatile variables extend beyond the volatile variables themselves. To make matters more concrete, let's suppose thread A writes to a volatile variable, and then thread B reads the same volatile variable. In such cases, the values that were visible to A before writing the volatile variable will be visible to B after reading the volatile variable. Piggybacking Because of the strength of the happens-before memory ordering, sometimes we can piggyback on the visibility properties of another volatile variable. For instance, in our particular example, we just need to mark the ready variable as volatile: public class TaskRunner { private static int number ; // not volatile private volatile static boolean ready ; // same as before } Anything prior to writing true to the ready variable is visible to anything after reading the ready variable. Therefore, the number variable piggybacks on the memory visibility enforced by the ready variable. Put simply, even though it's not a volatile variable, it is exhibiting a volatile behavior. By making use of these semantics, we can define only a few of the variables in our class as volatile and optimize the visibility guarantee. Publication and escape Publishing an object means making it available to code outside of its current scope, such as by storing a reference to it where other code can find it, returning it from a nonprivate method, or passing it to a method in another class. In many situations, we want to ensure that objects and their internals are not published. In other situations, we do want to publish an object for general use, but doing so in a thread-safe manner may require synchronization. Publishing internal state variables can compromise encapsulation and make it more difficult to preserve invariants; publishing objects before they are fully constructed can compromise thread safety. An object that is published when it should not have been is said to have escaped. The most blatant form of publication is to store a reference in a public static field, where any class and thread could see it: public static Set < Secret > knownSecrets ; public void initialize () { knownSecrets = new HashSet < Secret >(); } // Don't do this class UnsafeStates { private String [] states = new String [] { \"AK\" , \"AL\" ... }; public String [] getStates () { return states ; } } Publishing one object may indirectly publish others. If you add a Secret to the published knownSecrets set, you\u2019ve also published that Secret, because any code can iterate the Set and obtain a reference to the new Secret. Publishing states in UnsafeStates way is problematic because any caller can modify its contents. In this case, the states array has escaped its intended scope, because what was supposed to be private state has been effectively made public. Whether another thread actually does something with a published reference doesn\u2019t really matter, because the risk of misuse is still present. Once an object escapes, you have to assume that another class or thread may, maliciously or carelessly, misuse it. This is a compelling reason to use encapsulation: it makes it practical to analyze programs for correctness and harder to violate design constraints accidentally. A final mechanism by which an object or its internal state can be published is to publish an inner class instance: public class ThisEscape { // Don't do this public ThisEscape ( EventSource source ) { source . registerListener ( new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }); } } } When ThisEscape publishes the EventListener , it implicitly publishes the enclosing ThisEscape instance as well, because inner class instances contain a hidden reference to the enclosing instance. Safe construction practices ThisEscape illustrates an important special case of escape\u2014when the this references escapes during construction. When the inner EventListener instance is published, so is the enclosing ThisEscape instance. But an object is in a predictable, consistent state only after its constructor returns, so publishing an object from within its constructor can publish an incompletely constructed object. This is true even if the publication is the last statement in the constructor. If the this reference escapes during construction, the object is considered not properly constructed. Do not allow the this reference to escape during construction. A common mistake that can let the this reference escape during construction is to start a thread from a constructor. When an object creates a thread from its constructor, it almost always shares its this reference with the new thread, either explicitly (by passing it to the constructor) or implicitly (because the Thread or Runnable is an inner class of the owning object). The new thread might then be able to see the owning object before it is fully constructed. There\u2019s nothing wrong with creating a thread in a constructor, but it is best not to start the thread immediately. Instead, expose a start or initialize method that starts the owned thread. Calling an overrideable instance method (one that is neither private nor final) from the constructor can also allow the this reference to escape. If you are tempted to register an event listener or start a thread from a constructor, you can avoid the improper construction by using a private constructor and a public factory method. public class SafeListener { private final EventListener listener ; private SafeListener () { listener = new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }; } public static SafeListener newInstance ( EventSource source ) { SafeListener safe = new SafeListener (); source . registerListener ( safe . listener ); return safe ; } } Thread confinement Accessing shared, mutable data requires using synchronization; one way to avoid this requirement is to not share. If data is only accessed from a single thread, no synchronization is needed. This technique, thread confinement, is one of the simplest ways to achieve thread safety. When an object is confined to a thread, such usage is automatically thread-safe even if the confined object itself is not. Examples: Swing: The Swing visual components and data model objects are not thread safe; instead, safety is achieved by confining them to the Swing event dispatch thread. To use Swing properly, code running in threads other than the event thread should not access these objects. (To make this easier, Swing provides the invokeLater mechanism to schedule a Runnable for execution in the event thread.) Many concurrency errors in Swing applications stem from improper use of these confined objects from another thread. Pooled JDBC Connection objects: The JDBC specification does not require that Connection objects be thread-safe 4 . In typical server applications, a thread acquires a connection from the pool, uses it for processing a single request, and returns it. Since most requests, such as servlet requests or EJB (Enterprise JavaBeans) calls, are processed synchronously by a single thread, and the pool will not dispense the same connection to another thread until it has been returned, this pattern of connection management implicitly confines the Connection to that thread for the duration of the request. Just as the language has no mechanism for enforcing that a variable is guarded by a lock, it has no means of confining an object to a thread. Thread confinement is an element of your program\u2019s design that must be enforced by its implementation. The language and core libraries provide mechanisms that can help in maintaining thread confinement\u2014local variables and the ThreadLocal class\u2014but even with these, it is still the programmer\u2019s responsibility to ensure that thread-confined objects do not escape from their intended thread. Ad-hoc thread confinement Ad-hoc thread confinement describes when the responsibility for maintaining thread confinement falls entirely on the implementation. Ad-hoc thread confinement can be fragile because none of the language features, such as visibility modifiers or local variables, helps confine the object to the target thread. In fact, references to thread-confined objects such as visual components or data models in GUI applications are often held in public fields. The decision to use thread confinement is often a consequence of the decision to implement a particular subsystem, such as the GUI, as a single-threaded sub- system. Single-threaded subsystems can sometimes offer a simplicity benefit that outweighs the fragility of ad-hoc thread confinement 5 . A special case of thread confinement applies to volatile variables. It is safe to perform read-modify-write operations on shared volatile variables as long as you ensure that the volatile variable is only written from a single thread. In this case, you are confining the modification to a single thread to prevent race conditions, and the visibility guarantees for volatile variables ensure that other threads see the most up-to-date value. Because of its fragility, ad-hoc thread confinement should be used sparingly; if possible, use one of the stronger forms of thread confinment (stack confinement or ThreadLocal) instead. Stack confinement Stack confinement is a special case of thread confinement in which an object can only be reached through local variables. Just as encapsulation can make it easier to preserve invariants, local variables can make it easier to confine objects to a thread. Local variables are intrinsically confined to the executing thread; they exist on the executing thread\u2019s stack, which is not accessible to other threads. Stack confinement (also called within-thread or thread-local usage, but not to be confused with the ThreadLocal library class) is simpler to maintain and less fragile than ad-hoc thread confinement. For primitively typed local variables, such as numPairs in loadTheArk in below sample you cannot violate stack confinement even if you tried. There is no way to obtain a reference to a primitive variable, so the language semantics ensure that primitive local variables are always stack confined. public int loadTheArk ( Collection < Animal > candidates ) { SortedSet < Animal > animals ; int numPairs = 0 ; Animal candidate = null ; // animals confined to method, don\u2019t let them escape! animals = new TreeSet < Animal >( new SpeciesGenderComparator ()); animals . addAll ( candidates ); for ( Animal a : animals ) { if ( candidate == null || ! candidate . isPotentialMate ( a )) candidate = a ; else { ark . load ( new AnimalPair ( candidate , a )); ++ numPairs ; candidate = null ; } } return numPairs ; } In loadTheArk, we instantiate a TreeSet and store a reference to it in animals. At this point, there is exactly one reference to the Set, held in a local variable and therefore confined to the executing thread. However, if we were to publish a reference to the Set (or any of its internals), the confinement would be violated and the animals would escape. The design requirement that the object be confined to the executing thread, or the awareness that the confined object is not thread-safe, often exists only in the head of the developer when the code is written. If the assumption of within-thread usage is not clearly documented, future maintainers might mistakenly allow the object to escape. ThreadLocal (typically private static fields) A more formal means of maintaining thread confinement is ThreadLocal, which allows you to associate a per-thread value with a value-holding object. ThreadLocal provides get and set accessor methods that maintain a separate copy of the value for each thread that uses it, so a get returns the most recent value passed to set from the currently executing thread. Thread-local variables are often used to prevent sharing in designs based on mutable Singletons or global variables. For example, a single-threaded application might maintain a global database connection that is initialized at startup to avoid having to pass a Connection to every method. Since JDBC connections may not be thread-safe, a multithreaded application that uses a global connection without additional coordination is not thread-safe either. By using a ThreadLocal to store the JDBC connection, as in ConnectionHolder in below sample, each thread will have its own connection. private static ThreadLocal < Connection > connectionHolder = new ThreadLocal < Connection >() { public Connection initialValue () { return DriverManager . getConnection ( DB_URL ); } }; public static Connection getConnection () { return connectionHolder . get (); } When a thread calls ThreadLocal.get for the first time, initialValue is consulted to provide the initial value for that thread. The thread-specific values are stored in the Thread object itself; when the thread terminates, the thread-specific values can be garbage collected. If you are porting a single-threaded application to a multithreaded environment, you can preserve thread safety by converting shared global variables into ThreadLocals, if the semantics of the shared globals permits this; an application- wide cache would not be as useful if it were turned into a number of thread-local caches. It is easy to abuse ThreadLocal by treating its thread confinement property as a license to use global variables or as a means of creating \u201chidden\u201d method arguments. Like global variables, thread-local variables can detract from reusability and introduce hidden couplings among classes, and should therefore be used with care. Immutability If an object\u2019s state cannot be modified, these risks and complexities simply go away. An immutable object is one whose state cannot be changed after construction. Immutable objects are inherently thread-safe; their invariants are established by the constructor, and if their state cannot be changed, these invariants always hold. Immutable objects are always thread-safe. Immutable objects are simple. They can only be in one state, which is carefully controlled by the constructor. One of the most difficult elements of program design is reasoning about the possible states of complex objects. Reasoning about the state of immutable objects, on the other hand, is trivial. Immutable objects are also safer. Passing a mutable object to untrusted code, or otherwise publishing it where untrusted code could find it, is dangerous\u2014the untrusted code might modify its state, or, worse, retain a reference to it and modify its state later from another thread. On the other hand, immutable objects cannot be subverted in this manner by malicious or buggy code, so they are safe to share and publish freely without the need to make defensive copies. Neither the Java Language Specification nor the Java Memory Model formally defines immutability, but immutability is not equivalent to simply declaring all fields of an object final. An object whose fields are all final may still be mutable, since final fields can hold references to mutable objects. An object is immutable if: Its state cannot be modified after construction. All its fields are final. It is properly constructed (the this reference does not escape during construction). While the Set that stores the names is mutable, the design of ThreeStooges makes it impossible to modify that Set after construction. The stooges reference is final, so all object state is reached through a final field. The last requirement, proper construction, is easily met since the constructor does nothing that would cause the this reference to become accessible to code other than the constructor and its caller. @Immutable public final class ThreeStooges { private final Set < String > stooges = new HashSet < String >(); public ThreeStooges () { stooges . add ( \"Moe\" ); stooges . add ( \"Larry\" ); stooges . add ( \"Curly\" ); } public boolean isStooge ( String name ) { return stooges . contains ( name ); } } Final fields The final keyword, a more limited version of the const mechanism from C++, supports the construction of immutable objects. Final fields can\u2019t be modified (although the objects they refer to can be modified if they are mutable), but they also have special semantics under the Java Memory Model. It is the use of final fields that makes possible the guarantee of initialization safety that lets immutable objects be freely accessed and shared without synchronization. Even if an object is mutable, making some fields final can still simplify reasoning about its state, since limiting the mutability of an object restricts its set of possible states. An object that is \u201cmostly immutable\u201d but has one or two mutable state variables is still simpler than one that has many mutable variables. Declaring fields final also documents to maintainers that these fields are not expected to change. Just as it is a good practice to make all fields private unless they need greater visibility, it is a good practice to make all fields final unless they need to be mutable. Example: Using volatile to publish immutable objects Immutable objects can sometimes provide a weak form of atomicity. The factoring servlet performs two operations that must be atomic: updating the cached result and conditionally fetching the cached factors if the cached number matches the requested number. Whenever a group of related data items must be acted on atomically, consider creating an immutable holder class for them, such as OneValueCache 6 : @Immutable class OneValueCache { private final BigInteger lastNumber ; private final BigInteger [] lastFactors ; public OneValueCache ( BigInteger i , BigInteger [] factors ) { lastNumber = i ; lastFactors = Arrays . copyOf ( factors , factors . length ); } public BigInteger [] getFactors ( BigInteger i ) { if ( lastNumber == null || ! lastNumber . equals ( i )) return null ; else return Arrays . copyOf ( lastFactors , lastFactors . length ); } } Race conditions in accessing or updating multiple related variables can be eliminated by using an immutable object to hold all the variables. With a mutable holder object, you would have to use locking to ensure atomicity; with an im- mutable one, once a thread acquires a reference to it, it need never worry about another thread modifying its state. If the variables are to be updated, a new holder object is created, but any threads working with the previous holder still see it in a consistent state. VolatileCachedFactorizer uses a OneValueCache to store the cached number and factors. When a thread sets the volatile cache field to reference a new OneValueCache , the new cached data becomes immediately visible to other threads. The cache-related operations cannot interfere with each other because OneValueCache is immutable and the cache field is accessed only once in each of the relevant code paths. This combination of an immutable holder object for multiple state variables related by an invariant, and a volatile reference used to ensure its timely visibility, allows VolatileCachedFactorizer to be thread-safe even though it does no explicit locking. @ThreadSafe public class VolatileCachedFactorizer implements Servlet { private volatile OneValueCache cache = new OneValueCache ( null , null ); public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = cache . getFactors ( i ); if ( factors == null ) { factors = factor ( i ); cache = new OneValueCache ( i , factors ); } encodeIntoResponse ( resp , factors ); } } Safe publication Sometimes we do want to share objects across threads, and in this case we must do so safely. Unfortunately, simply storing a reference to an object into a public field, is not enough to publish that object safely. // Unsafe publication public Holder holder ; public void initialize () { holder = new Holder ( 42 ); } Because of visibility problems, the Holder could appear to another thread to be in an inconsistent state, even though its invariants were properly established by its constructor! This improper publication could allow another thread to observe a partially constructed object. Improper publication: when good objects go bad You cannot rely on the integrity of partially constructed objects. An observing thread could see the object in an inconsistent state, and then later see its state suddenly change, even though it has not been modified since publication 7 . Because synchronization was not used to make the Holder visible to other threads, we say the Holder was not properly published. Other threads could see a stale value for the holder field, and thus see a null reference or other older value even though a value has been placed in holder. But far worse, other threads could see an up-to-date value for the holder reference, but stale values for the state of the Holder. To make things even less predictable, a thread may see a stale value the first time it reads a field and then a more up-to-date value the next time. Immutable objects and initialization safety Because immutable objects are so important, the Java Memory Model offers a special guarantee of initialization safety for sharing immutable objects. As we\u2019ve seen, that an object reference becomes visible to another thread does not necessarily mean that the state of that object is visible to the consuming thread. In order to guarantee a consistent view of the object\u2019s state, synchronization is needed. Immutable objects, on the other hand, can be safely accessed even when synchronization is not used to publish the object reference. For this guarantee of initialization safety to hold, all of the requirements for immutability must be met: unmodifiable state, all fields are final, and proper construction. Immutable objects can be used safely by any thread without additional synchronization, even when synchronization is not used to publish them. This guarantee extends to the values of all final fields of properly constructed objects; final fields can be safely accessed without additional synchronization. However, if final fields refer to mutable objects, synchronization is still required to access the state of the objects they refer to. Safe publication idioms Objects that are not immutable must be safely published, which usually entails synchronization by both the publishing and the consuming thread. To publish an object safely, both the reference to the object and the object\u2019s state must be made visible to other threads at the same time. A properly constructed object can be safely published by: Initializing an object reference from a static initializer; Storing a reference to it into a volatile field or AtomicReference; Storing a reference to it into a final field of a properly constructed object; Storing a reference to it into a field that is properly guarded by a lock. The internal synchronization in thread-safe collections means that placing an object in a thread-safe collection, such as a Vector or synchronizedList, fulfills the last of these requirements. If thread A places object X in a thread-safe collection and thread B subsequently retrieves it, B is guaranteed to see the state of X as A left it, even though the application code that hands X off in this manner has no explicit synchronization. The thread-safe library collections offer the following safe publication guarantees, even if the Javadoc is less than clear on the subject: Placing a key or value in a Hashtable, synchronizedMap, or ConcurrentMap safely publishes it to any thread that retrieves it from the Map (whether directly or via an iterator); Placing an element in a Vector, CopyOnWriteArrayList, CopyOnWriteArraySet, synchronizedList, or synchronizedSet safely publishes it to any thread that retrieves it from the collection; Placing an element on a BlockingQueue or a ConcurrentLinkedQueue safely publishes it to any thread that retrieves it from the queue. Using a static initializer is often the easiest and safest way to publish objects that can be statically constructed: public static Holder holder = new Holder(42); Static initializers are executed by the JVM at class initialization time; because of internal synchronization in the JVM, this mechanism is guaranteed to safely publish any objects initialized in this way Effectively immutable objects The safe publication mechanisms all guarantee that the as-published state of an object is visible to all accessing threads as soon as the reference to it is visible, and if that state is not going to be changed again, this is sufficient to ensure that any access is safe. Objects that are not technically immutable, but whose state will not be modified after publication, are called effectively immutable. They do not need to meet the strict definition of immutability; they merely need to be treated by the program as if they were immutable after they are published. Using effectively immutable objects can simplify development and improve performance by reducing the need for synchronization. Safely published effectively immutable objects can be used safely by any thread without additional synchronization. Mutable objects If an object may be modified after construction, safe publication ensures only the visibility of the as-published state. Synchronization must be used not only to publish a mutable object, but also every time the object is accessed to ensure visibility of subsequent modifications. To share mutable objects safely, they must be safely published and be either thread-safe or guarded by a lock. The publication requirements for an object depend on its mutability: Immutable objects can be published through any mechanism. Effectively immutable objects must be safely published. Mutable objects must be safely published, and must be either thread-safe or guarded by a lock. Sharing objects safely Whenever you acquire a reference to an object, you should know what you are allowed to do with it. Do you need to acquire a lock before using it? Are you allowed to modify its state, or only to read it? Many concurrency errors stem from failing to understand these \u201crules of engagement\u201d for a shared object. When you publish an object, you should document how the object can be accessed. The most useful policies for using and sharing objects in a concurrent program are: Thread-confined. A thread-confined object is owned exclusively by and confined to one thread, and can be modified by its owning thread. Shared read-only. A shared read-only object can be accessed concurrently by multiple threads without additional synchronization, but cannot be modified by any thread. Shared read-only objects include immutable and effectively immutable objects. Shared thread-safe. A thread-safe object performs synchronization internally, so multiple threads can freely access it through its public interface without further synchronization. Guarded. A guarded object can be accessed only with a specific lock held. Guarded objects include those that are encapsulated within other thread-safe objects and published objects that are known to be guarded by a specific lock. This may seem like a broken design, but it is meant to allow JVMs to take full advantage of the performance of modern multiprocessor hardware. For example, in the absence of synchronization, the Java Memory Model permits the compiler to reorder operations and cache values in registers, and permits CPUs to reorder operations and cache values in processor-specific caches. \u21a9 When the Java Virtual Machine Specification was written, many widely used processor architectures could not efficiently provide atomic 64-bit arithmetic operations. \u21a9 Volatile reads are only slightly more expensive than nonvolatile reads on most current processor architectures. \u21a9 The connection pool implementations provided by application servers are thread-safe; connection pools are necessarily accessed from multiple threads, so a non-thread-safe implementation would not make sense. \u21a9 Another reason to make a subsystem single-threaded is deadlock avoidance; this is one of the primary reasons most GUI frameworks are single-threaded. \u21a9 OneValueCache wouldn\u2019t be immutable without the copyOf calls in the constructor and getter. Arrays.copyOf was added as a convenience in Java 6; clone would also work. \u21a9 The problem here is not the Holder class itself, but that the Holder is not properly published. However, Holder can be made immune to improper publication by declaring the n field to be final which would make Holder immutable. \u21a9","title":"Sharing objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#3-sharing-objects","text":"","title":"3. Sharing objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#visibility","text":"It is a common misconception that synchronized is only about atomicity or demarcating \u201ccritical sections\u201d. Synchronization also has another significant, and subtle, aspect: memory visibility. We want not only to prevent one thread from modifying the state of an object when another is using it, but also to ensure that when a thread modifies the state of an object, other threads can actually see the changes that were made. But without synchronization, this may not happen. In a single-threaded environment, if you write a value to a variable and later read that variable with no intervening writes, you can expect to get the same value back. This seems only natural. But when the reads and writes occur in different threads, this is simply not the case. In general, there is no guarantee that the reading thread will see a value written by another thread \u201cat the right time\u201d basis, or even at all. In order to ensure visibility of memory writes across threads, you must use synchronization. public class NoVisibility { private static boolean ready ; private static int number ; /* yield() provides a mechanism to inform the \u201cscheduler\u201d that the current thread is willing to relinquish its current use of processor but it'd like to be scheduled back soon as possible. */ private static class ReaderThread extends Thread { public void run () { while (! ready ) Thread . yield (); System . out . println ( number ); } } public static void main ( String [] args ) { new ReaderThread (). start (); number = 42 ; ready = true ; } } Two threads, the main thread and the reader thread, access the shared variables ready and number. The main thread starts the reader thread and then sets number to 42 and ready to true. The reader thread spins until it sees ready is true, and then prints out number. While it may seem obvious that NoVisibility will print 42, it is in fact possible that it will print zero, or never terminate at all! Because it does not use adequate synchronization, there is no guarantee that the values of ready and number written by the main thread will be visible to the reader thread. NoVisibility could loop forever because the value of ready might never become visible to the reader thread. Even more strangely, NoVisibility could print zero because the write to ready might be made visible to the reader thread before the write to number, a phenomenon known as reordering. There is no guarantee that operations in one thread will be performed in the order given by the program, as long as the reordering is not detectable from within that thread\u2014even if the reordering is apparent to other threads 1 . When the main thread writes first to number and then to ready without synchronization, the reader thread could see those writes happen in the opposite order\u2014or not at all. In the absence of synchronization, the compiler, processor, and runtime can do some downright weird things to the order in which operations appear to execute. Attempts to reason about the order in which memory actions \u201cmust\u201d happen in insufficiently synchronized multithreaded programs will almost certainly be incorrect. Always use the proper synchronization whenever data is shared across threads.","title":"Visibility"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#stale-data","text":"NoVisibility demonstrated one of the ways that insufficiently synchronized programs can cause surprising results: stale data. When the reader thread examines ready, it may see an out-of-date value. Unless synchronization is used every time a variable is accessed , it is possible to see a stale value for that variable. Worse, staleness is not all-or-nothing: a thread can see an up-to-date value of one variable but a stale value of another variable that was written first. Stale values can cause serious safety or liveness failures. In NoVisibility, stale values could cause it to print the wrong value or prevent the program from terminating. Things can get even more complicated with stale values of object references, such as the link pointers in a linked list implementation. Stale data can cause serious and confusing failures such as unexpected exceptions, corrupted data structures, inaccurate computations, and infinite loops. In a not thread-safe program where a value field is accessed from both get and set without synchronization it is susceptible to stale values: if one thread calls set, other threads calling get may or may not see that update. It can become safe by synchronizing the getter and setter. Synchronizing only the setter would not be sufficient: threads calling get would still be able to see stale values.","title":"Stale data"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#nonatomic-64-bit-operations","text":"When a thread reads a variable without synchronization, it may see a stale value, but at least it sees a value that was actually placed there by some thread rather than some random value. This safety guarantee is called out-of-thin-air safety. Out-of-thin-air safety applies to all variables, with one exception: 64-bit numeric variables (double and long) that are not declared volatile. The Java Memory Model requires fetch and store operations to be atomic, but for nonvolatile long and double variables, the JVM is permitted to treat a 64-bit read or write as two separate 32-bit operations. If the reads and writes occur in different threads, it is therefore possible to read a nonvolatile long and get back the high 32 bits of one value and the low 32 bits of another 2 . Thus, even if you don\u2019t care about stale values, it is not safe to use shared mutable long and double variables in multithreaded programs unless they are declared volatile or guarded by a lock.","title":"Nonatomic 64-bit operations"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#locking-and-visibility-piggybacking","text":"When thread A executes a synchronized block, and subsequently thread B enters a synchronized block guarded by the same lock, the values of variables that were visible to A prior to releasing the lock are guaranteed to be visible to B upon acquiring the lock. In other words, everything A did in or prior to a synchronized block is visible to B when it executes a synchronized block guarded by the same lock. Without synchronization, there is no such guarantee. Locking is not just about mutual exclusion; it is also about memory visibility. To ensure that all threads see the most up-to-date values of shared mutable variables, the reading and writing threads must synchronize on a common lock.","title":"Locking and visibility (piggybacking?)"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#volatile-variables","text":"The Java language also provides an alternative, weaker form of synchronization, volatile variables, to ensure that updates to a variable are propagated predictably to other threads. When a field is declared volatile, the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. Yet accessing a volatile variable performs no locking and so cannot cause the executing thread to block, making volatile variables a lighter-weight synchronization mechanism than synchronized 3 . So from a memory visibility perspective, writing a volatile variable is like exiting a synchronized block and reading a volatile variable is like entering a synchronized block. Use volatile variables only when they simplify implementing and verifying your synchronization policy; Good uses of volatile variables include ensuring the visibility of their own state, that of the object they refer to, or indicating that an important life-cycle event (such as initialization or shutdown) has occurred. volatile boolean asleep ; ... while (! asleep ) countSomeSheep (); For this example to work, the asleep flag must be volatile. Otherwise, the thread might not notice when asleep has been set by another thread. We could instead have used locking to ensure visibility of changes to asleep, but that would have made the code more cumbersome. Volatile variables are convenient, but they have limitations. The most common use for volatile variables is as a completion, interruption, or status flag, such as the asleep flag in above sample. Volatile variables can be used for other kinds of state information, but more care is required when attempting this. For example, the semantics of volatile are not strong enough to make the increment operation ( count++ ) atomic, unless you can guarantee that the variable is written only from a single thread. (Atomic variables do provide atomic read-modify-write support and can often be used as \u201cbetter volatile variables\u201d). Locking can guarantee both visibility and atomicity; volatile variables can only guarantee visibility. You can use volatile variables only when all the following criteria are met: Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value. The variable does not participate in invariants with other state variables. Locking is not required for any other reason while the variable is being accessed.","title":"Volatile variables"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#more-about-volatile","text":"","title":"More about volatile"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#overview","text":"In the absence of necessary synchronizations, the compiler, runtime, or processors may apply all sorts of optimizations. Even though these optimizations are beneficial most of the time, sometimes they can cause subtle issues. Caching and reordering are among those optimizations that may surprise us in concurrent contexts. Java and the JVM provide many ways to control memory order, and the volatile keyword is one of them.","title":"Overview"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#shared-multiprocessor-architecture","text":"Processors are responsible for executing program instructions. Therefore, they need to retrieve both program instructions and required data from RAM. As CPUs are capable of carrying out a significant number of instructions per second, fetching from RAM is not that ideal for them. To improve this situation, processors are using tricks like Out of Order Execution, Branch Prediction, Speculative Execution, and, of course, Caching. This is where the following memory hierarchy comes into play: As different cores execute more instructions and manipulate more data, they fill up their caches with more relevant data and instructions. This will improve the overall performance at the expense of introducing cache coherence challenges. Put simply, we should think twice about what happens when one thread updates a cached value.","title":"Shared Multiprocessor Architecture"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#memory-visibility-more-about-novisibility","text":"In this simple example, we have two application threads: the main thread and the reader thread. Let's imagine a scenario in which the OS schedules those threads on two different CPU cores, where: The main thread has its copy of ready and number variables in its core cache The reader thread ends up with its copies, too The main thread updates the cached values On most modern processors, write requests won't be applied right away after they're issued. In fact, processors tend to queue those writes in a special write buffer. After a while, they will apply those writes to main memory all at once. With all that being said, when the main thread updates the number and ready variables, there is no guarantee about what the reader thread may see. In other words, the reader thread may see the updated value right away, or with some delay, or never at all! This memory visibility may cause liveness issues in programs that are relying on visibility.","title":"Memory Visibility (more about NoVisibility)"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#reordering","text":"To make matters even worse, the reader thread may see those writes in any order other than the actual program order. For instance, since we first update the number variable: public static void main ( String [] args ) { new Reader (). start (); number = 42 ; ready = true ; } We may expect the reader thread prints 42. However, it's actually possible to see zero as the printed value! The reordering is an optimization technique for performance improvements. Interestingly, different components may apply this optimization: The processor may flush its write buffer in any order other than the program order The processor may apply out-of-order execution technique The JIT compiler may optimize via reordering To ensure that updates to variables propagate predictably to other threads, we should apply the volatile modifier to those variables. This way, we communicate with runtime and processor to not reorder any instruction involving the volatile variable. Also, processors understand that they should flush any updates to these variables right away.","title":"Reordering"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#volatile-and-thread-synchronization","text":"For multithreaded applications, we need to ensure a couple of rules for consistent behavior: Mutual Exclusion \u2013 only one thread executes a critical section at a time Visibility \u2013 changes made by one thread to the shared data are visible to other threads to maintain data consistency synchronized methods and blocks provide both of the above properties, at the cost of application performance. volatile is quite a useful keyword because it can help ensure the visibility aspect of the data change without, of course, providing mutual exclusion. Thus, it's useful in the places where we're ok with multiple threads executing a block of code in parallel, but we need to ensure the visibility property.","title":"volatile and Thread Synchronization"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#happens-before-ordering","text":"The memory visibility effects of volatile variables extend beyond the volatile variables themselves. To make matters more concrete, let's suppose thread A writes to a volatile variable, and then thread B reads the same volatile variable. In such cases, the values that were visible to A before writing the volatile variable will be visible to B after reading the volatile variable.","title":"Happens-Before Ordering"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#piggybacking","text":"Because of the strength of the happens-before memory ordering, sometimes we can piggyback on the visibility properties of another volatile variable. For instance, in our particular example, we just need to mark the ready variable as volatile: public class TaskRunner { private static int number ; // not volatile private volatile static boolean ready ; // same as before } Anything prior to writing true to the ready variable is visible to anything after reading the ready variable. Therefore, the number variable piggybacks on the memory visibility enforced by the ready variable. Put simply, even though it's not a volatile variable, it is exhibiting a volatile behavior. By making use of these semantics, we can define only a few of the variables in our class as volatile and optimize the visibility guarantee.","title":"Piggybacking"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#publication-and-escape","text":"Publishing an object means making it available to code outside of its current scope, such as by storing a reference to it where other code can find it, returning it from a nonprivate method, or passing it to a method in another class. In many situations, we want to ensure that objects and their internals are not published. In other situations, we do want to publish an object for general use, but doing so in a thread-safe manner may require synchronization. Publishing internal state variables can compromise encapsulation and make it more difficult to preserve invariants; publishing objects before they are fully constructed can compromise thread safety. An object that is published when it should not have been is said to have escaped. The most blatant form of publication is to store a reference in a public static field, where any class and thread could see it: public static Set < Secret > knownSecrets ; public void initialize () { knownSecrets = new HashSet < Secret >(); } // Don't do this class UnsafeStates { private String [] states = new String [] { \"AK\" , \"AL\" ... }; public String [] getStates () { return states ; } } Publishing one object may indirectly publish others. If you add a Secret to the published knownSecrets set, you\u2019ve also published that Secret, because any code can iterate the Set and obtain a reference to the new Secret. Publishing states in UnsafeStates way is problematic because any caller can modify its contents. In this case, the states array has escaped its intended scope, because what was supposed to be private state has been effectively made public. Whether another thread actually does something with a published reference doesn\u2019t really matter, because the risk of misuse is still present. Once an object escapes, you have to assume that another class or thread may, maliciously or carelessly, misuse it. This is a compelling reason to use encapsulation: it makes it practical to analyze programs for correctness and harder to violate design constraints accidentally. A final mechanism by which an object or its internal state can be published is to publish an inner class instance: public class ThisEscape { // Don't do this public ThisEscape ( EventSource source ) { source . registerListener ( new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }); } } } When ThisEscape publishes the EventListener , it implicitly publishes the enclosing ThisEscape instance as well, because inner class instances contain a hidden reference to the enclosing instance.","title":"Publication and escape"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#safe-construction-practices","text":"ThisEscape illustrates an important special case of escape\u2014when the this references escapes during construction. When the inner EventListener instance is published, so is the enclosing ThisEscape instance. But an object is in a predictable, consistent state only after its constructor returns, so publishing an object from within its constructor can publish an incompletely constructed object. This is true even if the publication is the last statement in the constructor. If the this reference escapes during construction, the object is considered not properly constructed. Do not allow the this reference to escape during construction. A common mistake that can let the this reference escape during construction is to start a thread from a constructor. When an object creates a thread from its constructor, it almost always shares its this reference with the new thread, either explicitly (by passing it to the constructor) or implicitly (because the Thread or Runnable is an inner class of the owning object). The new thread might then be able to see the owning object before it is fully constructed. There\u2019s nothing wrong with creating a thread in a constructor, but it is best not to start the thread immediately. Instead, expose a start or initialize method that starts the owned thread. Calling an overrideable instance method (one that is neither private nor final) from the constructor can also allow the this reference to escape. If you are tempted to register an event listener or start a thread from a constructor, you can avoid the improper construction by using a private constructor and a public factory method. public class SafeListener { private final EventListener listener ; private SafeListener () { listener = new EventListener () { public void onEvent ( Event e ) { doSomething ( e ); } }; } public static SafeListener newInstance ( EventSource source ) { SafeListener safe = new SafeListener (); source . registerListener ( safe . listener ); return safe ; } }","title":"Safe construction practices"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#thread-confinement","text":"Accessing shared, mutable data requires using synchronization; one way to avoid this requirement is to not share. If data is only accessed from a single thread, no synchronization is needed. This technique, thread confinement, is one of the simplest ways to achieve thread safety. When an object is confined to a thread, such usage is automatically thread-safe even if the confined object itself is not. Examples: Swing: The Swing visual components and data model objects are not thread safe; instead, safety is achieved by confining them to the Swing event dispatch thread. To use Swing properly, code running in threads other than the event thread should not access these objects. (To make this easier, Swing provides the invokeLater mechanism to schedule a Runnable for execution in the event thread.) Many concurrency errors in Swing applications stem from improper use of these confined objects from another thread. Pooled JDBC Connection objects: The JDBC specification does not require that Connection objects be thread-safe 4 . In typical server applications, a thread acquires a connection from the pool, uses it for processing a single request, and returns it. Since most requests, such as servlet requests or EJB (Enterprise JavaBeans) calls, are processed synchronously by a single thread, and the pool will not dispense the same connection to another thread until it has been returned, this pattern of connection management implicitly confines the Connection to that thread for the duration of the request. Just as the language has no mechanism for enforcing that a variable is guarded by a lock, it has no means of confining an object to a thread. Thread confinement is an element of your program\u2019s design that must be enforced by its implementation. The language and core libraries provide mechanisms that can help in maintaining thread confinement\u2014local variables and the ThreadLocal class\u2014but even with these, it is still the programmer\u2019s responsibility to ensure that thread-confined objects do not escape from their intended thread.","title":"Thread confinement"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#ad-hoc-thread-confinement","text":"Ad-hoc thread confinement describes when the responsibility for maintaining thread confinement falls entirely on the implementation. Ad-hoc thread confinement can be fragile because none of the language features, such as visibility modifiers or local variables, helps confine the object to the target thread. In fact, references to thread-confined objects such as visual components or data models in GUI applications are often held in public fields. The decision to use thread confinement is often a consequence of the decision to implement a particular subsystem, such as the GUI, as a single-threaded sub- system. Single-threaded subsystems can sometimes offer a simplicity benefit that outweighs the fragility of ad-hoc thread confinement 5 . A special case of thread confinement applies to volatile variables. It is safe to perform read-modify-write operations on shared volatile variables as long as you ensure that the volatile variable is only written from a single thread. In this case, you are confining the modification to a single thread to prevent race conditions, and the visibility guarantees for volatile variables ensure that other threads see the most up-to-date value. Because of its fragility, ad-hoc thread confinement should be used sparingly; if possible, use one of the stronger forms of thread confinment (stack confinement or ThreadLocal) instead.","title":"Ad-hoc thread confinement"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#stack-confinement","text":"Stack confinement is a special case of thread confinement in which an object can only be reached through local variables. Just as encapsulation can make it easier to preserve invariants, local variables can make it easier to confine objects to a thread. Local variables are intrinsically confined to the executing thread; they exist on the executing thread\u2019s stack, which is not accessible to other threads. Stack confinement (also called within-thread or thread-local usage, but not to be confused with the ThreadLocal library class) is simpler to maintain and less fragile than ad-hoc thread confinement. For primitively typed local variables, such as numPairs in loadTheArk in below sample you cannot violate stack confinement even if you tried. There is no way to obtain a reference to a primitive variable, so the language semantics ensure that primitive local variables are always stack confined. public int loadTheArk ( Collection < Animal > candidates ) { SortedSet < Animal > animals ; int numPairs = 0 ; Animal candidate = null ; // animals confined to method, don\u2019t let them escape! animals = new TreeSet < Animal >( new SpeciesGenderComparator ()); animals . addAll ( candidates ); for ( Animal a : animals ) { if ( candidate == null || ! candidate . isPotentialMate ( a )) candidate = a ; else { ark . load ( new AnimalPair ( candidate , a )); ++ numPairs ; candidate = null ; } } return numPairs ; } In loadTheArk, we instantiate a TreeSet and store a reference to it in animals. At this point, there is exactly one reference to the Set, held in a local variable and therefore confined to the executing thread. However, if we were to publish a reference to the Set (or any of its internals), the confinement would be violated and the animals would escape. The design requirement that the object be confined to the executing thread, or the awareness that the confined object is not thread-safe, often exists only in the head of the developer when the code is written. If the assumption of within-thread usage is not clearly documented, future maintainers might mistakenly allow the object to escape.","title":"Stack confinement"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#threadlocal-typically-private-static-fields","text":"A more formal means of maintaining thread confinement is ThreadLocal, which allows you to associate a per-thread value with a value-holding object. ThreadLocal provides get and set accessor methods that maintain a separate copy of the value for each thread that uses it, so a get returns the most recent value passed to set from the currently executing thread. Thread-local variables are often used to prevent sharing in designs based on mutable Singletons or global variables. For example, a single-threaded application might maintain a global database connection that is initialized at startup to avoid having to pass a Connection to every method. Since JDBC connections may not be thread-safe, a multithreaded application that uses a global connection without additional coordination is not thread-safe either. By using a ThreadLocal to store the JDBC connection, as in ConnectionHolder in below sample, each thread will have its own connection. private static ThreadLocal < Connection > connectionHolder = new ThreadLocal < Connection >() { public Connection initialValue () { return DriverManager . getConnection ( DB_URL ); } }; public static Connection getConnection () { return connectionHolder . get (); } When a thread calls ThreadLocal.get for the first time, initialValue is consulted to provide the initial value for that thread. The thread-specific values are stored in the Thread object itself; when the thread terminates, the thread-specific values can be garbage collected. If you are porting a single-threaded application to a multithreaded environment, you can preserve thread safety by converting shared global variables into ThreadLocals, if the semantics of the shared globals permits this; an application- wide cache would not be as useful if it were turned into a number of thread-local caches. It is easy to abuse ThreadLocal by treating its thread confinement property as a license to use global variables or as a means of creating \u201chidden\u201d method arguments. Like global variables, thread-local variables can detract from reusability and introduce hidden couplings among classes, and should therefore be used with care.","title":"ThreadLocal (typically private static fields)"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#immutability","text":"If an object\u2019s state cannot be modified, these risks and complexities simply go away. An immutable object is one whose state cannot be changed after construction. Immutable objects are inherently thread-safe; their invariants are established by the constructor, and if their state cannot be changed, these invariants always hold. Immutable objects are always thread-safe. Immutable objects are simple. They can only be in one state, which is carefully controlled by the constructor. One of the most difficult elements of program design is reasoning about the possible states of complex objects. Reasoning about the state of immutable objects, on the other hand, is trivial. Immutable objects are also safer. Passing a mutable object to untrusted code, or otherwise publishing it where untrusted code could find it, is dangerous\u2014the untrusted code might modify its state, or, worse, retain a reference to it and modify its state later from another thread. On the other hand, immutable objects cannot be subverted in this manner by malicious or buggy code, so they are safe to share and publish freely without the need to make defensive copies. Neither the Java Language Specification nor the Java Memory Model formally defines immutability, but immutability is not equivalent to simply declaring all fields of an object final. An object whose fields are all final may still be mutable, since final fields can hold references to mutable objects. An object is immutable if: Its state cannot be modified after construction. All its fields are final. It is properly constructed (the this reference does not escape during construction). While the Set that stores the names is mutable, the design of ThreeStooges makes it impossible to modify that Set after construction. The stooges reference is final, so all object state is reached through a final field. The last requirement, proper construction, is easily met since the constructor does nothing that would cause the this reference to become accessible to code other than the constructor and its caller. @Immutable public final class ThreeStooges { private final Set < String > stooges = new HashSet < String >(); public ThreeStooges () { stooges . add ( \"Moe\" ); stooges . add ( \"Larry\" ); stooges . add ( \"Curly\" ); } public boolean isStooge ( String name ) { return stooges . contains ( name ); } }","title":"Immutability"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#final-fields","text":"The final keyword, a more limited version of the const mechanism from C++, supports the construction of immutable objects. Final fields can\u2019t be modified (although the objects they refer to can be modified if they are mutable), but they also have special semantics under the Java Memory Model. It is the use of final fields that makes possible the guarantee of initialization safety that lets immutable objects be freely accessed and shared without synchronization. Even if an object is mutable, making some fields final can still simplify reasoning about its state, since limiting the mutability of an object restricts its set of possible states. An object that is \u201cmostly immutable\u201d but has one or two mutable state variables is still simpler than one that has many mutable variables. Declaring fields final also documents to maintainers that these fields are not expected to change. Just as it is a good practice to make all fields private unless they need greater visibility, it is a good practice to make all fields final unless they need to be mutable.","title":"Final fields"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#example-using-volatile-to-publish-immutable-objects","text":"Immutable objects can sometimes provide a weak form of atomicity. The factoring servlet performs two operations that must be atomic: updating the cached result and conditionally fetching the cached factors if the cached number matches the requested number. Whenever a group of related data items must be acted on atomically, consider creating an immutable holder class for them, such as OneValueCache 6 : @Immutable class OneValueCache { private final BigInteger lastNumber ; private final BigInteger [] lastFactors ; public OneValueCache ( BigInteger i , BigInteger [] factors ) { lastNumber = i ; lastFactors = Arrays . copyOf ( factors , factors . length ); } public BigInteger [] getFactors ( BigInteger i ) { if ( lastNumber == null || ! lastNumber . equals ( i )) return null ; else return Arrays . copyOf ( lastFactors , lastFactors . length ); } } Race conditions in accessing or updating multiple related variables can be eliminated by using an immutable object to hold all the variables. With a mutable holder object, you would have to use locking to ensure atomicity; with an im- mutable one, once a thread acquires a reference to it, it need never worry about another thread modifying its state. If the variables are to be updated, a new holder object is created, but any threads working with the previous holder still see it in a consistent state. VolatileCachedFactorizer uses a OneValueCache to store the cached number and factors. When a thread sets the volatile cache field to reference a new OneValueCache , the new cached data becomes immediately visible to other threads. The cache-related operations cannot interfere with each other because OneValueCache is immutable and the cache field is accessed only once in each of the relevant code paths. This combination of an immutable holder object for multiple state variables related by an invariant, and a volatile reference used to ensure its timely visibility, allows VolatileCachedFactorizer to be thread-safe even though it does no explicit locking. @ThreadSafe public class VolatileCachedFactorizer implements Servlet { private volatile OneValueCache cache = new OneValueCache ( null , null ); public void service ( ServletRequest req , ServletResponse resp ) { BigInteger i = extractFromRequest ( req ); BigInteger [] factors = cache . getFactors ( i ); if ( factors == null ) { factors = factor ( i ); cache = new OneValueCache ( i , factors ); } encodeIntoResponse ( resp , factors ); } }","title":"Example: Using volatile to publish immutable objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#safe-publication","text":"Sometimes we do want to share objects across threads, and in this case we must do so safely. Unfortunately, simply storing a reference to an object into a public field, is not enough to publish that object safely. // Unsafe publication public Holder holder ; public void initialize () { holder = new Holder ( 42 ); } Because of visibility problems, the Holder could appear to another thread to be in an inconsistent state, even though its invariants were properly established by its constructor! This improper publication could allow another thread to observe a partially constructed object.","title":"Safe publication"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#improper-publication-when-good-objects-go-bad","text":"You cannot rely on the integrity of partially constructed objects. An observing thread could see the object in an inconsistent state, and then later see its state suddenly change, even though it has not been modified since publication 7 . Because synchronization was not used to make the Holder visible to other threads, we say the Holder was not properly published. Other threads could see a stale value for the holder field, and thus see a null reference or other older value even though a value has been placed in holder. But far worse, other threads could see an up-to-date value for the holder reference, but stale values for the state of the Holder. To make things even less predictable, a thread may see a stale value the first time it reads a field and then a more up-to-date value the next time.","title":"Improper publication: when good objects go bad"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#immutable-objects-and-initialization-safety","text":"Because immutable objects are so important, the Java Memory Model offers a special guarantee of initialization safety for sharing immutable objects. As we\u2019ve seen, that an object reference becomes visible to another thread does not necessarily mean that the state of that object is visible to the consuming thread. In order to guarantee a consistent view of the object\u2019s state, synchronization is needed. Immutable objects, on the other hand, can be safely accessed even when synchronization is not used to publish the object reference. For this guarantee of initialization safety to hold, all of the requirements for immutability must be met: unmodifiable state, all fields are final, and proper construction. Immutable objects can be used safely by any thread without additional synchronization, even when synchronization is not used to publish them. This guarantee extends to the values of all final fields of properly constructed objects; final fields can be safely accessed without additional synchronization. However, if final fields refer to mutable objects, synchronization is still required to access the state of the objects they refer to.","title":"Immutable objects and initialization safety"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#safe-publication-idioms","text":"Objects that are not immutable must be safely published, which usually entails synchronization by both the publishing and the consuming thread. To publish an object safely, both the reference to the object and the object\u2019s state must be made visible to other threads at the same time. A properly constructed object can be safely published by: Initializing an object reference from a static initializer; Storing a reference to it into a volatile field or AtomicReference; Storing a reference to it into a final field of a properly constructed object; Storing a reference to it into a field that is properly guarded by a lock. The internal synchronization in thread-safe collections means that placing an object in a thread-safe collection, such as a Vector or synchronizedList, fulfills the last of these requirements. If thread A places object X in a thread-safe collection and thread B subsequently retrieves it, B is guaranteed to see the state of X as A left it, even though the application code that hands X off in this manner has no explicit synchronization. The thread-safe library collections offer the following safe publication guarantees, even if the Javadoc is less than clear on the subject: Placing a key or value in a Hashtable, synchronizedMap, or ConcurrentMap safely publishes it to any thread that retrieves it from the Map (whether directly or via an iterator); Placing an element in a Vector, CopyOnWriteArrayList, CopyOnWriteArraySet, synchronizedList, or synchronizedSet safely publishes it to any thread that retrieves it from the collection; Placing an element on a BlockingQueue or a ConcurrentLinkedQueue safely publishes it to any thread that retrieves it from the queue. Using a static initializer is often the easiest and safest way to publish objects that can be statically constructed: public static Holder holder = new Holder(42); Static initializers are executed by the JVM at class initialization time; because of internal synchronization in the JVM, this mechanism is guaranteed to safely publish any objects initialized in this way","title":"Safe publication idioms"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#effectively-immutable-objects","text":"The safe publication mechanisms all guarantee that the as-published state of an object is visible to all accessing threads as soon as the reference to it is visible, and if that state is not going to be changed again, this is sufficient to ensure that any access is safe. Objects that are not technically immutable, but whose state will not be modified after publication, are called effectively immutable. They do not need to meet the strict definition of immutability; they merely need to be treated by the program as if they were immutable after they are published. Using effectively immutable objects can simplify development and improve performance by reducing the need for synchronization. Safely published effectively immutable objects can be used safely by any thread without additional synchronization.","title":"Effectively immutable objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#mutable-objects","text":"If an object may be modified after construction, safe publication ensures only the visibility of the as-published state. Synchronization must be used not only to publish a mutable object, but also every time the object is accessed to ensure visibility of subsequent modifications. To share mutable objects safely, they must be safely published and be either thread-safe or guarded by a lock. The publication requirements for an object depend on its mutability: Immutable objects can be published through any mechanism. Effectively immutable objects must be safely published. Mutable objects must be safely published, and must be either thread-safe or guarded by a lock.","title":"Mutable objects"},{"location":"java/concurrency/fundamentals/c03-sharing-objects/#sharing-objects-safely","text":"Whenever you acquire a reference to an object, you should know what you are allowed to do with it. Do you need to acquire a lock before using it? Are you allowed to modify its state, or only to read it? Many concurrency errors stem from failing to understand these \u201crules of engagement\u201d for a shared object. When you publish an object, you should document how the object can be accessed. The most useful policies for using and sharing objects in a concurrent program are: Thread-confined. A thread-confined object is owned exclusively by and confined to one thread, and can be modified by its owning thread. Shared read-only. A shared read-only object can be accessed concurrently by multiple threads without additional synchronization, but cannot be modified by any thread. Shared read-only objects include immutable and effectively immutable objects. Shared thread-safe. A thread-safe object performs synchronization internally, so multiple threads can freely access it through its public interface without further synchronization. Guarded. A guarded object can be accessed only with a specific lock held. Guarded objects include those that are encapsulated within other thread-safe objects and published objects that are known to be guarded by a specific lock. This may seem like a broken design, but it is meant to allow JVMs to take full advantage of the performance of modern multiprocessor hardware. For example, in the absence of synchronization, the Java Memory Model permits the compiler to reorder operations and cache values in registers, and permits CPUs to reorder operations and cache values in processor-specific caches. \u21a9 When the Java Virtual Machine Specification was written, many widely used processor architectures could not efficiently provide atomic 64-bit arithmetic operations. \u21a9 Volatile reads are only slightly more expensive than nonvolatile reads on most current processor architectures. \u21a9 The connection pool implementations provided by application servers are thread-safe; connection pools are necessarily accessed from multiple threads, so a non-thread-safe implementation would not make sense. \u21a9 Another reason to make a subsystem single-threaded is deadlock avoidance; this is one of the primary reasons most GUI frameworks are single-threaded. \u21a9 OneValueCache wouldn\u2019t be immutable without the copyOf calls in the constructor and getter. Arrays.copyOf was added as a convenience in Java 6; clone would also work. \u21a9 The problem here is not the Holder class itself, but that the Holder is not properly published. However, Holder can be made immune to improper publication by declaring the n field to be final which would make Holder immutable. \u21a9","title":"Sharing objects safely"},{"location":"java/concurrency/fundamentals/c04-composing-objects/","text":"4. Composing objects Designing a thread-safe class An invariant is a set of assertions that must always hold true during the life of an object for the program to be valid. A postcondition is a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. The design process for a thread-safe class should include these three basic elements: Identify the variables that form the object\u2019s state Identify the invariants that constrain the state variables Establish a policy for managing concurrent access to the object\u2019s state. An object\u2019s state starts with its fields. If they are all of primitive type, the fields comprise the entire state. The state of an object with n primitive fields is just the n-tuple of its field values; the state of a 2D Point is its (x, y) value. If the object has fields that are references to other objects, its state will encompass fields from the referenced objects as well. The synchronization policy defines how an object coordinates access to its state without violating its invariants or postconditions. It specifies what combination of immutability, thread confinement, and locking is used to maintain thread safety, and which variables are guarded by which locks. To ensure that the class can be analyzed and maintained, document the synchronization policy. Gathering synchronization requirements Making a class thread-safe means ensuring that its invariants hold under concurrent access; this requires reasoning about its state. Objects and variables have a state space: the range of possible states they can take on. The smaller this state space, the easier it is to reason about. Many classes have invariants that identify certain states as valid or invalid (i.e.: in counters negative values are not allowed). Similarly, operations may have postconditions that identify certain state transi- tions as invalid. If the current state of a Counter is 17, the only valid next state is 18. When the next state is derived from the current state, the operation is necessarily a compound action. Constraints placed on states or state transitions by invariants and postconditions create additional synchronization or encapsulation requirements. A class can also have invariants that constrain multiple state variables. Multivariable invariants like this one create atomicity requirements: related variables must be fetched or updated in a single atomic operation. You cannot update one, release and reacquire the lock, and then update the others, since this could involve leaving the object in an invalid state when the lock was released. You cannot ensure thread safety without understanding an object\u2019s invariants and postconditions. Constraints on the valid values or state transitions for state variables can create atomicity and encapsulation requirements. State-dependent operations (i.e.: wait until true) Class invariants and method postconditions constrain the valid states and state transitions for an object. Some objects also have methods with state-based preconditions. For example, you cannot remove an item from an empty queue; a queue must be in the \u201cnonempty\u201d state before you can remove an element. Operations with state-based preconditions are called state-dependent. In a single-threaded program, if a precondition does not hold, the operation has no choice but to fail. But in a concurrent program, the precondition may become true later due to the action of another thread. Concurrent programs add the possibility of waiting until the precondition becomes true, and then proceeding with the operation. The built-in mechanisms for efficiently waiting for a condition to become true\u2014wait and notify\u2014are tightly bound to intrinsic locking, and can be difficult to use correctly. To create operations that wait for a precondition to become true before proceeding, it is often easier to use existing library classes, such as blocking queues or semaphores, to provide the desired state-dependent behavior. State ownership When defining which variables form an object\u2019s state, we want to consider only the data that object owns. Ownership is not embodied explicitly in the language, but is instead an element of class design. For better or worse, garbage collection lets us avoid thinking carefully about ownership. When passing an object to a method in C++, you have to think fairly carefully about whether you are transferring ownership, engaging in a short-term loan, or envisioning long-term joint ownership. In Java, all these same ownership models are possible, but the garbage collector reduces the cost of many of the common errors in reference sharing, enabling less-than-precise thinking about ownership. In many cases, ownership and encapsulation go together\u2014the object encapsulates the state it owns and owns the state it encapsulates. It is the owner of a given state variable that gets to decide on the locking protocol used to maintain the integrity of that variable\u2019s state. Ownership implies control, but once you publish a reference to a mutable object, you no longer have exclusive control; at best, you might have \u201cshared ownership\u201d. A class usually does not own the objects passed to its methods or constructors, unless the method is designed to explicitly transfer ownership of objects passed in (such as the synchronized collection wrapper factory methods). Collection classes often exhibit a form of \u201csplit ownership\u201d, in which the collection owns the state of the collection infrastructure, but client code owns the objects stored in the collection. Instance confinement If an object is not thread-safe you can ensure that it is only accessed from a single thread (thread confinement), or that all access to it is properly guarded by a lock. Encapsulation simplifies making classes thread-safe by promoting instance confinement. When an object is encapsulated within another object, all code paths that have access to the encapsulated object are known and can be therefore be analyzed more easily than if that object were accessible to the entire program. Combining confinement with an appropriate locking discipline can ensure that otherwise non-thread-safe objects are used in a thread-safe manner. Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). PersonSet in code bellow illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet, which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson, and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. @ThreadSafe public class PersonSet { @GuardedBy ( \"this\" ) private final Set < Person > mySet = new HashSet < Person >(); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } } Above example makes no assumptions about the thread-safety of Person , but if it is mutable, additional synchronization will be needed when accessing a Person retrieved from a PersonSet . The most reliable way to do this would be to make Person thread-safe; less reliable would be to guard the Person objects with a lock and ensure that all clients follow the protocol of acquiring the appropriate lock before accessing the Person . Of course, it is still possible to violate confinement by publishing a supposedly confined object; if an object is intended to be confined to a specific scope, then letting it escape from that scope is a bug. Confined objects can also escape by publishing other objects such as iterators or inner class instances that may indirectly publish the confined objects. The Java monitor pattern Following the principle of instance confinement to its logical conclusion leads you to the Java monitor pattern. An object following the Java monitor pattern encapsulates all its mutable state and guards it with the object\u2019s own intrinsic lock. The Java monitor pattern is merely a convention; any lock object could be used to guard an object\u2019s state so long as it is used consistently. public class PrivateLock { private final Object myLock = new Object (); @GuardedBy ( \"myLock\" ) Widget widget ; void someMethod () { synchronized ( myLock ) { // Access or modify the state of widget } } } There are advantages to using a private lock object instead of an object\u2019s intrinsic lock (or any other publicly accessible lock). Making the lock object private encapsulates the lock so that client code cannot acquire it, whereas a publicly accessible lock allows client code to participate in its synchronization policy\u2014correctly or incorrectly. Clients that improperly acquire another object\u2019s lock could cause liveness problems, and verifying that a publicly accessible lock is properly used requires examining the entire program rather than a single class. Delegating thread safety The Java monitor pattern is useful when building classes from scratch or composing classes out of objects that are not thread-safe. But what if the components of our class are already thread-safe? Do we need to add an additional layer of thread safety? The answer is: \u201cit depends\u201d. In CountingFactorizer , we added an AtomicLong to an otherwise stateless object, and the resulting composite object was still thread-safe. Since the state of CountingFactorizer is the state of the thread-safe AtomicLong, and since CountingFactorizer imposes no additional validity constraints on the state of the counter, it is easy to see that CountingFactorizer is thread-safe. We could say that CountingFactorizer delegates its thread safety responsibilities to the AtomicLong: CountingFactorizer is thread-safe because AtomicLong is. Independent state variables We can also delegate thread safety to more than one underlying state variable as long as those underlying state variables are independent, meaning that the composite class does not impose any invariants involving the multiple state variables. When delegation fails NumberRange in bellow code uses two AtomicIntegers to manage its state, but imposes an additional constraint\u2014that the first number be less than or equal to the second. public class NumberRange { // INVARIANT: lower <= upper private final AtomicInteger lower = new AtomicInteger ( 0 ); private final AtomicInteger upper = new AtomicInteger ( 0 ); public void setLower ( int i ) { // Warning -- unsafe check-then-act if ( i > upper . get ()) throw new IllegalArgumentException ( \"can\u2019t set lower to \" + i + \" > upper\" ); lower . set ( i ); } } public void setUpper ( int i ) { // Warning -- unsafe check-then-act if ( i < lower . get ()) throw new IllegalArgumentException ( \"can\u2019t set upper to \" + i + \" < lower\" ); upper . set ( i ); } public boolean isInRange ( int i ) { return ( i >= lower . get () && i <= upper . get ()); } } NumberRange is not thread-safe; it does not preserve the invariant that constrains lower and upper. The setLower and setUpper methods attempt to respect this invariant, but do so poorly. Both setLower and setUpper are check-then-act sequences, but they do not use sufficient locking to make them atomic. If the number range holds (0, 10), and one thread calls setLower(5) while another thread calls setUpper(4), with some unlucky timing both will pass the checks in the setters and both modifications will be applied. The result is that the range now holds (5, 4)\u2014an invalid state. So while the underlying AtomicIntegers are thread-safe, the composite class is not. Because the underlying state variables lower and upper are not independent, NumberRange cannot simply delegate thread safety to its thread-safe state variables. NumberRange could be made thread-safe by using locking to maintain its in- variants, such as guarding lower and upper with a common lock. If a class has compound actions, as NumberRange does, delegation alone is again not a suitable approach for thread safety. In these cases, the class must provide its own locking to ensure that compound actions are atomic, unless the entire compound action can also be delegated to the underlying state variables. Publishing underlying state variables When you delegate thread safety to an object\u2019s underlying state variables, under what conditions can you publish those variables so that other classes can modify them as well? Again, the answer depends on what invariants your class imposes on those variables. If a state variable is thread-safe, does not participate in any invariants that constrain its value, and has no prohibited state transitions for any of its operations, then it can safely be published. Adding functionality to existing thread-safe classes Sometimes a thread-safe class that supports all of the operations we want already exists, but often the best we can find is a class that supports almost all the operations we want, and then we need to add a new operation to it without undermining its thread safety. The safest way to add a new atomic operation is to modify the original class to support the desired operation, but this is not always possible because you may not have access to the source code or may not be free to modify it. If you can modify the original class, you need to understand the implementation\u2019s synchronization policy so that you can enhance it in a manner consistent with its original design. Adding the new method directly to the class means that all the code that implements the synchronization policy for that class is still contained in one source file, facilitating easier comprehension and maintenance. Another approach is to extend the class, assuming it was designed for extension. Extension is more fragile than adding code directly to a class, because the implementation of the synchronization policy is now distributed over multiple, separately maintained source files. If the underlying class were to change its synchronization policy by choosing a different lock to guard its state variables, the subclass would subtly and silently break, because it no longer used the right lock to control concurrent access to the base class\u2019s state. Client-side locking A third strategy is to extend the functionality of the class without extending the class itself by placing extension code in a \u201chelper\u201d class. @NotThreadSafe public class ListHelper < E > { public List < E > list = Collections . synchronizedList ( new ArrayList < E >()); // ... public synchronized boolean putIfAbsent ( E x ) { boolean absent = ! list . contains ( x ); if ( absent ) list . add ( x ); return absent ; } } Why wouldn\u2019t this work? After all, putIfAbsent is synchronized, right? The problem is that it synchronizes on the wrong lock. Whatever lock the List uses to guard its state, it sure isn\u2019t the lock on the ListHelper . ListHelper provides only the illusion of synchronization; the various list operations, while all synchronized, use different locks, which means that putIfAbsent is not atomic relative to other operations on the List. So there is no guarantee that another thread won\u2019t modify the list while putIfAbsent is executing. To make this approach work, we have to use the same lock that the List uses by using client-side locking or external locking. Client-side locking implies guarding client code that uses some object X with the lock X uses to guard its own state. In order to use client-side locking, you must know what lock X uses. The documentation for Vector and the synchronized wrapper classes states, not clearly, that they support client-side locking, by using the intrinsic lock for the Vector or the wrapper collection (not the wrapped collection). Bellow code shows a putIfAbsent operation on a thread-safe List that correctly uses client-side locking. @ThreadSafe public class ListHelper < E > { public List < E > list = Collections . synchronizedList ( new ArrayList < E >()); // ... public synchronized boolean putIfAbsent ( E x ) { synchronized ( list ) { boolean absent = ! list . contains ( x ); if ( absent ) list . add ( x ); return absent ; } } } If extending a class to add another atomic operation is fragile because it distributes the locking code for a class over multiple classes in an object hierarchy, client-side locking is even more fragile because it entails putting locking code for class C into classes that are totally unrelated to C. Exercise care when using client-side locking on classes that do not commit to their locking strategy. Client-side locking has a lot in common with class extension\u2014they both couple the behavior of the derived class to the implementation of the base class. Just as extension violates encapsulation of implementation, client-side locking violates encapsulation of synchronization policy. Composition There is a less fragile alternative for adding an atomic operation to an existing class: composition. ImprovedList in bellow code implements the List operations by delegating them to an underlying List instance, and adds an atomic putIfAbsent method. (Like Collections.synchronizedList and other collections wrappers, ImprovedList assumes that once a list is passed to its constructor, the client will not use the underlying list directly again, accessing it only through the ImprovedList .) @ThreadSafe public class ImprovedList < T > implements List < T > { private final List < T > list ; public ImprovedList ( List < T > list ) { this . list = list ; } public synchronized boolean putIfAbsent ( T x ) { boolean contains = list . contains ( x ); if ( contains ) list . add ( x ); return ! contains ; } public synchronized void clear () { list . clear (); } // ... similarly delegate other List methods } ImprovedList adds an additional level of locking using its own intrinsic lock. It does not care whether the underlying List is thread-safe, because it provides its own consistent locking that provides thread safety even if the List is not thread-safe or changes its locking implementation. While the extra layer of synchronization may add some small performance penalty, the implementation in ImprovedList is less fragile than attempting to mimic the locking strategy of another object. In effect, we\u2019ve used the Java monitor pattern to encapsulate an existing List, and this is guaranteed to provide thread safety so long as our class holds the only outstanding reference to the underlying List. Documenting synchronization policies Documentation is one of the most powerful (and, sadly, most underutilized) tools for managing thread safety. Users look to the documentation to find out if a class is thread-safe, and maintainers look to the documentation to understand the implementation strategy so they can maintain it without inadvertently compromising safety. Unfortunately, both of these constituencies usually find less information in the documentation than they\u2019d like. Document a class\u2019s thread safety guarantees for its clients; document its synchronization policy for its maintainers. Each use of synchronized, volatile, or any thread-safe class reflects a synchronization policy defining a strategy for ensuring the integrity of data in the face of concurrent access. That policy is an element of your program\u2019s design, and should be documented. Of course, the best time to document design decisions is at design time. Weeks or months later, the details may be a blur\u2014so write it down before you forget. Interpreting vague documentation \u201cthread\u201d and \u201cconcurrent\u201d do not appear at all in the JDBC specification, and appear frustratingly rarely in the servlet specification. So what do you do? You are going to have to guess. One way to improve the quality of your guess is to interpret the specification from the perspective of someone who will implement it (such as a container or database vendor), as opposed to someone who will merely use it. Servlets are always called from a container-managed thread, and it is safe to assume that if there is more than one such thread, the container knows this.","title":"Composing objects"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#4-composing-objects","text":"","title":"4. Composing objects"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#designing-a-thread-safe-class","text":"An invariant is a set of assertions that must always hold true during the life of an object for the program to be valid. A postcondition is a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. The design process for a thread-safe class should include these three basic elements: Identify the variables that form the object\u2019s state Identify the invariants that constrain the state variables Establish a policy for managing concurrent access to the object\u2019s state. An object\u2019s state starts with its fields. If they are all of primitive type, the fields comprise the entire state. The state of an object with n primitive fields is just the n-tuple of its field values; the state of a 2D Point is its (x, y) value. If the object has fields that are references to other objects, its state will encompass fields from the referenced objects as well. The synchronization policy defines how an object coordinates access to its state without violating its invariants or postconditions. It specifies what combination of immutability, thread confinement, and locking is used to maintain thread safety, and which variables are guarded by which locks. To ensure that the class can be analyzed and maintained, document the synchronization policy.","title":"Designing a thread-safe class"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#gathering-synchronization-requirements","text":"Making a class thread-safe means ensuring that its invariants hold under concurrent access; this requires reasoning about its state. Objects and variables have a state space: the range of possible states they can take on. The smaller this state space, the easier it is to reason about. Many classes have invariants that identify certain states as valid or invalid (i.e.: in counters negative values are not allowed). Similarly, operations may have postconditions that identify certain state transi- tions as invalid. If the current state of a Counter is 17, the only valid next state is 18. When the next state is derived from the current state, the operation is necessarily a compound action. Constraints placed on states or state transitions by invariants and postconditions create additional synchronization or encapsulation requirements. A class can also have invariants that constrain multiple state variables. Multivariable invariants like this one create atomicity requirements: related variables must be fetched or updated in a single atomic operation. You cannot update one, release and reacquire the lock, and then update the others, since this could involve leaving the object in an invalid state when the lock was released. You cannot ensure thread safety without understanding an object\u2019s invariants and postconditions. Constraints on the valid values or state transitions for state variables can create atomicity and encapsulation requirements.","title":"Gathering synchronization requirements"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#state-dependent-operations-ie-wait-until-true","text":"Class invariants and method postconditions constrain the valid states and state transitions for an object. Some objects also have methods with state-based preconditions. For example, you cannot remove an item from an empty queue; a queue must be in the \u201cnonempty\u201d state before you can remove an element. Operations with state-based preconditions are called state-dependent. In a single-threaded program, if a precondition does not hold, the operation has no choice but to fail. But in a concurrent program, the precondition may become true later due to the action of another thread. Concurrent programs add the possibility of waiting until the precondition becomes true, and then proceeding with the operation. The built-in mechanisms for efficiently waiting for a condition to become true\u2014wait and notify\u2014are tightly bound to intrinsic locking, and can be difficult to use correctly. To create operations that wait for a precondition to become true before proceeding, it is often easier to use existing library classes, such as blocking queues or semaphores, to provide the desired state-dependent behavior.","title":"State-dependent operations (i.e.: wait until true)"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#state-ownership","text":"When defining which variables form an object\u2019s state, we want to consider only the data that object owns. Ownership is not embodied explicitly in the language, but is instead an element of class design. For better or worse, garbage collection lets us avoid thinking carefully about ownership. When passing an object to a method in C++, you have to think fairly carefully about whether you are transferring ownership, engaging in a short-term loan, or envisioning long-term joint ownership. In Java, all these same ownership models are possible, but the garbage collector reduces the cost of many of the common errors in reference sharing, enabling less-than-precise thinking about ownership. In many cases, ownership and encapsulation go together\u2014the object encapsulates the state it owns and owns the state it encapsulates. It is the owner of a given state variable that gets to decide on the locking protocol used to maintain the integrity of that variable\u2019s state. Ownership implies control, but once you publish a reference to a mutable object, you no longer have exclusive control; at best, you might have \u201cshared ownership\u201d. A class usually does not own the objects passed to its methods or constructors, unless the method is designed to explicitly transfer ownership of objects passed in (such as the synchronized collection wrapper factory methods). Collection classes often exhibit a form of \u201csplit ownership\u201d, in which the collection owns the state of the collection infrastructure, but client code owns the objects stored in the collection.","title":"State ownership"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#instance-confinement","text":"If an object is not thread-safe you can ensure that it is only accessed from a single thread (thread confinement), or that all access to it is properly guarded by a lock. Encapsulation simplifies making classes thread-safe by promoting instance confinement. When an object is encapsulated within another object, all code paths that have access to the encapsulated object are known and can be therefore be analyzed more easily than if that object were accessible to the entire program. Combining confinement with an appropriate locking discipline can ensure that otherwise non-thread-safe objects are used in a thread-safe manner. Confined objects must not escape their intended scope. An object may be confined to a class instance (such as a private class member), a lexical scope (such as a local variable), or a thread (such as an object that is passed from method to method within a thread, but not supposed to be shared across threads). PersonSet in code bellow illustrates how confinement and locking can work together to make a class thread-safe even when its component state variables are not. The state of PersonSet is managed by a HashSet, which is not thread-safe. But because mySet is private and not allowed to escape, the HashSet is confined to the PersonSet. The only code paths that can access mySet are addPerson and containsPerson, and each of these acquires the lock on the PersonSet. All its state is guarded by its intrinsic lock, making PersonSet thread-safe. @ThreadSafe public class PersonSet { @GuardedBy ( \"this\" ) private final Set < Person > mySet = new HashSet < Person >(); public synchronized void addPerson ( Person p ) { mySet . add ( p ); } public synchronized boolean containsPerson ( Person p ) { return mySet . contains ( p ); } } Above example makes no assumptions about the thread-safety of Person , but if it is mutable, additional synchronization will be needed when accessing a Person retrieved from a PersonSet . The most reliable way to do this would be to make Person thread-safe; less reliable would be to guard the Person objects with a lock and ensure that all clients follow the protocol of acquiring the appropriate lock before accessing the Person . Of course, it is still possible to violate confinement by publishing a supposedly confined object; if an object is intended to be confined to a specific scope, then letting it escape from that scope is a bug. Confined objects can also escape by publishing other objects such as iterators or inner class instances that may indirectly publish the confined objects.","title":"Instance confinement"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#the-java-monitor-pattern","text":"Following the principle of instance confinement to its logical conclusion leads you to the Java monitor pattern. An object following the Java monitor pattern encapsulates all its mutable state and guards it with the object\u2019s own intrinsic lock. The Java monitor pattern is merely a convention; any lock object could be used to guard an object\u2019s state so long as it is used consistently. public class PrivateLock { private final Object myLock = new Object (); @GuardedBy ( \"myLock\" ) Widget widget ; void someMethod () { synchronized ( myLock ) { // Access or modify the state of widget } } } There are advantages to using a private lock object instead of an object\u2019s intrinsic lock (or any other publicly accessible lock). Making the lock object private encapsulates the lock so that client code cannot acquire it, whereas a publicly accessible lock allows client code to participate in its synchronization policy\u2014correctly or incorrectly. Clients that improperly acquire another object\u2019s lock could cause liveness problems, and verifying that a publicly accessible lock is properly used requires examining the entire program rather than a single class.","title":"The Java monitor pattern"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#delegating-thread-safety","text":"The Java monitor pattern is useful when building classes from scratch or composing classes out of objects that are not thread-safe. But what if the components of our class are already thread-safe? Do we need to add an additional layer of thread safety? The answer is: \u201cit depends\u201d. In CountingFactorizer , we added an AtomicLong to an otherwise stateless object, and the resulting composite object was still thread-safe. Since the state of CountingFactorizer is the state of the thread-safe AtomicLong, and since CountingFactorizer imposes no additional validity constraints on the state of the counter, it is easy to see that CountingFactorizer is thread-safe. We could say that CountingFactorizer delegates its thread safety responsibilities to the AtomicLong: CountingFactorizer is thread-safe because AtomicLong is.","title":"Delegating thread safety"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#independent-state-variables","text":"We can also delegate thread safety to more than one underlying state variable as long as those underlying state variables are independent, meaning that the composite class does not impose any invariants involving the multiple state variables.","title":"Independent state variables"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#when-delegation-fails","text":"NumberRange in bellow code uses two AtomicIntegers to manage its state, but imposes an additional constraint\u2014that the first number be less than or equal to the second. public class NumberRange { // INVARIANT: lower <= upper private final AtomicInteger lower = new AtomicInteger ( 0 ); private final AtomicInteger upper = new AtomicInteger ( 0 ); public void setLower ( int i ) { // Warning -- unsafe check-then-act if ( i > upper . get ()) throw new IllegalArgumentException ( \"can\u2019t set lower to \" + i + \" > upper\" ); lower . set ( i ); } } public void setUpper ( int i ) { // Warning -- unsafe check-then-act if ( i < lower . get ()) throw new IllegalArgumentException ( \"can\u2019t set upper to \" + i + \" < lower\" ); upper . set ( i ); } public boolean isInRange ( int i ) { return ( i >= lower . get () && i <= upper . get ()); } } NumberRange is not thread-safe; it does not preserve the invariant that constrains lower and upper. The setLower and setUpper methods attempt to respect this invariant, but do so poorly. Both setLower and setUpper are check-then-act sequences, but they do not use sufficient locking to make them atomic. If the number range holds (0, 10), and one thread calls setLower(5) while another thread calls setUpper(4), with some unlucky timing both will pass the checks in the setters and both modifications will be applied. The result is that the range now holds (5, 4)\u2014an invalid state. So while the underlying AtomicIntegers are thread-safe, the composite class is not. Because the underlying state variables lower and upper are not independent, NumberRange cannot simply delegate thread safety to its thread-safe state variables. NumberRange could be made thread-safe by using locking to maintain its in- variants, such as guarding lower and upper with a common lock. If a class has compound actions, as NumberRange does, delegation alone is again not a suitable approach for thread safety. In these cases, the class must provide its own locking to ensure that compound actions are atomic, unless the entire compound action can also be delegated to the underlying state variables.","title":"When delegation fails"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#publishing-underlying-state-variables","text":"When you delegate thread safety to an object\u2019s underlying state variables, under what conditions can you publish those variables so that other classes can modify them as well? Again, the answer depends on what invariants your class imposes on those variables. If a state variable is thread-safe, does not participate in any invariants that constrain its value, and has no prohibited state transitions for any of its operations, then it can safely be published.","title":"Publishing underlying state variables"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#adding-functionality-to-existing-thread-safe-classes","text":"Sometimes a thread-safe class that supports all of the operations we want already exists, but often the best we can find is a class that supports almost all the operations we want, and then we need to add a new operation to it without undermining its thread safety. The safest way to add a new atomic operation is to modify the original class to support the desired operation, but this is not always possible because you may not have access to the source code or may not be free to modify it. If you can modify the original class, you need to understand the implementation\u2019s synchronization policy so that you can enhance it in a manner consistent with its original design. Adding the new method directly to the class means that all the code that implements the synchronization policy for that class is still contained in one source file, facilitating easier comprehension and maintenance. Another approach is to extend the class, assuming it was designed for extension. Extension is more fragile than adding code directly to a class, because the implementation of the synchronization policy is now distributed over multiple, separately maintained source files. If the underlying class were to change its synchronization policy by choosing a different lock to guard its state variables, the subclass would subtly and silently break, because it no longer used the right lock to control concurrent access to the base class\u2019s state.","title":"Adding functionality to existing thread-safe classes"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#client-side-locking","text":"A third strategy is to extend the functionality of the class without extending the class itself by placing extension code in a \u201chelper\u201d class. @NotThreadSafe public class ListHelper < E > { public List < E > list = Collections . synchronizedList ( new ArrayList < E >()); // ... public synchronized boolean putIfAbsent ( E x ) { boolean absent = ! list . contains ( x ); if ( absent ) list . add ( x ); return absent ; } } Why wouldn\u2019t this work? After all, putIfAbsent is synchronized, right? The problem is that it synchronizes on the wrong lock. Whatever lock the List uses to guard its state, it sure isn\u2019t the lock on the ListHelper . ListHelper provides only the illusion of synchronization; the various list operations, while all synchronized, use different locks, which means that putIfAbsent is not atomic relative to other operations on the List. So there is no guarantee that another thread won\u2019t modify the list while putIfAbsent is executing. To make this approach work, we have to use the same lock that the List uses by using client-side locking or external locking. Client-side locking implies guarding client code that uses some object X with the lock X uses to guard its own state. In order to use client-side locking, you must know what lock X uses. The documentation for Vector and the synchronized wrapper classes states, not clearly, that they support client-side locking, by using the intrinsic lock for the Vector or the wrapper collection (not the wrapped collection). Bellow code shows a putIfAbsent operation on a thread-safe List that correctly uses client-side locking. @ThreadSafe public class ListHelper < E > { public List < E > list = Collections . synchronizedList ( new ArrayList < E >()); // ... public synchronized boolean putIfAbsent ( E x ) { synchronized ( list ) { boolean absent = ! list . contains ( x ); if ( absent ) list . add ( x ); return absent ; } } } If extending a class to add another atomic operation is fragile because it distributes the locking code for a class over multiple classes in an object hierarchy, client-side locking is even more fragile because it entails putting locking code for class C into classes that are totally unrelated to C. Exercise care when using client-side locking on classes that do not commit to their locking strategy. Client-side locking has a lot in common with class extension\u2014they both couple the behavior of the derived class to the implementation of the base class. Just as extension violates encapsulation of implementation, client-side locking violates encapsulation of synchronization policy.","title":"Client-side locking"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#composition","text":"There is a less fragile alternative for adding an atomic operation to an existing class: composition. ImprovedList in bellow code implements the List operations by delegating them to an underlying List instance, and adds an atomic putIfAbsent method. (Like Collections.synchronizedList and other collections wrappers, ImprovedList assumes that once a list is passed to its constructor, the client will not use the underlying list directly again, accessing it only through the ImprovedList .) @ThreadSafe public class ImprovedList < T > implements List < T > { private final List < T > list ; public ImprovedList ( List < T > list ) { this . list = list ; } public synchronized boolean putIfAbsent ( T x ) { boolean contains = list . contains ( x ); if ( contains ) list . add ( x ); return ! contains ; } public synchronized void clear () { list . clear (); } // ... similarly delegate other List methods } ImprovedList adds an additional level of locking using its own intrinsic lock. It does not care whether the underlying List is thread-safe, because it provides its own consistent locking that provides thread safety even if the List is not thread-safe or changes its locking implementation. While the extra layer of synchronization may add some small performance penalty, the implementation in ImprovedList is less fragile than attempting to mimic the locking strategy of another object. In effect, we\u2019ve used the Java monitor pattern to encapsulate an existing List, and this is guaranteed to provide thread safety so long as our class holds the only outstanding reference to the underlying List.","title":"Composition"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#documenting-synchronization-policies","text":"Documentation is one of the most powerful (and, sadly, most underutilized) tools for managing thread safety. Users look to the documentation to find out if a class is thread-safe, and maintainers look to the documentation to understand the implementation strategy so they can maintain it without inadvertently compromising safety. Unfortunately, both of these constituencies usually find less information in the documentation than they\u2019d like. Document a class\u2019s thread safety guarantees for its clients; document its synchronization policy for its maintainers. Each use of synchronized, volatile, or any thread-safe class reflects a synchronization policy defining a strategy for ensuring the integrity of data in the face of concurrent access. That policy is an element of your program\u2019s design, and should be documented. Of course, the best time to document design decisions is at design time. Weeks or months later, the details may be a blur\u2014so write it down before you forget.","title":"Documenting synchronization policies"},{"location":"java/concurrency/fundamentals/c04-composing-objects/#interpreting-vague-documentation","text":"\u201cthread\u201d and \u201cconcurrent\u201d do not appear at all in the JDBC specification, and appear frustratingly rarely in the servlet specification. So what do you do? You are going to have to guess. One way to improve the quality of your guess is to interpret the specification from the perspective of someone who will implement it (such as a container or database vendor), as opposed to someone who will merely use it. Servlets are always called from a container-managed thread, and it is safe to assume that if there is more than one such thread, the container knows this.","title":"Interpreting vague documentation"},{"location":"java/concurrency/fundamentals/c05-building-blocks/","text":"5. Building Blocks Where practical, delegation is one of the most effective strategies for creating thread-safe classes: just let existing thread-safe classes manage all the state. Synchronized collections Problems with synchronized collections The synchronized collections are thread-safe, but you may sometimes need to use additional client-side locking to guard compound actions. Because the synchronized collections commit to a synchronization policy that supports client-side locking, it is possible to create new operations that are atomic with respect to other collection operations as long as we know which lock to use. The synchronized collection classes guard each method with the lock on the synchronized collection object itself. The problem of unreliable iteration can again be addressed by client-side locking, at some additional cost to scalability. By holding the Vector lock for the duration of iteration, we prevent other threads from modifying the Vector while we are iterating it. Unfortunately, we also prevent other threads from accessing it at all during this time, impairing concurrency. Iterators and ConcurrentModificationException The standard way to iterate a Collection is with an Iterator , either explicitly or through the for-each loop syntax introduced in Java 5.0, but using iterators does not obviate the need to lock the collection during iteration if other threads can concurrently modify it. The iterators returned by the synchronized collections are not designed to deal with concurrent modification, and they are fail-fast\u2014meaning that if they detect that the collection has changed since iteration began, they throw the unchecked ConcurrentModificationException . These fail-fast iterators are not designed to be foolproof\u2014they are designed to catch concurrency errors on a \u201cgood-faith-effort\u201d basis and thus act only as early-warning indicators for concurrency problems. They are implemented by associating a modification count with the collection: if the modification count changes during iteration, hasNext or next throws ConcurrentModificationException . However, this check is done without synchronization, so there is a risk of seeing a stale value of the modification count and therefore that the iterator does not realize a modification has been made. This was a deliberate design tradeoff to reduce the performance impact of the concurrent modification detection code 1 . List < Widget > widgetList = Collections . synchronizedList ( new ArrayList < Widget >()); // ... // May throw ConcurrentModificationException for ( Widget w : widgetList ) doSomething ( w ); There are several reasons, however, why locking a collection during iteration may be undesirable. Other threads that need to access the collection will block until the iteration is complete; if the collection is large or the task performed for each element is lengthy, they could wait a long time. Also, if the collection is locked as above listing, doSomething is being called with a lock held, which is a risk factor for deadlock (see Chapter 10). Even in the absence of starvation or deadlock risk, locking collections for significant periods of time hurts application scalability. The longer a lock is held, the more likely it is to be contended, and if many threads are blocked waiting for a lock throughput and CPU utilization can suffer. An alternative to locking the collection during iteration is to clone the collection and iterate the copy instead. Since the clone is thread-confined, no other thread can modify it during iteration, eliminating the possibility of ConcurrentModificationException . (The collection still must be locked during the clone operation itself.) Cloning the collection has an obvious performance cost; whether this is a favorable tradeoff depends on many factors including the size of the collection, how much work is done for each element, the relative frequency of iteration compared to other collection operations, and responsiveness and throughput requirements. Hidden iterators While locking can prevent iterators from throwing ConcurrentModificationException , you have to remember to use locking everywhere a shared collection might be iterated. This is trickier than it sounds, as iterators are sometimes hidden (like in toString ). private final Set < Integer > mySet = new HashSet < Integer >(); public void addTenThings () { System . out . println ( \"DEBUG: added ten elements to \" + set ); } The real lesson here is that the greater the distance between the state and the synchronization that guards it, the more likely that someone will forget to use proper synchronization when accessing that state. By using a synchronizedSet , encapsulating the synchronization, this sort of error would not occur. Just as encapsulating an object\u2019s state makes it easier to preserve its invariants, encapsulating its synchronization makes it easier to enforce its synchronization policy. Iteration is also indirectly invoked by the collection\u2019s hashCode and equals methods, which may be called if the collection is used as an element or key of another collection. Similarly, the containsAll , removeAll , and retainAll methods, as well as the constructors that take collections as arguments, also iterate the collection. All of these indirect uses of iteration can cause ConcurrentModificationException . Concurrent collections Java 5.0 improves on the synchronized collections by providing several concurrent collection classes. Synchronized collections achieve their thread safety by serializing all access to the collection\u2019s state. The cost of this approach is poor concurrency; when multiple threads contend for the collection-wide lock, throughput suffers. The concurrent collections, on the other hand, are designed for concurrent access from multiple threads. Java 5.0 adds ConcurrentHashMap , a replacement for synchronized hash-based Map implementations, and CopyOnWriteArrayList , a replacement for synchronized List implementations for cases where traversal is the dominant operation. The new ConcurrentMap interface adds support for common compound actions such as put-if-absent, replace, and conditional remove. Replacing synchronized collections with concurrent collections can offer dramatic scalability improvements with little risk. Java 5.0 also adds two new collection types, Queue and BlockingQueue . A Queue is intended to hold a set of elements temporarily while they await processing. Several implementations are provided, including ConcurrentLinkedQueue , a traditional FIFO queue, and PriorityQueue , a (non concurrent) priority ordered queue. Queue classes were added because eliminating the random-access requirements of List admits more efficient concurrent implementations. BlockingQueue extends Queue to add blocking insertion and retrieval operations. If the queue is empty, a retrieval blocks until an element is available, and if the queue is full (for bounded queues) an insertion blocks until there is space available. Blocking queues are extremely useful in producer-consumer designs. Just as ConcurrentHashMap is a concurrent replacement for a synchronized hash-based Map, Java 6 adds ConcurrentSkipListMap and ConcurrentSkipListSet , which are concurrent replacements for a synchronized SortedMap or SortedSet (such as TreeMap or TreeSet wrapped with synchronizedMap ). ConcurrentHashMap ConcurrentHashMap is a hash-based Map like HashMap, but it uses an entirely different locking strategy that offers better concurrency and scalability. Instead of synchronizing every method on a common lock, restricting access to a single thread at a time, it uses a finer-grained locking mechanism called lock striping to allow a greater degree of shared access. Arbitrarily many reading threads can access the map concurrently, readers can access the map concurrently with writers, and a limited number of writers can modify the map concurrently. The result is far higher throughput under concurrent access, with little performance penalty for single-threaded access. ConcurrentHashMap , along with the other concurrent collections, further improve on the synchronized collection classes by providing iterators that do not throw ConcurrentModificationException , thus eliminating the need to lock the collection during iteration. The iterators returned by ConcurrentHashMap are weakly consistent instead of fail-fast. A weakly consistent iterator can tolerate concurrent modification, traverses elements as they existed when the iterator was constructed, and may (but is not guaranteed to) reflect modifications to the collection after the construction of the iterator. As with all improvements, there are still a few tradeoffs: size is allowed to return an approximation instead of an exact count or isEmpty may not reflect if the structure is empty. So the requirements for these operations were weakened to enable performance optimizations for the most important operations, primarily get , put , containsKey , and remove . Because it has so many advantages and so few disadvantages compared to Hashtable or synchronizedMap , replacing synchronized Map implementations with ConcurrentHashMap in most cases results only in better scalability. Only if your application needs to lock the map for exclusive access is ConcurrentHashMap not an appropriate drop-in replacement. Since a ConcurrentHashMap cannot be locked for exclusive access, we cannot use client-side locking to create new atomic operations such as put-if-absent but it already have those implemented. CopyOnWriteArrayList CopyOnWriteArrayList is a concurrent replacement for a synchronized List that offers better concurrency in some common situations and eliminates the need to lock or copy the collection during iteration. The copy-on-write collections derive their thread safety from the fact that as long as an effectively immutable object is properly published, no further synchronization is required when accessing it. They implement mutability by creating and republishing a new copy of the collection every time it is modified. Iterators for the copy-on-write collections retain a reference to the backing array that was current at the start of iteration, and since this will never change, they need to synchronize only briefly to ensure visibility of the array contents. As a result, multiple threads can iterate the collection without interference from one another or from threads wanting to modify the collection. The iterators returned by the copy-on-write collections do not throw ConcurrentModificationException and return the elements exactly as they were at the time the iterator was created, regardless of subsequent modifications. Blocking queues and the producer-consumer pattern Blocking queues provide blocking put and take methods as well as the timed equivalents offer and poll . If the queue is full, put blocks until space becomes available; if the queue is empty, take blocks until an element is available. Queues can be bounded or unbounded; unbounded queues are never full, so a put on an unbounded queue never blocks. The familiar division of labor for two people washing the dishes is an example of a producer-consumer design: one person washes the dishes and places them in the dish rack, and the other person retrieves the dishes from the rack and dries them. In this scenario, the dish rack acts as a blocking queue; if there are no dishes in the rack, the consumer waits until there are dishes to dry, and if the rack fills up, the producer has to stop washing until there is more space. The last BlockingQueue implementation, SynchronousQueue , is not really a queue at all, in that it maintains no storage space for queued elements. Instead, it maintains a list of queued threads waiting to enqueue or dequeue an element. In the dish-washing analogy, this would be like having no dish rack, but instead handing the washed dishes directly to the next available dryer. While this may seem a strange way to implement a queue, it reduces the latency associated with moving data from producer to consumer because the work can be handed off directly. Since a SynchronousQueue has no storage capacity, put and take will block unless another thread is already waiting to participate in the handoff. Synchronous queues are generally suitable only when there are enough consumers that there nearly always will be one ready to take the handoff. Serial thread confinement For mutable objects, producer-consumer designs and blocking queues facilitate serial thread confinement for handing off ownership of objects from producers to consumers. A thread-confined object is owned exclusively by a single thread, but that ownership can be \u201ctransferred\u201d by publishing it safely where only one other thread will gain access to it and ensuring that the publishing thread does not access it after the handoff. The safe publication ensures that the object\u2019s state is visible to the new owner, and since the original owner will not touch it again, it is now confined to the new thread. The new owner may modify it freely since it has exclusive access. Deques and work stealing Java 6 also adds another two collection types, Deque (pronounced \u201cdeck\u201d) and BlockingDeque , that extend Queue and BlockingQueue . A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque. Blocking and interruptible methods The distinction between a blocking operation and an ordinary operation that merely takes a long time to finish is that a blocked thread must wait for an event that is beyond its control before it can proceed\u2014the I/O completes, the lock becomes available, or the external computation finishes. When that external event occurs, the thread is placed back in the RUNNABLE state and becomes eligible again for scheduling. The put and take methods of BlockingQueue throw the checked InterruptedException , as do a number of other library methods such as Thread.sleep . When a method can throw InterruptedException, it is telling you that it is a blocking method, and further that if it is interrupted, it will make an effort to stop blocking early. Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. While there is nothing in the API or language specification that demands any specific application-level semantics for interruption, the most sensible use for interruption is to cancel an activity. Blocking methods that are responsive to interruption make it easier to cancel long-running activities on a timely basis. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to interruption. For library code, there are basically two choices: Propagate the InterruptedException. This is often the most sensible policy if you can get away with it\u2014just propagate the InterruptedException to your caller. This could involve not catching InterruptedException, or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt. Sometimes you cannot throw InterruptedException, for instance when your code is part of a Runnable. In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. public class TaskRunnable implements Runnable { BlockingQueue < Task > queue ; ... public void run () { try { processTask ( queue . take ()); } catch ( InterruptedException e ) { // restore interrupted status Thread . currentThread (). interrupt (); } } } You can get much more sophisticated with interruption, but these two approaches should work in the vast majority of situations. But there is one thing you should not do with InterruptedException \u2014catch it and do nothing in response. The only situation in which it is acceptable to swallow an interrupt is when you are extending Thread and therefore control all the code higher up on the call stack. Synchronizers A synchronizer is any object that coordinates the control flow of threads based on its state. Blocking queues can act as synchronizers (they coordinate the control flow of producer and consumer threads because take and put block until the queue enters the desired state (not empty or not full)); other types of synchronizers include semaphores, barriers, and latches. All synchronizers share certain structural properties: they encapsulate state that determines whether threads arriving at the synchronizer should be allowed to pass or forced to wait, provide methods to manipulate that state, and provide methods to wait efficiently for the synchronizer to enter the desired state. Latches A latch is a synchronizer that can delay the progress of threads until it reaches its terminal state then it will allow threads to continue. Once the latch reaches the terminal state, it cannot change state again, so it remains open forever. Latches can be used to ensure that certain activities do not proceed until other one-time activities complete. Ensuring that a computation does not proceed until resources it needs have been initialized. Ensuring that a service does not start until other services on which it depends have started. Waiting until all the parties involved in an activity, for instance the players in a multi-player game, are ready to proceed. CountDownLatch It is a flexible latch implementation that can be used in any of these situations; it allows one or more threads to wait for a set of events to occur. The latch state consists of a counter initialized to a positive number, representing the number of events to wait for. The countDown method decrements the counter, indicating that an event has occurred, and the await methods wait for the counter to reach zero, which happens when all the events have occurred. If the counter is nonzero on entry, await blocks until the counter reaches zero, the waiting thread is interrupted, or the wait times out. public class TestHarness { public long timeTasks ( int nThreads , final Runnable task ) throws InterruptedException { final CountDownLatch startGate = new CountDownLatch ( 1 ); final CountDownLatch endGate = new CountDownLatch ( nThreads ); for ( int i = 0 ; i < nThreads ; i ++) { Thread t = new Thread () { public void run () { try { startGate . await (); try { task . run (); } finally { endGate . countDown (); } } catch ( InterruptedException ignored ) {} } }; t . start (); } long start = System . nanoTime (); startGate . countDown (); endGate . await (); long end = System . nanoTime (); return end - start ; } } TestHarness illustrates two common uses for latches. TestHarness creates a number of threads that run a given task concurrently. It uses two latches, a \u201cstarting gate\u201d and an \u201cending gate\u201d. The starting gate is initialized with a count of one; the ending gate is initialized with a count equal to the number of worker threads. The first thing each worker thread does is wait on the starting gate; this ensures that none of them starts working until they all are ready to start. The last thing each does is count down on the ending gate; this allows the master thread to wait efficiently until the last of the worker threads has finished, so it can calculate the elapsed time. FutureTask FutureTask also acts like a latch. ( FutureTask implements Future , which describes an abstract result-bearing computation.) A computation represented by a FutureTask is implemented with a Callable , the result-bearing equivalent of Runnable , and can be in one of three states: waiting to run, running, or completed. Completion subsumes all the ways a computation can complete, including normal completion, cancellation, and exception. Once a FutureTask enters the completed state, it stays in that state forever. The behavior of Future.get depends on the state of the task. If it is completed, get returns the result immediately, and otherwise blocks until the task transitions to the completed state and then returns the result or throws an exception. FutureTask transports the result from the thread executing the computation to the thread(s) retrieving the result; the specification of FutureTask guarantees that this transfer constitutes a safe publication of the result. It is inadvisable to start a thread from a constructor or static initializer. Semaphores Counting semaphores are used to control the number of activities that can access a certain resource or perform a given action at the same time. Counting semaphores can be used to implement resource pools or to impose a bound on a collection. A Semaphore manages a set of virtual permits; the initial number of permits is passed to the Semaphore constructor. Activities can acquire permits (as long as some remain) and release permits when they are done with them. If no permit is available, acquire blocks until one is (or until interrupted or the operation times out). The release method returns a permit to the semaphore 2 . A degenerate case of a counting semaphore is a binary semaphore, a Semaphore with an initial count of one. A binary semaphore can be used as a mutex with nonreentrant locking semantics; whoever holds the sole permit holds the mutex. Semaphores are useful for implementing resource pools such as database connection pools. While it is easy to construct a fixed-sized pool that fails if you request a resource from an empty pool, what you really want is to block if the pool is empty and unblock when it becomes nonempty again. If you initialize a Semaphore to the pool size, acquire a permit before trying to fetch a resource from the pool, and release the permit after putting a resource back in the pool, acquire blocks until the pool becomes nonempty. public class BoundedHashSet < T > { private final Set < T > set ; private final Semaphore sem ; public BoundedHashSet ( int bound ) { this . set = Collections . synchronizedSet ( new HashSet < T >()); sem = new Semaphore ( bound ); } public boolean add ( T o ) throws InterruptedException { sem . acquire (); boolean wasAdded = false ; try { wasAdded = set . add ( o ); return wasAdded ; } finally { if (! wasAdded ) sem . release (); } } public boolean remove ( Object o ) { boolean wasRemoved = set . remove ( o ); if ( wasRemoved ) sem . release (); return wasRemoved ; } } The semaphore is initialized to the desired maximum size of the collection. The add operation acquires a permit before adding the item into the underlying collection. If the underlying add operation does not actually add anything, it releases the permit immediately. Similarly, a successful remove operation releases a permit, enabling more elements to be added. The underlying Set implementation knows nothing about the bound; this is handled by BoundedHashSet. Barriers Barriers are similar to latches in that they block a group of threads until some event has occurred. The key difference is that with a barrier, all the threads must come together at a barrier point at the same time in order to proceed. Latches are for waiting for events; barriers are for waiting for other threads. A barrier implements the protocol some families use to meet at an agreed time and place during a day at the mall: \u201cEveryone meet at McDonald\u2019s at 6:00; once you get there, stay there until everyone shows up, and then we\u2019ll figure out what we\u2019re doing next.\u201d CyclicBarrier It allows a fixed number of parties to rendezvous repeatedly at a barrier point and is useful in parallel iterative algorithms that break down a problem into a fixed number of independent subproblems. Threads call await when they reach the barrier point, and await blocks until all the threads have reached the barrier point. If all threads meet at the barrier point, the barrier has been successfully passed, in which case all threads are released and the barrier is reset so it can be used again. If a call to await times out or a thread blocked in await is interrupted, then the barrier is considered broken and all outstanding calls to await terminate with BrokenBarrierException . If the barrier is successfully passed, await returns a unique arrival index for each thread, which can be used to \u201celect\u201d a leader that takes some special action in the next iteration. CyclicBarrier also lets you pass a barrier action to the constructor; this is a Runnable that is executed (in one of the subtask threads) when the barrier is successfully passed but before the blocked threads are released. Another form of barrier is Exchanger , a two-party barrier in which the parties exchange data at the barrier point. Exchangers are useful when the parties perform asymmetric activities, for example when one thread fills a buffer with data and the other thread consumes the data from the buffer; these threads could use an Exchanger to meet and exchange a full buffer for an empty one. When two threads exchange objects via an Exchanger, the exchange constitutes a safe publication of both objects to the other party. The timing of the exchange depends on the responsiveness requirements of the application. The simplest approach is that the filling task exchanges when the buffer is full, and the emptying task exchanges when the buffer is empty; this minimizes the number of exchanges but can delay processing of some data if the arrival rate of new data is unpredictable. Another approach would be that the filler exchanges when the buffer is full, but also when the buffer is partially filled and a certain amount of time has elapsed. public class CellularAutomata { private final Board mainBoard ; private final CyclicBarrier barrier ; private final Worker [] workers ; public CellularAutomata ( Board board ) { this . mainBoard = board ; int count = Runtime . getRuntime (). availableProcessors (); this . barrier = new CyclicBarrier ( count , new Runnable () { public void run () { mainBoard . commitNewValues (); }}); this . workers = new Worker [ count ]; for ( int i = 0 ; i < count ; i ++) workers [ i ] = new Worker ( mainBoard . getSubBoard ( count , i )); } private class Worker implements Runnable { private final Board board ; public Worker ( Board board ) { this . board = board ; } public void run () { while (! board . hasConverged ()) { for ( int x = 0 ; x < board . getMaxX (); x ++) for ( int y = 0 ; y < board . getMaxY (); y ++) board . setNewValue ( x , y , computeValue ( x , y )); try { barrier . await (); } catch ( InterruptedException ex ) { return ; } catch ( BrokenBarrierException ex ) { return ; } } } private int computeValue ( int x , int y ) { // Compute the new value that goes in (x,y) return 0 ; } } public void start () { for ( int i = 0 ; i < workers . length ; i ++) new Thread ( workers [ i ]). start (); mainBoard . waitForConvergence (); } interface Board { int getMaxX (); int getMaxY (); int getValue ( int x , int y ); int setNewValue ( int x , int y , int value ); void commitNewValues (); boolean hasConverged (); void waitForConvergence (); Board getSubBoard ( int numPartitions , int index ); } } CellularAutomata in above listing demonstrates using a barrier to compute a cellular automata simulation. When parallelizing a simulation, it is generally impractical to assign a separate thread to each element (in the case of Life, a cell); this would require too many threads, and the overhead of coordinating them would dwarf the computation. Instead, it makes sense to partition the problem into a number of subparts, let each thread solve a subpart, and then merge the results. CellularAutomata partitions the board into N-cpu parts, where N-cpu is the number of CPUs available, and assigns each part to a thread. At each step, the worker threads calculate new values for all the cells in their part of the board. When all worker threads have reached the barrier, the barrier action commits the new values to the data model. After the barrier action runs, the worker threads are released to compute the next step of the calculation, which includes consulting an isDone method to determine whether further iterations are required. ConcurrentModificationException can arise in single-threaded code as well; this happens when objects are removed from the collection directly rather than through Iterator.remove . \u21a9 The implementation has no actual permit objects, and Semaphore does not associate dispensed permits with threads, so a permit acquired in one thread can be released from another thread. You can think of acquire as consuming a permit and release as creating one; a Semaphore is not limited to the number of permits it was created with. \u21a9","title":"Building blocks"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#5-building-blocks","text":"Where practical, delegation is one of the most effective strategies for creating thread-safe classes: just let existing thread-safe classes manage all the state.","title":"5. Building Blocks"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#synchronized-collections","text":"","title":"Synchronized collections"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#problems-with-synchronized-collections","text":"The synchronized collections are thread-safe, but you may sometimes need to use additional client-side locking to guard compound actions. Because the synchronized collections commit to a synchronization policy that supports client-side locking, it is possible to create new operations that are atomic with respect to other collection operations as long as we know which lock to use. The synchronized collection classes guard each method with the lock on the synchronized collection object itself. The problem of unreliable iteration can again be addressed by client-side locking, at some additional cost to scalability. By holding the Vector lock for the duration of iteration, we prevent other threads from modifying the Vector while we are iterating it. Unfortunately, we also prevent other threads from accessing it at all during this time, impairing concurrency.","title":"Problems with synchronized collections"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#iterators-and-concurrentmodificationexception","text":"The standard way to iterate a Collection is with an Iterator , either explicitly or through the for-each loop syntax introduced in Java 5.0, but using iterators does not obviate the need to lock the collection during iteration if other threads can concurrently modify it. The iterators returned by the synchronized collections are not designed to deal with concurrent modification, and they are fail-fast\u2014meaning that if they detect that the collection has changed since iteration began, they throw the unchecked ConcurrentModificationException . These fail-fast iterators are not designed to be foolproof\u2014they are designed to catch concurrency errors on a \u201cgood-faith-effort\u201d basis and thus act only as early-warning indicators for concurrency problems. They are implemented by associating a modification count with the collection: if the modification count changes during iteration, hasNext or next throws ConcurrentModificationException . However, this check is done without synchronization, so there is a risk of seeing a stale value of the modification count and therefore that the iterator does not realize a modification has been made. This was a deliberate design tradeoff to reduce the performance impact of the concurrent modification detection code 1 . List < Widget > widgetList = Collections . synchronizedList ( new ArrayList < Widget >()); // ... // May throw ConcurrentModificationException for ( Widget w : widgetList ) doSomething ( w ); There are several reasons, however, why locking a collection during iteration may be undesirable. Other threads that need to access the collection will block until the iteration is complete; if the collection is large or the task performed for each element is lengthy, they could wait a long time. Also, if the collection is locked as above listing, doSomething is being called with a lock held, which is a risk factor for deadlock (see Chapter 10). Even in the absence of starvation or deadlock risk, locking collections for significant periods of time hurts application scalability. The longer a lock is held, the more likely it is to be contended, and if many threads are blocked waiting for a lock throughput and CPU utilization can suffer. An alternative to locking the collection during iteration is to clone the collection and iterate the copy instead. Since the clone is thread-confined, no other thread can modify it during iteration, eliminating the possibility of ConcurrentModificationException . (The collection still must be locked during the clone operation itself.) Cloning the collection has an obvious performance cost; whether this is a favorable tradeoff depends on many factors including the size of the collection, how much work is done for each element, the relative frequency of iteration compared to other collection operations, and responsiveness and throughput requirements.","title":"Iterators and ConcurrentModificationException"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#hidden-iterators","text":"While locking can prevent iterators from throwing ConcurrentModificationException , you have to remember to use locking everywhere a shared collection might be iterated. This is trickier than it sounds, as iterators are sometimes hidden (like in toString ). private final Set < Integer > mySet = new HashSet < Integer >(); public void addTenThings () { System . out . println ( \"DEBUG: added ten elements to \" + set ); } The real lesson here is that the greater the distance between the state and the synchronization that guards it, the more likely that someone will forget to use proper synchronization when accessing that state. By using a synchronizedSet , encapsulating the synchronization, this sort of error would not occur. Just as encapsulating an object\u2019s state makes it easier to preserve its invariants, encapsulating its synchronization makes it easier to enforce its synchronization policy. Iteration is also indirectly invoked by the collection\u2019s hashCode and equals methods, which may be called if the collection is used as an element or key of another collection. Similarly, the containsAll , removeAll , and retainAll methods, as well as the constructors that take collections as arguments, also iterate the collection. All of these indirect uses of iteration can cause ConcurrentModificationException .","title":"Hidden iterators"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#concurrent-collections","text":"Java 5.0 improves on the synchronized collections by providing several concurrent collection classes. Synchronized collections achieve their thread safety by serializing all access to the collection\u2019s state. The cost of this approach is poor concurrency; when multiple threads contend for the collection-wide lock, throughput suffers. The concurrent collections, on the other hand, are designed for concurrent access from multiple threads. Java 5.0 adds ConcurrentHashMap , a replacement for synchronized hash-based Map implementations, and CopyOnWriteArrayList , a replacement for synchronized List implementations for cases where traversal is the dominant operation. The new ConcurrentMap interface adds support for common compound actions such as put-if-absent, replace, and conditional remove. Replacing synchronized collections with concurrent collections can offer dramatic scalability improvements with little risk. Java 5.0 also adds two new collection types, Queue and BlockingQueue . A Queue is intended to hold a set of elements temporarily while they await processing. Several implementations are provided, including ConcurrentLinkedQueue , a traditional FIFO queue, and PriorityQueue , a (non concurrent) priority ordered queue. Queue classes were added because eliminating the random-access requirements of List admits more efficient concurrent implementations. BlockingQueue extends Queue to add blocking insertion and retrieval operations. If the queue is empty, a retrieval blocks until an element is available, and if the queue is full (for bounded queues) an insertion blocks until there is space available. Blocking queues are extremely useful in producer-consumer designs. Just as ConcurrentHashMap is a concurrent replacement for a synchronized hash-based Map, Java 6 adds ConcurrentSkipListMap and ConcurrentSkipListSet , which are concurrent replacements for a synchronized SortedMap or SortedSet (such as TreeMap or TreeSet wrapped with synchronizedMap ).","title":"Concurrent collections"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#concurrenthashmap","text":"ConcurrentHashMap is a hash-based Map like HashMap, but it uses an entirely different locking strategy that offers better concurrency and scalability. Instead of synchronizing every method on a common lock, restricting access to a single thread at a time, it uses a finer-grained locking mechanism called lock striping to allow a greater degree of shared access. Arbitrarily many reading threads can access the map concurrently, readers can access the map concurrently with writers, and a limited number of writers can modify the map concurrently. The result is far higher throughput under concurrent access, with little performance penalty for single-threaded access. ConcurrentHashMap , along with the other concurrent collections, further improve on the synchronized collection classes by providing iterators that do not throw ConcurrentModificationException , thus eliminating the need to lock the collection during iteration. The iterators returned by ConcurrentHashMap are weakly consistent instead of fail-fast. A weakly consistent iterator can tolerate concurrent modification, traverses elements as they existed when the iterator was constructed, and may (but is not guaranteed to) reflect modifications to the collection after the construction of the iterator. As with all improvements, there are still a few tradeoffs: size is allowed to return an approximation instead of an exact count or isEmpty may not reflect if the structure is empty. So the requirements for these operations were weakened to enable performance optimizations for the most important operations, primarily get , put , containsKey , and remove . Because it has so many advantages and so few disadvantages compared to Hashtable or synchronizedMap , replacing synchronized Map implementations with ConcurrentHashMap in most cases results only in better scalability. Only if your application needs to lock the map for exclusive access is ConcurrentHashMap not an appropriate drop-in replacement. Since a ConcurrentHashMap cannot be locked for exclusive access, we cannot use client-side locking to create new atomic operations such as put-if-absent but it already have those implemented.","title":"ConcurrentHashMap"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#copyonwritearraylist","text":"CopyOnWriteArrayList is a concurrent replacement for a synchronized List that offers better concurrency in some common situations and eliminates the need to lock or copy the collection during iteration. The copy-on-write collections derive their thread safety from the fact that as long as an effectively immutable object is properly published, no further synchronization is required when accessing it. They implement mutability by creating and republishing a new copy of the collection every time it is modified. Iterators for the copy-on-write collections retain a reference to the backing array that was current at the start of iteration, and since this will never change, they need to synchronize only briefly to ensure visibility of the array contents. As a result, multiple threads can iterate the collection without interference from one another or from threads wanting to modify the collection. The iterators returned by the copy-on-write collections do not throw ConcurrentModificationException and return the elements exactly as they were at the time the iterator was created, regardless of subsequent modifications.","title":"CopyOnWriteArrayList"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#blocking-queues-and-the-producer-consumer-pattern","text":"Blocking queues provide blocking put and take methods as well as the timed equivalents offer and poll . If the queue is full, put blocks until space becomes available; if the queue is empty, take blocks until an element is available. Queues can be bounded or unbounded; unbounded queues are never full, so a put on an unbounded queue never blocks. The familiar division of labor for two people washing the dishes is an example of a producer-consumer design: one person washes the dishes and places them in the dish rack, and the other person retrieves the dishes from the rack and dries them. In this scenario, the dish rack acts as a blocking queue; if there are no dishes in the rack, the consumer waits until there are dishes to dry, and if the rack fills up, the producer has to stop washing until there is more space. The last BlockingQueue implementation, SynchronousQueue , is not really a queue at all, in that it maintains no storage space for queued elements. Instead, it maintains a list of queued threads waiting to enqueue or dequeue an element. In the dish-washing analogy, this would be like having no dish rack, but instead handing the washed dishes directly to the next available dryer. While this may seem a strange way to implement a queue, it reduces the latency associated with moving data from producer to consumer because the work can be handed off directly. Since a SynchronousQueue has no storage capacity, put and take will block unless another thread is already waiting to participate in the handoff. Synchronous queues are generally suitable only when there are enough consumers that there nearly always will be one ready to take the handoff.","title":"Blocking queues and the producer-consumer pattern"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#serial-thread-confinement","text":"For mutable objects, producer-consumer designs and blocking queues facilitate serial thread confinement for handing off ownership of objects from producers to consumers. A thread-confined object is owned exclusively by a single thread, but that ownership can be \u201ctransferred\u201d by publishing it safely where only one other thread will gain access to it and ensuring that the publishing thread does not access it after the handoff. The safe publication ensures that the object\u2019s state is visible to the new owner, and since the original owner will not touch it again, it is now confined to the new thread. The new owner may modify it freely since it has exclusive access.","title":"Serial thread confinement"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#deques-and-work-stealing","text":"Java 6 also adds another two collection types, Deque (pronounced \u201cdeck\u201d) and BlockingDeque , that extend Queue and BlockingQueue . A Deque is a double-ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque . Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else\u2019s deque.","title":"Deques and work stealing"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#blocking-and-interruptible-methods","text":"The distinction between a blocking operation and an ordinary operation that merely takes a long time to finish is that a blocked thread must wait for an event that is beyond its control before it can proceed\u2014the I/O completes, the lock becomes available, or the external computation finishes. When that external event occurs, the thread is placed back in the RUNNABLE state and becomes eligible again for scheduling. The put and take methods of BlockingQueue throw the checked InterruptedException , as do a number of other library methods such as Thread.sleep . When a method can throw InterruptedException, it is telling you that it is a blocking method, and further that if it is interrupted, it will make an effort to stop blocking early. Thread provides the interrupt method for interrupting a thread and for querying whether a thread has been interrupted. Each thread has a boolean property that represents its interrupted status; interrupting a thread sets this status. Interruption is a cooperative mechanism. One thread cannot force another to stop what it is doing and do something else; when thread A interrupts thread B, A is merely requesting that B stop what it is doing when it gets to a convenient stopping point\u2014if it feels like it. While there is nothing in the API or language specification that demands any specific application-level semantics for interruption, the most sensible use for interruption is to cancel an activity. Blocking methods that are responsive to interruption make it easier to cancel long-running activities on a timely basis. When your code calls a method that throws InterruptedException , then your method is a blocking method too, and must have a plan for responding to interruption. For library code, there are basically two choices: Propagate the InterruptedException. This is often the most sensible policy if you can get away with it\u2014just propagate the InterruptedException to your caller. This could involve not catching InterruptedException, or catching it and throwing it again after performing some brief activity-specific cleanup. Restore the interrupt. Sometimes you cannot throw InterruptedException, for instance when your code is part of a Runnable. In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued. public class TaskRunnable implements Runnable { BlockingQueue < Task > queue ; ... public void run () { try { processTask ( queue . take ()); } catch ( InterruptedException e ) { // restore interrupted status Thread . currentThread (). interrupt (); } } } You can get much more sophisticated with interruption, but these two approaches should work in the vast majority of situations. But there is one thing you should not do with InterruptedException \u2014catch it and do nothing in response. The only situation in which it is acceptable to swallow an interrupt is when you are extending Thread and therefore control all the code higher up on the call stack.","title":"Blocking and interruptible methods"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#synchronizers","text":"A synchronizer is any object that coordinates the control flow of threads based on its state. Blocking queues can act as synchronizers (they coordinate the control flow of producer and consumer threads because take and put block until the queue enters the desired state (not empty or not full)); other types of synchronizers include semaphores, barriers, and latches. All synchronizers share certain structural properties: they encapsulate state that determines whether threads arriving at the synchronizer should be allowed to pass or forced to wait, provide methods to manipulate that state, and provide methods to wait efficiently for the synchronizer to enter the desired state.","title":"Synchronizers"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#latches","text":"A latch is a synchronizer that can delay the progress of threads until it reaches its terminal state then it will allow threads to continue. Once the latch reaches the terminal state, it cannot change state again, so it remains open forever. Latches can be used to ensure that certain activities do not proceed until other one-time activities complete. Ensuring that a computation does not proceed until resources it needs have been initialized. Ensuring that a service does not start until other services on which it depends have started. Waiting until all the parties involved in an activity, for instance the players in a multi-player game, are ready to proceed.","title":"Latches"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#countdownlatch","text":"It is a flexible latch implementation that can be used in any of these situations; it allows one or more threads to wait for a set of events to occur. The latch state consists of a counter initialized to a positive number, representing the number of events to wait for. The countDown method decrements the counter, indicating that an event has occurred, and the await methods wait for the counter to reach zero, which happens when all the events have occurred. If the counter is nonzero on entry, await blocks until the counter reaches zero, the waiting thread is interrupted, or the wait times out. public class TestHarness { public long timeTasks ( int nThreads , final Runnable task ) throws InterruptedException { final CountDownLatch startGate = new CountDownLatch ( 1 ); final CountDownLatch endGate = new CountDownLatch ( nThreads ); for ( int i = 0 ; i < nThreads ; i ++) { Thread t = new Thread () { public void run () { try { startGate . await (); try { task . run (); } finally { endGate . countDown (); } } catch ( InterruptedException ignored ) {} } }; t . start (); } long start = System . nanoTime (); startGate . countDown (); endGate . await (); long end = System . nanoTime (); return end - start ; } } TestHarness illustrates two common uses for latches. TestHarness creates a number of threads that run a given task concurrently. It uses two latches, a \u201cstarting gate\u201d and an \u201cending gate\u201d. The starting gate is initialized with a count of one; the ending gate is initialized with a count equal to the number of worker threads. The first thing each worker thread does is wait on the starting gate; this ensures that none of them starts working until they all are ready to start. The last thing each does is count down on the ending gate; this allows the master thread to wait efficiently until the last of the worker threads has finished, so it can calculate the elapsed time.","title":"CountDownLatch"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#futuretask","text":"FutureTask also acts like a latch. ( FutureTask implements Future , which describes an abstract result-bearing computation.) A computation represented by a FutureTask is implemented with a Callable , the result-bearing equivalent of Runnable , and can be in one of three states: waiting to run, running, or completed. Completion subsumes all the ways a computation can complete, including normal completion, cancellation, and exception. Once a FutureTask enters the completed state, it stays in that state forever. The behavior of Future.get depends on the state of the task. If it is completed, get returns the result immediately, and otherwise blocks until the task transitions to the completed state and then returns the result or throws an exception. FutureTask transports the result from the thread executing the computation to the thread(s) retrieving the result; the specification of FutureTask guarantees that this transfer constitutes a safe publication of the result. It is inadvisable to start a thread from a constructor or static initializer.","title":"FutureTask"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#semaphores","text":"Counting semaphores are used to control the number of activities that can access a certain resource or perform a given action at the same time. Counting semaphores can be used to implement resource pools or to impose a bound on a collection. A Semaphore manages a set of virtual permits; the initial number of permits is passed to the Semaphore constructor. Activities can acquire permits (as long as some remain) and release permits when they are done with them. If no permit is available, acquire blocks until one is (or until interrupted or the operation times out). The release method returns a permit to the semaphore 2 . A degenerate case of a counting semaphore is a binary semaphore, a Semaphore with an initial count of one. A binary semaphore can be used as a mutex with nonreentrant locking semantics; whoever holds the sole permit holds the mutex. Semaphores are useful for implementing resource pools such as database connection pools. While it is easy to construct a fixed-sized pool that fails if you request a resource from an empty pool, what you really want is to block if the pool is empty and unblock when it becomes nonempty again. If you initialize a Semaphore to the pool size, acquire a permit before trying to fetch a resource from the pool, and release the permit after putting a resource back in the pool, acquire blocks until the pool becomes nonempty. public class BoundedHashSet < T > { private final Set < T > set ; private final Semaphore sem ; public BoundedHashSet ( int bound ) { this . set = Collections . synchronizedSet ( new HashSet < T >()); sem = new Semaphore ( bound ); } public boolean add ( T o ) throws InterruptedException { sem . acquire (); boolean wasAdded = false ; try { wasAdded = set . add ( o ); return wasAdded ; } finally { if (! wasAdded ) sem . release (); } } public boolean remove ( Object o ) { boolean wasRemoved = set . remove ( o ); if ( wasRemoved ) sem . release (); return wasRemoved ; } } The semaphore is initialized to the desired maximum size of the collection. The add operation acquires a permit before adding the item into the underlying collection. If the underlying add operation does not actually add anything, it releases the permit immediately. Similarly, a successful remove operation releases a permit, enabling more elements to be added. The underlying Set implementation knows nothing about the bound; this is handled by BoundedHashSet.","title":"Semaphores"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#barriers","text":"Barriers are similar to latches in that they block a group of threads until some event has occurred. The key difference is that with a barrier, all the threads must come together at a barrier point at the same time in order to proceed. Latches are for waiting for events; barriers are for waiting for other threads. A barrier implements the protocol some families use to meet at an agreed time and place during a day at the mall: \u201cEveryone meet at McDonald\u2019s at 6:00; once you get there, stay there until everyone shows up, and then we\u2019ll figure out what we\u2019re doing next.\u201d","title":"Barriers"},{"location":"java/concurrency/fundamentals/c05-building-blocks/#cyclicbarrier","text":"It allows a fixed number of parties to rendezvous repeatedly at a barrier point and is useful in parallel iterative algorithms that break down a problem into a fixed number of independent subproblems. Threads call await when they reach the barrier point, and await blocks until all the threads have reached the barrier point. If all threads meet at the barrier point, the barrier has been successfully passed, in which case all threads are released and the barrier is reset so it can be used again. If a call to await times out or a thread blocked in await is interrupted, then the barrier is considered broken and all outstanding calls to await terminate with BrokenBarrierException . If the barrier is successfully passed, await returns a unique arrival index for each thread, which can be used to \u201celect\u201d a leader that takes some special action in the next iteration. CyclicBarrier also lets you pass a barrier action to the constructor; this is a Runnable that is executed (in one of the subtask threads) when the barrier is successfully passed but before the blocked threads are released. Another form of barrier is Exchanger , a two-party barrier in which the parties exchange data at the barrier point. Exchangers are useful when the parties perform asymmetric activities, for example when one thread fills a buffer with data and the other thread consumes the data from the buffer; these threads could use an Exchanger to meet and exchange a full buffer for an empty one. When two threads exchange objects via an Exchanger, the exchange constitutes a safe publication of both objects to the other party. The timing of the exchange depends on the responsiveness requirements of the application. The simplest approach is that the filling task exchanges when the buffer is full, and the emptying task exchanges when the buffer is empty; this minimizes the number of exchanges but can delay processing of some data if the arrival rate of new data is unpredictable. Another approach would be that the filler exchanges when the buffer is full, but also when the buffer is partially filled and a certain amount of time has elapsed. public class CellularAutomata { private final Board mainBoard ; private final CyclicBarrier barrier ; private final Worker [] workers ; public CellularAutomata ( Board board ) { this . mainBoard = board ; int count = Runtime . getRuntime (). availableProcessors (); this . barrier = new CyclicBarrier ( count , new Runnable () { public void run () { mainBoard . commitNewValues (); }}); this . workers = new Worker [ count ]; for ( int i = 0 ; i < count ; i ++) workers [ i ] = new Worker ( mainBoard . getSubBoard ( count , i )); } private class Worker implements Runnable { private final Board board ; public Worker ( Board board ) { this . board = board ; } public void run () { while (! board . hasConverged ()) { for ( int x = 0 ; x < board . getMaxX (); x ++) for ( int y = 0 ; y < board . getMaxY (); y ++) board . setNewValue ( x , y , computeValue ( x , y )); try { barrier . await (); } catch ( InterruptedException ex ) { return ; } catch ( BrokenBarrierException ex ) { return ; } } } private int computeValue ( int x , int y ) { // Compute the new value that goes in (x,y) return 0 ; } } public void start () { for ( int i = 0 ; i < workers . length ; i ++) new Thread ( workers [ i ]). start (); mainBoard . waitForConvergence (); } interface Board { int getMaxX (); int getMaxY (); int getValue ( int x , int y ); int setNewValue ( int x , int y , int value ); void commitNewValues (); boolean hasConverged (); void waitForConvergence (); Board getSubBoard ( int numPartitions , int index ); } } CellularAutomata in above listing demonstrates using a barrier to compute a cellular automata simulation. When parallelizing a simulation, it is generally impractical to assign a separate thread to each element (in the case of Life, a cell); this would require too many threads, and the overhead of coordinating them would dwarf the computation. Instead, it makes sense to partition the problem into a number of subparts, let each thread solve a subpart, and then merge the results. CellularAutomata partitions the board into N-cpu parts, where N-cpu is the number of CPUs available, and assigns each part to a thread. At each step, the worker threads calculate new values for all the cells in their part of the board. When all worker threads have reached the barrier, the barrier action commits the new values to the data model. After the barrier action runs, the worker threads are released to compute the next step of the calculation, which includes consulting an isDone method to determine whether further iterations are required. ConcurrentModificationException can arise in single-threaded code as well; this happens when objects are removed from the collection directly rather than through Iterator.remove . \u21a9 The implementation has no actual permit objects, and Semaphore does not associate dispensed permits with threads, so a permit acquired in one thread can be released from another thread. You can think of acquire as consuming a permit and release as creating one; a Semaphore is not limited to the number of permits it was created with. \u21a9","title":"CyclicBarrier"},{"location":"java/concurrency/fundamentals/summary/","text":"Summary of Part I It\u2019s the mutable state, stupid. All concurrency issues boil down to coordinating access to mutable state. The less mutable state, the easier it is to ensure thread safety. Make fields final unless they need to be mutable. Immutable objects are automatically thread-safe. Immutable objects simplify concurrent programming tremendously. They are simpler and safer, and can be shared freely without locking or defensive copying. Encapsulation makes it practical to manage the complexity. You could write a thread-safe program with all data stored in global variables, but why would you want to? Encapsulating data within objects makes it easier to preserve their invariants; encapsulating synchronization within objects makes it easier to comply with their synchronization policy. Guard each mutable variable with a lock. Guard all variables in an invariant with the same lock. Hold locks for the duration of compound actions. A program that accesses a mutable variable from multiple threads without synchronization is a broken program. Don\u2019t rely on clever reasoning about why you don\u2019t need to synchronize. Include thread safety in the design process\u2014or explicitly document that your class is not thread-safe. Document your synchronization policy. Review volatile and ThreadLocal concepts (again)","title":"Summary Part I"},{"location":"java/concurrency/fundamentals/summary/#summary-of-part-i","text":"It\u2019s the mutable state, stupid. All concurrency issues boil down to coordinating access to mutable state. The less mutable state, the easier it is to ensure thread safety. Make fields final unless they need to be mutable. Immutable objects are automatically thread-safe. Immutable objects simplify concurrent programming tremendously. They are simpler and safer, and can be shared freely without locking or defensive copying. Encapsulation makes it practical to manage the complexity. You could write a thread-safe program with all data stored in global variables, but why would you want to? Encapsulating data within objects makes it easier to preserve their invariants; encapsulating synchronization within objects makes it easier to comply with their synchronization policy. Guard each mutable variable with a lock. Guard all variables in an invariant with the same lock. Hold locks for the duration of compound actions. A program that accesses a mutable variable from multiple threads without synchronization is a broken program. Don\u2019t rely on clever reasoning about why you don\u2019t need to synchronize. Include thread safety in the design process\u2014or explicitly document that your class is not thread-safe. Document your synchronization policy. Review volatile and ThreadLocal concepts (again)","title":"Summary of Part I"},{"location":"java/concurrency/introduction/c01-intro/","text":"1. Java Concurrency The language provides low-level mechanisms such as synchronization and condition waits , but these mechanisms must be used consistently to implement application-level protocols or policies. Without such policies, it is all too easy to create programs that compile and appear to work but are nevertheless broken. Processes Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously: Resource utilization . Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run. Fairness . Multiple users and programs may have equal claims on the machine\u2019s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start an- other. Convenience . It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks. A daily life sample The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence\u2014mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions\u2014open the cupboard, select a flavor of tea, measure some tea into the pot, see if there\u2019s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step\u2014waiting for the water to boil\u2014also involves a degree of asynchrony. While the water is heating, you have a choice of what to do\u2014just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people\u2014and the same is true of programs . Threads The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of threads. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs. Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms . But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results. Benefits of threads When used properly, threads can reduce development and maintenance costs and improve the performance of complex applications. Threads make it easier to model how humans work and interact, by turning asynchronous workflows into mostly sequential ones. They can also turn otherwise convoluted code into straight-line code that is easier to write, read, and maintain. Exploiting multiple processors Since the basic unit of scheduling is the thread, a program with only one thread can run on at most one processor at a time. On a two-processor system, a single-threaded program is giving up access to half the available CPU resources; on a 100-processor system, it is giving up access to 99%. On the other hand, programs with multiple active threads can execute simultaneously on multiple processors. When properly designed, multithreaded programs can improve throughput by utilizing available processor resources more effectively. Using multiple threads can also help achieve better throughput on single-processor systems. If a program is single-threaded, the processor remains idle while it waits for a synchronous I/O operation to complete. In a multithreaded program, another thread can still run while the first thread is waiting for the I/O to complete, allowing the application to still make progress during the blocking I/O. (This is like reading the newspaper while waiting for the water to boil, rather than waiting for the water to boil before starting to read.) Simplicity of modeling It is often easier to manage your time when you have only one type of task to perform (fix these twelve bugs) than when you have several (fix the bugs, interview replacement candidates for the system administrator, complete your team\u2019s performance evaluations, and create the slides for your presentation next week). When you have only one type of task to do, you can start at the top of the pile and keep working until the pile is exhausted (or you are); you don\u2019t have to spend any mental energy figuring out what to work on next. On the other hand, managing multiple priorities and deadlines and switching from task to task usually carries some overhead. The same is true for software: a program that processes one type of task sequentially is simpler to write, less error-prone, and easier to test than one managing multiple different types of tasks at once. Assigning a thread to each type of task or to each element in a simulation affords the illusion of sequentiality and insulates domain logic from the details of scheduling, interleaved operations, asynchronous I/O, and resource waits. A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points. Simplified handling of asynchronous events A server application that accepts socket connections from multiple remote clients may be easier to develop when each connection is allocated its own thread and allowed to use synchronous I/O. If an application goes to read from a socket when no data is available, read blocks until some data is available. In a single-threaded application, this means that not only does processing the corresponding request stall, but processing of all requests stalls while the single thread is blocked. To avoid this problem, single-threaded server applications are forced to use nonblocking I/O, which is far more complicated and error-prone than synchronous I/O. However, if each request has its own thread, then blocking does not affect the processing of other requests. Historically, operating systems placed relatively low limits on the number of threads that a process could create, as few as several hundred (or even less). As a result, operating systems developed efficient facilities for multiplexed I/O, such as the Unix select and poll system calls, and to access these facilities, the Java class libraries acquired a set of packages (java.nio) for nonblocking I/O. However, operating system support for larger numbers of threads has improved significantly, making the thread-per-client model practical even for large numbers of clients on some platforms. Risks of threads Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising. Safety hazards Because threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. This is a tremendous convenience, because it makes data sharing much easier than would other inter-thread communications mechanisms. But it is also a significant risk: threads can be confused by having data change unexpectedly. Allowing multiple threads to access and modify the same variables introduces an element of nonsequentiality into an otherwise sequential programming model, which can be confusing and difficult to reason about. For a multithreaded program\u2019s behavior to be predictable, access to shared variables must be properly coordinated so that threads do not interfere with one another. Liveness hazards A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. One form of liveness failure that can occur in sequential programs is an inadvertent infinite loop, where the code that follows the loop never gets executed. The use of threads introduces additional liveness risks. For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever. Like most concurrency bugs, bugs that cause liveness failures can be elusive because they depend on the relative timing of events in different threads, and therefore do not always manifest themselves in development or testing. Performance hazards Performance issues subsume a broad range of problems, including poor service time, responsiveness, throughput, resource consumption, or scalability. Just as with safety and liveness, multithreaded programs are subject to all the performance hazards of single-threaded programs, and to others as well that are introduced by the use of threads. In well designed concurrent applications the use of threads is a net performance gain, but threads nevertheless carry some degree of runtime overhead. Context switches\u2014when the scheduler suspends the active thread temporarily so another thread can run\u2014are more frequent in applications with many threads, and have significant costs: saving and restoring execution context, loss of locality, and CPU time spent scheduling threads instead of running them. When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus.","title":"Introduction"},{"location":"java/concurrency/introduction/c01-intro/#1-java-concurrency","text":"The language provides low-level mechanisms such as synchronization and condition waits , but these mechanisms must be used consistently to implement application-level protocols or policies. Without such policies, it is all too easy to create programs that compile and appear to work but are nevertheless broken.","title":"1. Java Concurrency"},{"location":"java/concurrency/introduction/c01-intro/#processes","text":"Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously: Resource utilization . Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run. Fairness . Multiple users and programs may have equal claims on the machine\u2019s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start an- other. Convenience . It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks.","title":"Processes"},{"location":"java/concurrency/introduction/c01-intro/#a-daily-life-sample","text":"The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence\u2014mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions\u2014open the cupboard, select a flavor of tea, measure some tea into the pot, see if there\u2019s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step\u2014waiting for the water to boil\u2014also involves a degree of asynchrony. While the water is heating, you have a choice of what to do\u2014just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people\u2014and the same is true of programs .","title":"A daily life sample"},{"location":"java/concurrency/introduction/c01-intro/#threads","text":"The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of threads. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs. Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms . But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results.","title":"Threads"},{"location":"java/concurrency/introduction/c01-intro/#benefits-of-threads","text":"When used properly, threads can reduce development and maintenance costs and improve the performance of complex applications. Threads make it easier to model how humans work and interact, by turning asynchronous workflows into mostly sequential ones. They can also turn otherwise convoluted code into straight-line code that is easier to write, read, and maintain.","title":"Benefits of threads"},{"location":"java/concurrency/introduction/c01-intro/#exploiting-multiple-processors","text":"Since the basic unit of scheduling is the thread, a program with only one thread can run on at most one processor at a time. On a two-processor system, a single-threaded program is giving up access to half the available CPU resources; on a 100-processor system, it is giving up access to 99%. On the other hand, programs with multiple active threads can execute simultaneously on multiple processors. When properly designed, multithreaded programs can improve throughput by utilizing available processor resources more effectively. Using multiple threads can also help achieve better throughput on single-processor systems. If a program is single-threaded, the processor remains idle while it waits for a synchronous I/O operation to complete. In a multithreaded program, another thread can still run while the first thread is waiting for the I/O to complete, allowing the application to still make progress during the blocking I/O. (This is like reading the newspaper while waiting for the water to boil, rather than waiting for the water to boil before starting to read.)","title":"Exploiting multiple processors"},{"location":"java/concurrency/introduction/c01-intro/#simplicity-of-modeling","text":"It is often easier to manage your time when you have only one type of task to perform (fix these twelve bugs) than when you have several (fix the bugs, interview replacement candidates for the system administrator, complete your team\u2019s performance evaluations, and create the slides for your presentation next week). When you have only one type of task to do, you can start at the top of the pile and keep working until the pile is exhausted (or you are); you don\u2019t have to spend any mental energy figuring out what to work on next. On the other hand, managing multiple priorities and deadlines and switching from task to task usually carries some overhead. The same is true for software: a program that processes one type of task sequentially is simpler to write, less error-prone, and easier to test than one managing multiple different types of tasks at once. Assigning a thread to each type of task or to each element in a simulation affords the illusion of sequentiality and insulates domain logic from the details of scheduling, interleaved operations, asynchronous I/O, and resource waits. A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points.","title":"Simplicity of modeling"},{"location":"java/concurrency/introduction/c01-intro/#simplified-handling-of-asynchronous-events","text":"A server application that accepts socket connections from multiple remote clients may be easier to develop when each connection is allocated its own thread and allowed to use synchronous I/O. If an application goes to read from a socket when no data is available, read blocks until some data is available. In a single-threaded application, this means that not only does processing the corresponding request stall, but processing of all requests stalls while the single thread is blocked. To avoid this problem, single-threaded server applications are forced to use nonblocking I/O, which is far more complicated and error-prone than synchronous I/O. However, if each request has its own thread, then blocking does not affect the processing of other requests. Historically, operating systems placed relatively low limits on the number of threads that a process could create, as few as several hundred (or even less). As a result, operating systems developed efficient facilities for multiplexed I/O, such as the Unix select and poll system calls, and to access these facilities, the Java class libraries acquired a set of packages (java.nio) for nonblocking I/O. However, operating system support for larger numbers of threads has improved significantly, making the thread-per-client model practical even for large numbers of clients on some platforms.","title":"Simplified handling of asynchronous events"},{"location":"java/concurrency/introduction/c01-intro/#risks-of-threads","text":"Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising.","title":"Risks of threads"},{"location":"java/concurrency/introduction/c01-intro/#safety-hazards","text":"Because threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. This is a tremendous convenience, because it makes data sharing much easier than would other inter-thread communications mechanisms. But it is also a significant risk: threads can be confused by having data change unexpectedly. Allowing multiple threads to access and modify the same variables introduces an element of nonsequentiality into an otherwise sequential programming model, which can be confusing and difficult to reason about. For a multithreaded program\u2019s behavior to be predictable, access to shared variables must be properly coordinated so that threads do not interfere with one another.","title":"Safety hazards"},{"location":"java/concurrency/introduction/c01-intro/#liveness-hazards","text":"A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. One form of liveness failure that can occur in sequential programs is an inadvertent infinite loop, where the code that follows the loop never gets executed. The use of threads introduces additional liveness risks. For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever. Like most concurrency bugs, bugs that cause liveness failures can be elusive because they depend on the relative timing of events in different threads, and therefore do not always manifest themselves in development or testing.","title":"Liveness hazards"},{"location":"java/concurrency/introduction/c01-intro/#performance-hazards","text":"Performance issues subsume a broad range of problems, including poor service time, responsiveness, throughput, resource consumption, or scalability. Just as with safety and liveness, multithreaded programs are subject to all the performance hazards of single-threaded programs, and to others as well that are introduced by the use of threads. In well designed concurrent applications the use of threads is a net performance gain, but threads nevertheless carry some degree of runtime overhead. Context switches\u2014when the scheduler suspends the active thread temporarily so another thread can run\u2014are more frequent in applications with many threads, and have significant costs: saving and restoring execution context, loss of locality, and CPU time spent scheduling threads instead of running them. When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus.","title":"Performance hazards"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/","text":"6. Task Execution Most concurrent applications are organized around the execution of tasks: abstract, discrete units of work. Dividing the work of an application into tasks simplifies program organization, facilitates error recovery by providing natural transaction boundaries, and promotes concurrency by providing a natural structure for parallelizing work. Executing tasks in threads The first step in organizing a program around task execution is identifying sensible task boundaries. Ideally, tasks are independent activities: work that doesn\u2019t depend on the state, result, or side effects of other tasks. Applications should exhibit good throughput, good responsiveness and a graceful degradation as they become overloaded, rather than simply falling over under heavy load. Choosing good task boundaries, coupled with a sensible task execution policy, can help achieve these goals. Executing tasks sequentially There are a number of possible policies for scheduling tasks within an application, some of which exploit the potential for concurrency better than others. The simplest is to execute tasks sequentially in a single thread. In some situations, sequential processing may offer a simplicity or safety advantage; most GUI frameworks process tasks sequentially using a single thread. In server applications, sequential processing rarely provides either good throughput or good responsiveness. public class SingleThreadWebServer { public static void main ( String [] args ) throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while ( true ) { Socket connection = socket . accept (); handleRequest ( connection ); } } private static void handleRequest ( Socket connection ) { // request-handling logic here } } Explicitly creating threads for tasks A more responsive approach is to create a new thread for servicing each request, as shown in ThreadPerTaskWebServer public class ThreadPerTaskWebServer { public static void main ( String [] args ) throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while ( true ) { final Socket connection = socket . accept (); Runnable task = new Runnable () { public void run () { handleRequest ( connection ); } }; new Thread ( task ). start (); } } private static void handleRequest ( Socket connection ) { // request-handling logic here } } ThreadPerTaskWebServer is similar in structure to the single-threaded version\u2014the main thread still alternates between accepting an incoming connection and dispatching the request. The difference is that for each connection, the main loop creates a new thread to process the request instead of processing it within the main thread. This has three main consequences: Task processing is offloaded from the main thread, enabling the main loop to resume waiting for the next incoming connection more quickly. This enables new connections to be accepted before previous requests complete, improving responsiveness. Tasks can be processed in parallel, enabling multiple requests to be serviced simultaneously. This may improve throughput if there are multiple processors, or if tasks need to block for any reason such as I/O completion, lock acquisition, or resource availability. Task-handling code must be thread-safe, because it may be invoked concurrently for multiple tasks. Under light to moderate load, the thread-per-task approach is an improvement over sequential execution. As long as the request arrival rate does not exceed the server\u2019s capacity to handle requests, this approach offers better responsiveness and throughput. Disadvantages of unbounded thread creation For production use, however, the thread-per-task approach has some practical drawbacks, especially when a large number of threads may be created: Thread lifecycle overhead. Thread creation and teardown are not free. The actual overhead varies across platforms, but thread creation takes time, introducing latency into request processing, and requires some processing activity by the JVM and OS. If requests are frequent and lightweight, as in most server applications, creating a new thread for each request can consume significant computing resources. Resource consumption. Active threads consume system resources, especially memory. When there are more runnable threads than available processors, threads sit idle. Having many idle threads can tie up a lot of memory, putting pressure on the garbage collector, and having many threads competing for the CPUs can impose other performance costs as well. If you have enough threads to keep all the CPUs busy, creating more threads won\u2019t help and may even hurt. Stability. There is a limit on how many threads can be created. The limit varies by platform and is affected by factors including JVM invocation parameters, the requested stack size in the Thread constructor, and limits on threads placed by the underlying operating system. When you hit this limit, the most likely result is an OutOfMemoryError. Trying to recover from such an error is very risky; it is far easier to structure your program to avoid hitting this limit. The way to stay out of danger is to place some bound on how many threads your application creates, and to test your application thoroughly to ensure that, even when this bound is reached, it does not run out of resources. Like other concurrency hazards, unbounded thread creation may appear to work just fine during prototyping and development, with problems surfacing only when the application is deployed and under heavy load. So a malicious user, or enough ordinary users, can make your web server crash if the traffic load ever reaches a certain threshold. For a server application that is supposed to provide high availability and graceful degradation under load, this is a serious failing. The Executor framework Tasks are logical units of work, and threads are a mechanism by which tasks can run asynchronously. We\u2019ve examined two policies for executing tasks using threads\u2014execute tasks sequentially in a single thread, and execute each task in its own thread. Both have serious limitations: the sequential approach suffers from poor responsiveness and throughput, and the thread-per-task approach suffers from poor resource management. Thread pools offer the same benefit for thread management, and java.util.concurrent provides a flexible thread pool implementation as part of the Executor framework. The primary abstraction for task execution in the Java class libraries is not Thread , but Executor . public interface Executor { void execute ( Runnable command ); } Executor may be a simple interface, but it forms the basis for a flexible and powerful framework for asynchronous task execution that supports a wide variety of task execution policies. It provides a standard means of decoupling task submission from task execution , describing tasks with Runnable . The Executor implementations also provide lifecycle support and hooks for adding statistics gathering, application management, and monitoring. Executor is based on the producer-consumer pattern, where activities that submit tasks are the producers (producing units of work to be done) and the threads that execute tasks are the consumers (consuming those units of work). Using an Executor is usually the easiest path to implementing a producer-consumer design in your application. Example: web server using Executor In TaskExecutionWebServer , submission of the request-handling task is decoupled from its execution using an Executor , and its behavior can be changed merely by substituting a different Executor implementation. public class TaskExecutionWebServer { private static final int NTHREADS = 100 ; private static final Executor exec = Executors . newFixedThreadPool ( NTHREADS ); public static void main ( String [] args ) throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while ( true ) { final Socket connection = socket . accept (); Runnable task = new Runnable () { public void run () { handleRequest ( connection ); } }; exec . execute ( task ); } } private static void handleRequest ( Socket connection ) { // request-handling logic here } } Execution policies The value of decoupling submission from execution is that it lets you easily specify, and subsequently change without great difficulty, the execution policy for a given class of tasks. An execution policy specifies the \u201cwhat, where, when, and how\u201d of task execution , including: In what thread will tasks be executed? In what order should tasks be executed (FIFO, LIFO, priority order)? How many tasks may execute concurrently? How many tasks may be queued pending execution? If a task has to be rejected because the system is overloaded, which task should be selected as the victim, and how should the application be notified? What actions should be taken before or after executing a task? Execution policies are a resource management tool, and the optimal policy depends on the available computing resources and your quality-of-service requirements. By limiting the number of concurrent tasks, you can ensure that the application does not fail due to resource exhaustion or suffer performance problems due to contention for scarce resources. Separating the specification of execution policy from task submission makes it practical to select an execution policy at deployment time that is matched to the available hardware. Whenever you see code of the form: new Thread(runnable).start() and you think you might at some point want a more flexible execution policy, seriously consider replacing it with the use of an Executor. Thread pools A thread pool, as its name suggests, manages a homogeneous pool of worker threads. A thread pool is tightly bound to a work queue holding tasks waiting to be executed. Worker threads have a simple life: request the next task from the work queue, execute it, and go back to waiting for another task. Reusing an existing thread instead of creating a new one amortizes thread creation and teardown costs over multiple requests. As an added bonus, since the worker thread often already exists at the time the request arrives, the latency associated with thread creation does not delay task execution, thus improving responsiveness. By properly tuning the size of the thread pool, you can have enough threads to keep the processors busy while not having so many that your application runs out of memory or thrashes due to competition among threads for resources. The class library provides a flexible thread pool implementation along with some useful predefined configurations. You can create a thread pool by calling one of the static factory methods in Executors: newFixedThreadPool A fixed-size thread pool creates threads as tasks are sub- mitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). newCachedThreadPool A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. newSingleThreadExecutor A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). newScheduledThreadPool A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer . The newFixedThreadPool and newCachedThreadPool factories return instances of the general-purpose ThreadPoolExecutor , which can also be used directly to construct more specialized executors. Switching from a thread-per-task policy to a pool-based policy has a big effect on application stability: the web server will no longer fail under heavy load. It also degrades more gracefully, since it does not create thousands of threads that compete for limited CPU and memory resources. And using an Executor opens the door to all sorts of additional opportunities for tuning, management, monitoring, logging, error reporting, and other possibilities that would have been far more difficult to add without a task execution framework. Executor lifecycle An Executor implementation is likely to create threads for processing tasks. But the JVM can\u2019t exit until all the (nondaemon) threads have terminated, so failing to shut down an Executor could prevent the JVM from exiting. Because an Executor processes tasks asynchronously, at any given time the state of previously submitted tasks is not immediately obvious. Some may have completed, some may be currently running, and others may be queued awaiting execution. In shutting down an application, there is a spectrum from graceful shutdown (finish what you\u2019ve started but don\u2019t accept any new work) to abrupt shutdown (turn off the power to the machine room), and various points in between. Since Executors provide a service to applications, they should be able to be shut down as well, both gracefully and abruptly, and feed back information to the application about the status of tasks that were affected by the shutdown. To address the issue of execution service lifecycle, the ExecutorService interface extends Executor , adding a number of methods for lifecycle management (as well as some convenience methods for task submission). public interface ExecutorService extends Executor { void shutdown (); List < Runnable > shutdownNow (); boolean isShutdown (); boolean isTerminated (); boolean awaitTermination ( long timeout , TimeUnit unit ) throws InterruptedException ; // ... additional convenience methods for task submission } The lifecycle implied by ExecutorService has three states\u2014running, shutting down, and terminated. ExecutorServices are initially created in the running state. The shutdown method initiates a graceful shutdown: no new tasks are accepted but previously submitted tasks are allowed to complete\u2014including those that have not yet begun execution. The shutdownNow method initiates an abrupt shutdown: it attempts to cancel outstanding tasks and does not start any tasks that are queued but not begun. Tasks submitted to an ExecutorService after it has been shut down are handled by the rejected execution handler, which might silently discard the task or might cause execute to throw the unchecked RejectedExecutionException . Once all tasks have completed, the ExecutorService transitions to the terminated state. You can wait for an ExecutorService to reach the terminated state with awaitTermination , or poll for whether it has yet terminated with isTerminated . It is common to follow shutdown immediately by awaitTermination , creating the effect of synchronously shutting down the ExecutorService . public class LifecycleWebServer { private final ExecutorService exec = ...; public void start () throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while (! exec . isShutdown ()) { try { final Socket conn = socket . accept (); exec . execute ( new Runnable () { public void run () { handleRequest ( conn ); } }); } catch ( RejectedExecutionException e ) { if (! exec . isShutdown ()) log ( \"task submission rejected\" , e ); } } } public void stop () { exec . shutdown (); } void handleRequest ( Socket connection ) { Request req = readRequest ( connection ); if ( isShutdownRequest ( req )) stop (); else dispatchRequest ( req ); } } Delayed and periodic tasks The Timer facility manages the execution of deferred (\u201crun this task in 100 ms\u201d) and periodic (\u201crun this task every 10 ms\u201d) tasks. However, Timer has some drawbacks, and ScheduledThreadPoolExecutor should be thought of as its replacement. A Timer creates only a single thread for executing timer tasks. If a timer task takes too long to run, the timing accuracy of other TimerTasks can suffer. If a recurring TimerTask is scheduled to run every 10 ms and another TimerTask takes 40 ms to run, the recurring task either (depending on whether it was scheduled at fixed rate or fixed delay) gets called four times in rapid succession after the long-running task completes, or \u201cmisses\u201d four invocations completely. Scheduled thread pools address this limitation by letting you provide multiple threads for executing deferred and periodic tasks. Another problem with Timer is that it behaves poorly if a TimerTask throws an unchecked exception. The Timer thread doesn\u2019t catch the exception, so an unchecked exception thrown from a TimerTask terminates the timer thread. Timer also doesn\u2019t resurrect the thread in this situation; instead, it erroneously assumes the entire Timer was cancelled. Finding exploitable parallelism The Executor framework makes it easy to specify an execution policy, but in order to use an Executor , you have to be able to describe your task as a Runnable . In most server applications, there is an obvious task boundary: a single client request. But sometimes good task boundaries are not quite so obvious, as in many desktop applications. Example: sequential page renderer public abstract class SingleThreadRenderer { void renderPage ( CharSequence source ) { renderText ( source ); List < ImageData > imageData = new ArrayList < ImageData >(); for ( ImageInfo imageInfo : scanForImageInfo ( source )) imageData . add ( imageInfo . downloadImage ()); for ( ImageData data : imageData ) renderImage ( data ); } } Result-bearing tasks: Callable and Future The Executor framework uses Runnable as its basic task representation. Runnable is a fairly limiting abstraction; run cannot return a value or throw checked exceptions, although it can have side effects such as writing to a log file or placing a result in a shared data structure. Many tasks are effectively deferred computations\u2014executing a database query, fetching a resource over the network, or computing a complicated function. For these types of tasks, Callable is a better abstraction: it expects that the main entry point, call, will return a value and anticipates that it might throw an exception. Executors includes several utility methods for wrapping other types of tasks, including Runnable and java.security.PrivilegedAction , with a Callable . The lifecycle of a task executed by an Executor has four phases: created, submitted, started, and completed. Since tasks can take a long time to run, we also want to be able to cancel a task. In the Executor framework, tasks that have been submitted but not yet started can always be cancelled, and tasks that have started can sometimes be cancelled if they are responsive to interruption. Cancelling a task that has already completed has no effect. Future represents the lifecycle of a task and provides methods to test whether the task has completed or been cancelled, retrieve its result, and cancel the task. Implicit in the specification of Future is that task lifecycle can only move forwards, not backwards\u2014just like the ExecutorService lifecycle. Once a task is completed, it stays in that state forever. The behavior of get varies depending on the task state (not yet started, running, completed). It returns immediately or throws an Exception if the task has already completed, but if not it blocks until the task completes. If the task completes by throwing an exception, get rethrows it wrapped in an ExecutionException ; if it was cancelled, get throws CancellationException . If get throws ExecutionException , the underlying exception can be retrieved with getCause . There are several ways to create a Future to describe a task. The submit methods in ExecutorService all return a Future , so that you can submit a Runnable or a Callable to an executor and get back a Future that can be used to retrieve the result or cancel the task. You can also explicitly instantiate a FutureTask for a given Runnable or Callable . (Because FutureTask implements Runnable , it can be submitted to an Executor for execution or executed directly by calling its run method.) Submitting a Runnable or Callable to an Executor constitutes a safe publication of the Runnable or Callable from the submitting thread to the thread that will eventually execute the task. Similarly, setting the result value for a Future constitutes a safe publication of the result from the thread in which it was computed to any thread that retrieves it via get . Example: page renderer with Future FutureRenderer allows the text to be rendered concurrently with downloading the image data. When all the images are downloaded, they are rendered onto the page. This is an improvement in that the user sees a result quickly and it exploits some parallelism, but we can do considerably better. There is no need for users to wait for all the images to be downloaded; they would probably prefer to see individual images drawn as they become available. The state-dependent nature of get means that the caller need not be aware of the state of the task, and the safe publication properties of task submission and result retrieval make this approach thread-safe. The exception handling code surrounding Future.get deals with two possible problems: that the task encountered an Exception, or the thread calling get was interrupted before the results were available. public abstract class FutureRenderer { private final ExecutorService executor = Executors . newCachedThreadPool (); void renderPage ( CharSequence source ) { final List < ImageInfo > imageInfos = scanForImageInfo ( source ); Callable < List < ImageData >> task = new Callable < List < ImageData >>() { public List < ImageData > call () { List < ImageData > result = new ArrayList < ImageData >(); for ( ImageInfo imageInfo : imageInfos ) result . add ( imageInfo . downloadImage ()); return result ; } }; Future < List < ImageData >> future = executor . submit ( task ); renderText ( source ); try { List < ImageData > imageData = future . get (); for ( ImageData data : imageData ) renderImage ( data ); } catch ( InterruptedException e ) { // Re-assert the thread's interrupted status Thread . currentThread (). interrupt (); // We don't need the result, so cancel the task too future . cancel ( true ); } catch ( ExecutionException e ) { throw launderThrowable ( e . getCause ()); } } } Limitations of parallelizing heterogeneous tasks Two people can divide the work of cleaning the dinner dishes fairly effectively: one person washes while the other dries. However, assigning a different type of task to each worker does not scale well; if several more people show up, it is not obvious how they can help without getting in the way or significantly restructuring the division of labor. Without finding finer-grained parallelism among similar tasks, this approach will yield diminishing returns. A further problem with dividing heterogeneous tasks among multiple workers is that the tasks may have disparate sizes. If you divide tasks A and B between two workers but A takes ten times as long as B, you\u2019ve only speeded up the total process by 9%. Finally, dividing a task among multiple workers always involves some amount of coordination overhead; for the division to be worthwhile, this overhead must be more than compensated by productivity improvements due to parallelism. The real performance payoff of dividing a program\u2019s workload into tasks comes when there are a large number of independent, homogeneous tasks that can be processed concurrently. CompletionService : Executor meets BlockingQueue If you have a batch of computations to submit to an Executor and you want to retrieve their results as they become available, you could retain the Future associated with each task and repeatedly poll for completion by calling get with a timeout of zero. This is possible, but tedious. Fortunately there is a better way: a completion service. CompletionService combines the functionality of an Executor and a BlockingQueue . You can submit Callable tasks to it for execution and use the queue-like methods take and poll to retrieve completed results, packaged as Futures , as they become available. ExecutorCompletionService implements CompletionService , delegating the computation to an Executor. The implementation of ExecutorCompletionService is quite straightforward. The constructor creates a BlockingQueue to hold the completed results. FutureTask has a done method that is called when the computation completes. When a task is submitted, it is wrapped with a QueueingFuture , a subclass of FutureTask that overrides done to place the result on the BlockingQueue , as shown below: private class QueueingFuture < V > extends FutureTask < V > { QueueingFuture ( Callable < V > c ) { super ( c ); } QueueingFuture ( Runnable t , V r ) { super ( t , r ); } protected void done () { completionQueue . add ( this ); } } ( QueueingFuture class used by ExecutorCompletionService ) Example: page renderer with CompletionService We can use a CompletionService to improve the performance of the page renderer in two ways: shorter total runtime and improved responsiveness. We can create a separate task for downloading each image and execute them in a thread pool, turning the sequential download into a parallel one: this reduces the amount of time to download all the images. And by fetching results from the CompletionService and rendering each image as soon as it is available, we can give the user a more dynamic and responsive user interface. public abstract class Renderer { private final ExecutorService executor ; Renderer ( ExecutorService executor ) { this . executor = executor ; } void renderPage ( CharSequence source ) { final List < ImageInfo > info = scanForImageInfo ( source ); CompletionService < ImageData > completionService = new ExecutorCompletionService < ImageData >( executor ); for ( final ImageInfo imageInfo : info ) completionService . submit ( new Callable < ImageData >() { public ImageData call () { return imageInfo . downloadImage (); } }); renderText ( source ); try { for ( int t = 0 , n = info . size (); t < n ; t ++) { Future < ImageData > f = completionService . take (); ImageData imageData = f . get (); renderImage ( imageData ); } } catch ( InterruptedException e ) { Thread . currentThread (). interrupt (); } catch ( ExecutionException e ) { throw launderThrowable ( e . getCause ()); } } } Multiple ExecutorCompletionServices can share a single Executor, so it is perfectly sensible to create an ExecutorCompletionService that is private to a particular computation while sharing a common Executor. When used in this way, a CompletionService acts as a handle for a batch of computations in much the same way that a Future acts as a handle for a single computation. By remembering how many tasks were submitted to the CompletionService and counting how many completed results are retrieved, you can know when all the results for a given batch have been retrieved, even if you use a shared Executor. Placing time limits on tasks Sometimes, if an activity does not complete within a certain amount of time, the result is no longer needed and the activity can be abandoned. For example, a web application may fetch its advertisements from an external ad server, but if the ad is not available within two seconds, it instead displays a default advertisement so that ad unavailability does not undermine the site\u2019s responsiveness requirements. Similarly, a portal site may fetch data in parallel from multiple data sources, but may be willing to wait only a certain amount of time for data to be available before rendering the page without it. The primary challenge in executing tasks within a time budget is making sure that you don\u2019t wait longer than the time budget to get an answer or find out that one is not forthcoming. The timed version of Future.get supports this requirement: it returns as soon as the result is ready, but throws TimeoutException if the result is not ready within the timeout period. A secondary problem when using timed tasks is to stop them when they run out of time, so they do not waste computing resources by continuing to compute a result that will not be used. This can be accomplished by having the task strictly manage its own time budget and abort if it runs out of time, or by cancelling the task if the timeout expires. Again, Future can help; if a timed get completes with a TimeoutException , you can cancel the task through the Future . If the task is written to be cancellable, it can be terminated early so as not to consume excessive resources. Listing below shows a typical application of a timed Future.get . It generates a composite web page that contains the requested content plus an advertisement fetched from an ad server. It submits the ad-fetching task to an executor, computes the rest of the page content, and then waits for the ad until its time budget runs out. If the get times out 1 , it cancels 2 the ad-fetching task and uses a default advertisement instead. Page renderPageWithAd () throws InterruptedException { long endNanos = System . nanoTime () + TIME_BUDGET ; Future < Ad > f = exec . submit ( new FetchAdTask ()); // Render the page while waiting for the ad Page page = renderPageBody (); Ad ad ; try { // Only wait for the remaining time budget long timeLeft = endNanos - System . nanoTime (); ad = f . get ( timeLeft , NANOSECONDS ); } catch ( ExecutionException e ) { ad = DEFAULT_AD ; } catch ( TimeoutException e ) { ad = DEFAULT_AD ; f . cancel ( true ); } page . setAd ( ad ); return page ; } Example: a travel reservations portal Consider a travel reservation portal: the user enters travel dates and requirements and the portal fetches and displays bids from a number of airlines, hotels or car rental companies. Depending on the company, fetching a bid might involve invoking a web service, consulting a database, performing an EDI transaction, or some other mechanism. Rather than have the response time for the page be driven by the slowest response, it may be preferable to present only the information available within a given time budget. For providers that do not respond in time, the page could either omit them completely or display a placeholder such as \u201cDid not hear from Air Java in time.\u201d Fetching a bid from one company is independent of fetching bids from another, so fetching a single bid is a sensible task boundary that allows bid retrieval to proceed concurrently. It would be easy enough to create n tasks, submit them to a thread pool, retain the Futures, and use a timed get to fetch each result sequentially via its Future, but there is an even easier way\u2014 invokeAll . Below code uses the timed version of invokeAll to submit multiple tasks to an ExecutorService and retrieve the results. The invokeAll method takes a collection of tasks and returns a collection of Futures. The two collections have identical structures; invokeAll adds the Futures to the returned collection in the order imposed by the task collection\u2019s iterator, thus allowing the caller to associate a Future with the Callable it represents. The timed version of invokeAll will return when all the tasks have completed, the calling thread is interrupted, or the timeout expires. Any tasks that are not complete when the timeout expires are cancelled. On return from invokeAll , each task will have either completed normally or been cancelled; the client code can call get or isCancelled to find out which. public class TimeBudget { private static ExecutorService exec = Executors . newCachedThreadPool (); public List < TravelQuote > getRankedTravelQuotes ( TravelInfo travelInfo , Set < TravelCompany > companies , Comparator < TravelQuote > ranking , long time , TimeUnit unit ) throws InterruptedException { List < QuoteTask > tasks = new ArrayList < QuoteTask >(); for ( TravelCompany company : companies ) tasks . add ( new QuoteTask ( company , travelInfo )); List < Future < TravelQuote >> futures = exec . invokeAll ( tasks , time , unit ); List < TravelQuote > quotes = new ArrayList < TravelQuote >( tasks . size ()); Iterator < QuoteTask > taskIter = tasks . iterator (); for ( Future < TravelQuote > f : futures ) { QuoteTask task = taskIter . next (); try { quotes . add ( f . get ()); } catch ( ExecutionException e ) { quotes . add ( task . getFailureQuote ( e . getCause ())); } catch ( CancellationException e ) { quotes . add ( task . getTimeoutQuote ( e )); } } Collections . sort ( quotes , ranking ); return quotes ; } } class QuoteTask implements Callable < TravelQuote > { private final TravelCompany company ; private final TravelInfo travelInfo ; // ... public TravelQuote call () throws Exception { return company . solicitQuote ( travelInfo ); } } Summary Structuring applications around the execution of tasks can simplify development and facilitate concurrency. The Executor framework permits you to decouple task submission from execution policy and supports a rich variety of execution policies; whenever you find yourself creating threads to perform tasks, consider using an Executor instead. To maximize the benefit of decomposing an application into tasks, you must identify sensible task boundaries. In some applications, the obvious task boundaries work well, whereas in others some analysis may be required to uncover finer-grained exploitable parallelism. The timeout passed to get is computed by subtracting the current time from the deadline; this may in fact yield a negative number, but all the timed methods in java.util.concurrent treat negative timeouts as zero, so no extra code is needed to deal with this case.. \u21a9 The true parameter to Future.cancel means that the task thread can be interrupted if the task is currently running. \u21a9","title":"Task Execution"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#6-task-execution","text":"Most concurrent applications are organized around the execution of tasks: abstract, discrete units of work. Dividing the work of an application into tasks simplifies program organization, facilitates error recovery by providing natural transaction boundaries, and promotes concurrency by providing a natural structure for parallelizing work.","title":"6. Task Execution"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#executing-tasks-in-threads","text":"The first step in organizing a program around task execution is identifying sensible task boundaries. Ideally, tasks are independent activities: work that doesn\u2019t depend on the state, result, or side effects of other tasks. Applications should exhibit good throughput, good responsiveness and a graceful degradation as they become overloaded, rather than simply falling over under heavy load. Choosing good task boundaries, coupled with a sensible task execution policy, can help achieve these goals.","title":"Executing tasks in threads"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#executing-tasks-sequentially","text":"There are a number of possible policies for scheduling tasks within an application, some of which exploit the potential for concurrency better than others. The simplest is to execute tasks sequentially in a single thread. In some situations, sequential processing may offer a simplicity or safety advantage; most GUI frameworks process tasks sequentially using a single thread. In server applications, sequential processing rarely provides either good throughput or good responsiveness. public class SingleThreadWebServer { public static void main ( String [] args ) throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while ( true ) { Socket connection = socket . accept (); handleRequest ( connection ); } } private static void handleRequest ( Socket connection ) { // request-handling logic here } }","title":"Executing tasks sequentially"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#explicitly-creating-threads-for-tasks","text":"A more responsive approach is to create a new thread for servicing each request, as shown in ThreadPerTaskWebServer public class ThreadPerTaskWebServer { public static void main ( String [] args ) throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while ( true ) { final Socket connection = socket . accept (); Runnable task = new Runnable () { public void run () { handleRequest ( connection ); } }; new Thread ( task ). start (); } } private static void handleRequest ( Socket connection ) { // request-handling logic here } } ThreadPerTaskWebServer is similar in structure to the single-threaded version\u2014the main thread still alternates between accepting an incoming connection and dispatching the request. The difference is that for each connection, the main loop creates a new thread to process the request instead of processing it within the main thread. This has three main consequences: Task processing is offloaded from the main thread, enabling the main loop to resume waiting for the next incoming connection more quickly. This enables new connections to be accepted before previous requests complete, improving responsiveness. Tasks can be processed in parallel, enabling multiple requests to be serviced simultaneously. This may improve throughput if there are multiple processors, or if tasks need to block for any reason such as I/O completion, lock acquisition, or resource availability. Task-handling code must be thread-safe, because it may be invoked concurrently for multiple tasks. Under light to moderate load, the thread-per-task approach is an improvement over sequential execution. As long as the request arrival rate does not exceed the server\u2019s capacity to handle requests, this approach offers better responsiveness and throughput.","title":"Explicitly creating threads for tasks"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#disadvantages-of-unbounded-thread-creation","text":"For production use, however, the thread-per-task approach has some practical drawbacks, especially when a large number of threads may be created: Thread lifecycle overhead. Thread creation and teardown are not free. The actual overhead varies across platforms, but thread creation takes time, introducing latency into request processing, and requires some processing activity by the JVM and OS. If requests are frequent and lightweight, as in most server applications, creating a new thread for each request can consume significant computing resources. Resource consumption. Active threads consume system resources, especially memory. When there are more runnable threads than available processors, threads sit idle. Having many idle threads can tie up a lot of memory, putting pressure on the garbage collector, and having many threads competing for the CPUs can impose other performance costs as well. If you have enough threads to keep all the CPUs busy, creating more threads won\u2019t help and may even hurt. Stability. There is a limit on how many threads can be created. The limit varies by platform and is affected by factors including JVM invocation parameters, the requested stack size in the Thread constructor, and limits on threads placed by the underlying operating system. When you hit this limit, the most likely result is an OutOfMemoryError. Trying to recover from such an error is very risky; it is far easier to structure your program to avoid hitting this limit. The way to stay out of danger is to place some bound on how many threads your application creates, and to test your application thoroughly to ensure that, even when this bound is reached, it does not run out of resources. Like other concurrency hazards, unbounded thread creation may appear to work just fine during prototyping and development, with problems surfacing only when the application is deployed and under heavy load. So a malicious user, or enough ordinary users, can make your web server crash if the traffic load ever reaches a certain threshold. For a server application that is supposed to provide high availability and graceful degradation under load, this is a serious failing.","title":"Disadvantages of unbounded thread creation"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#the-executor-framework","text":"Tasks are logical units of work, and threads are a mechanism by which tasks can run asynchronously. We\u2019ve examined two policies for executing tasks using threads\u2014execute tasks sequentially in a single thread, and execute each task in its own thread. Both have serious limitations: the sequential approach suffers from poor responsiveness and throughput, and the thread-per-task approach suffers from poor resource management. Thread pools offer the same benefit for thread management, and java.util.concurrent provides a flexible thread pool implementation as part of the Executor framework. The primary abstraction for task execution in the Java class libraries is not Thread , but Executor . public interface Executor { void execute ( Runnable command ); } Executor may be a simple interface, but it forms the basis for a flexible and powerful framework for asynchronous task execution that supports a wide variety of task execution policies. It provides a standard means of decoupling task submission from task execution , describing tasks with Runnable . The Executor implementations also provide lifecycle support and hooks for adding statistics gathering, application management, and monitoring. Executor is based on the producer-consumer pattern, where activities that submit tasks are the producers (producing units of work to be done) and the threads that execute tasks are the consumers (consuming those units of work). Using an Executor is usually the easiest path to implementing a producer-consumer design in your application.","title":"The Executor framework"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#example-web-server-using-executor","text":"In TaskExecutionWebServer , submission of the request-handling task is decoupled from its execution using an Executor , and its behavior can be changed merely by substituting a different Executor implementation. public class TaskExecutionWebServer { private static final int NTHREADS = 100 ; private static final Executor exec = Executors . newFixedThreadPool ( NTHREADS ); public static void main ( String [] args ) throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while ( true ) { final Socket connection = socket . accept (); Runnable task = new Runnable () { public void run () { handleRequest ( connection ); } }; exec . execute ( task ); } } private static void handleRequest ( Socket connection ) { // request-handling logic here } }","title":"Example: web server using Executor"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#execution-policies","text":"The value of decoupling submission from execution is that it lets you easily specify, and subsequently change without great difficulty, the execution policy for a given class of tasks. An execution policy specifies the \u201cwhat, where, when, and how\u201d of task execution , including: In what thread will tasks be executed? In what order should tasks be executed (FIFO, LIFO, priority order)? How many tasks may execute concurrently? How many tasks may be queued pending execution? If a task has to be rejected because the system is overloaded, which task should be selected as the victim, and how should the application be notified? What actions should be taken before or after executing a task? Execution policies are a resource management tool, and the optimal policy depends on the available computing resources and your quality-of-service requirements. By limiting the number of concurrent tasks, you can ensure that the application does not fail due to resource exhaustion or suffer performance problems due to contention for scarce resources. Separating the specification of execution policy from task submission makes it practical to select an execution policy at deployment time that is matched to the available hardware. Whenever you see code of the form: new Thread(runnable).start() and you think you might at some point want a more flexible execution policy, seriously consider replacing it with the use of an Executor.","title":"Execution policies"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#thread-pools","text":"A thread pool, as its name suggests, manages a homogeneous pool of worker threads. A thread pool is tightly bound to a work queue holding tasks waiting to be executed. Worker threads have a simple life: request the next task from the work queue, execute it, and go back to waiting for another task. Reusing an existing thread instead of creating a new one amortizes thread creation and teardown costs over multiple requests. As an added bonus, since the worker thread often already exists at the time the request arrives, the latency associated with thread creation does not delay task execution, thus improving responsiveness. By properly tuning the size of the thread pool, you can have enough threads to keep the processors busy while not having so many that your application runs out of memory or thrashes due to competition among threads for resources. The class library provides a flexible thread pool implementation along with some useful predefined configurations. You can create a thread pool by calling one of the static factory methods in Executors: newFixedThreadPool A fixed-size thread pool creates threads as tasks are sub- mitted, up to the maximum pool size, and then attempts to keep the pool size constant (adding new threads if a thread dies due to an unexpected Exception ). newCachedThreadPool A cached thread pool has more flexibility to reap idle threads when the current size of the pool exceeds the demand for processing, and to add new threads when demand increases, but places no bounds on the size of the pool. newSingleThreadExecutor A single-threaded executor creates a single worker thread to process tasks, replacing it if it dies unexpectedly. Tasks are guaranteed to be processed sequentially according to the order imposed by the task queue (FIFO, LIFO, priority order). newScheduledThreadPool A fixed-size thread pool that supports delayed and periodic task execution, similar to Timer . The newFixedThreadPool and newCachedThreadPool factories return instances of the general-purpose ThreadPoolExecutor , which can also be used directly to construct more specialized executors. Switching from a thread-per-task policy to a pool-based policy has a big effect on application stability: the web server will no longer fail under heavy load. It also degrades more gracefully, since it does not create thousands of threads that compete for limited CPU and memory resources. And using an Executor opens the door to all sorts of additional opportunities for tuning, management, monitoring, logging, error reporting, and other possibilities that would have been far more difficult to add without a task execution framework.","title":"Thread pools"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#executor-lifecycle","text":"An Executor implementation is likely to create threads for processing tasks. But the JVM can\u2019t exit until all the (nondaemon) threads have terminated, so failing to shut down an Executor could prevent the JVM from exiting. Because an Executor processes tasks asynchronously, at any given time the state of previously submitted tasks is not immediately obvious. Some may have completed, some may be currently running, and others may be queued awaiting execution. In shutting down an application, there is a spectrum from graceful shutdown (finish what you\u2019ve started but don\u2019t accept any new work) to abrupt shutdown (turn off the power to the machine room), and various points in between. Since Executors provide a service to applications, they should be able to be shut down as well, both gracefully and abruptly, and feed back information to the application about the status of tasks that were affected by the shutdown. To address the issue of execution service lifecycle, the ExecutorService interface extends Executor , adding a number of methods for lifecycle management (as well as some convenience methods for task submission). public interface ExecutorService extends Executor { void shutdown (); List < Runnable > shutdownNow (); boolean isShutdown (); boolean isTerminated (); boolean awaitTermination ( long timeout , TimeUnit unit ) throws InterruptedException ; // ... additional convenience methods for task submission } The lifecycle implied by ExecutorService has three states\u2014running, shutting down, and terminated. ExecutorServices are initially created in the running state. The shutdown method initiates a graceful shutdown: no new tasks are accepted but previously submitted tasks are allowed to complete\u2014including those that have not yet begun execution. The shutdownNow method initiates an abrupt shutdown: it attempts to cancel outstanding tasks and does not start any tasks that are queued but not begun. Tasks submitted to an ExecutorService after it has been shut down are handled by the rejected execution handler, which might silently discard the task or might cause execute to throw the unchecked RejectedExecutionException . Once all tasks have completed, the ExecutorService transitions to the terminated state. You can wait for an ExecutorService to reach the terminated state with awaitTermination , or poll for whether it has yet terminated with isTerminated . It is common to follow shutdown immediately by awaitTermination , creating the effect of synchronously shutting down the ExecutorService . public class LifecycleWebServer { private final ExecutorService exec = ...; public void start () throws IOException { ServerSocket socket = new ServerSocket ( 80 ); while (! exec . isShutdown ()) { try { final Socket conn = socket . accept (); exec . execute ( new Runnable () { public void run () { handleRequest ( conn ); } }); } catch ( RejectedExecutionException e ) { if (! exec . isShutdown ()) log ( \"task submission rejected\" , e ); } } } public void stop () { exec . shutdown (); } void handleRequest ( Socket connection ) { Request req = readRequest ( connection ); if ( isShutdownRequest ( req )) stop (); else dispatchRequest ( req ); } }","title":"Executor lifecycle"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#delayed-and-periodic-tasks","text":"The Timer facility manages the execution of deferred (\u201crun this task in 100 ms\u201d) and periodic (\u201crun this task every 10 ms\u201d) tasks. However, Timer has some drawbacks, and ScheduledThreadPoolExecutor should be thought of as its replacement. A Timer creates only a single thread for executing timer tasks. If a timer task takes too long to run, the timing accuracy of other TimerTasks can suffer. If a recurring TimerTask is scheduled to run every 10 ms and another TimerTask takes 40 ms to run, the recurring task either (depending on whether it was scheduled at fixed rate or fixed delay) gets called four times in rapid succession after the long-running task completes, or \u201cmisses\u201d four invocations completely. Scheduled thread pools address this limitation by letting you provide multiple threads for executing deferred and periodic tasks. Another problem with Timer is that it behaves poorly if a TimerTask throws an unchecked exception. The Timer thread doesn\u2019t catch the exception, so an unchecked exception thrown from a TimerTask terminates the timer thread. Timer also doesn\u2019t resurrect the thread in this situation; instead, it erroneously assumes the entire Timer was cancelled.","title":"Delayed and periodic tasks"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#finding-exploitable-parallelism","text":"The Executor framework makes it easy to specify an execution policy, but in order to use an Executor , you have to be able to describe your task as a Runnable . In most server applications, there is an obvious task boundary: a single client request. But sometimes good task boundaries are not quite so obvious, as in many desktop applications.","title":"Finding exploitable parallelism"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#example-sequential-page-renderer","text":"public abstract class SingleThreadRenderer { void renderPage ( CharSequence source ) { renderText ( source ); List < ImageData > imageData = new ArrayList < ImageData >(); for ( ImageInfo imageInfo : scanForImageInfo ( source )) imageData . add ( imageInfo . downloadImage ()); for ( ImageData data : imageData ) renderImage ( data ); } }","title":"Example: sequential page renderer"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#result-bearing-tasks-callable-and-future","text":"The Executor framework uses Runnable as its basic task representation. Runnable is a fairly limiting abstraction; run cannot return a value or throw checked exceptions, although it can have side effects such as writing to a log file or placing a result in a shared data structure. Many tasks are effectively deferred computations\u2014executing a database query, fetching a resource over the network, or computing a complicated function. For these types of tasks, Callable is a better abstraction: it expects that the main entry point, call, will return a value and anticipates that it might throw an exception. Executors includes several utility methods for wrapping other types of tasks, including Runnable and java.security.PrivilegedAction , with a Callable . The lifecycle of a task executed by an Executor has four phases: created, submitted, started, and completed. Since tasks can take a long time to run, we also want to be able to cancel a task. In the Executor framework, tasks that have been submitted but not yet started can always be cancelled, and tasks that have started can sometimes be cancelled if they are responsive to interruption. Cancelling a task that has already completed has no effect. Future represents the lifecycle of a task and provides methods to test whether the task has completed or been cancelled, retrieve its result, and cancel the task. Implicit in the specification of Future is that task lifecycle can only move forwards, not backwards\u2014just like the ExecutorService lifecycle. Once a task is completed, it stays in that state forever. The behavior of get varies depending on the task state (not yet started, running, completed). It returns immediately or throws an Exception if the task has already completed, but if not it blocks until the task completes. If the task completes by throwing an exception, get rethrows it wrapped in an ExecutionException ; if it was cancelled, get throws CancellationException . If get throws ExecutionException , the underlying exception can be retrieved with getCause . There are several ways to create a Future to describe a task. The submit methods in ExecutorService all return a Future , so that you can submit a Runnable or a Callable to an executor and get back a Future that can be used to retrieve the result or cancel the task. You can also explicitly instantiate a FutureTask for a given Runnable or Callable . (Because FutureTask implements Runnable , it can be submitted to an Executor for execution or executed directly by calling its run method.) Submitting a Runnable or Callable to an Executor constitutes a safe publication of the Runnable or Callable from the submitting thread to the thread that will eventually execute the task. Similarly, setting the result value for a Future constitutes a safe publication of the result from the thread in which it was computed to any thread that retrieves it via get .","title":"Result-bearing tasks: Callable and Future"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#example-page-renderer-with-future","text":"FutureRenderer allows the text to be rendered concurrently with downloading the image data. When all the images are downloaded, they are rendered onto the page. This is an improvement in that the user sees a result quickly and it exploits some parallelism, but we can do considerably better. There is no need for users to wait for all the images to be downloaded; they would probably prefer to see individual images drawn as they become available. The state-dependent nature of get means that the caller need not be aware of the state of the task, and the safe publication properties of task submission and result retrieval make this approach thread-safe. The exception handling code surrounding Future.get deals with two possible problems: that the task encountered an Exception, or the thread calling get was interrupted before the results were available. public abstract class FutureRenderer { private final ExecutorService executor = Executors . newCachedThreadPool (); void renderPage ( CharSequence source ) { final List < ImageInfo > imageInfos = scanForImageInfo ( source ); Callable < List < ImageData >> task = new Callable < List < ImageData >>() { public List < ImageData > call () { List < ImageData > result = new ArrayList < ImageData >(); for ( ImageInfo imageInfo : imageInfos ) result . add ( imageInfo . downloadImage ()); return result ; } }; Future < List < ImageData >> future = executor . submit ( task ); renderText ( source ); try { List < ImageData > imageData = future . get (); for ( ImageData data : imageData ) renderImage ( data ); } catch ( InterruptedException e ) { // Re-assert the thread's interrupted status Thread . currentThread (). interrupt (); // We don't need the result, so cancel the task too future . cancel ( true ); } catch ( ExecutionException e ) { throw launderThrowable ( e . getCause ()); } } }","title":"Example: page renderer with Future"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#limitations-of-parallelizing-heterogeneous-tasks","text":"Two people can divide the work of cleaning the dinner dishes fairly effectively: one person washes while the other dries. However, assigning a different type of task to each worker does not scale well; if several more people show up, it is not obvious how they can help without getting in the way or significantly restructuring the division of labor. Without finding finer-grained parallelism among similar tasks, this approach will yield diminishing returns. A further problem with dividing heterogeneous tasks among multiple workers is that the tasks may have disparate sizes. If you divide tasks A and B between two workers but A takes ten times as long as B, you\u2019ve only speeded up the total process by 9%. Finally, dividing a task among multiple workers always involves some amount of coordination overhead; for the division to be worthwhile, this overhead must be more than compensated by productivity improvements due to parallelism. The real performance payoff of dividing a program\u2019s workload into tasks comes when there are a large number of independent, homogeneous tasks that can be processed concurrently.","title":"Limitations of parallelizing heterogeneous tasks"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#completionservice-executor-meets-blockingqueue","text":"If you have a batch of computations to submit to an Executor and you want to retrieve their results as they become available, you could retain the Future associated with each task and repeatedly poll for completion by calling get with a timeout of zero. This is possible, but tedious. Fortunately there is a better way: a completion service. CompletionService combines the functionality of an Executor and a BlockingQueue . You can submit Callable tasks to it for execution and use the queue-like methods take and poll to retrieve completed results, packaged as Futures , as they become available. ExecutorCompletionService implements CompletionService , delegating the computation to an Executor. The implementation of ExecutorCompletionService is quite straightforward. The constructor creates a BlockingQueue to hold the completed results. FutureTask has a done method that is called when the computation completes. When a task is submitted, it is wrapped with a QueueingFuture , a subclass of FutureTask that overrides done to place the result on the BlockingQueue , as shown below: private class QueueingFuture < V > extends FutureTask < V > { QueueingFuture ( Callable < V > c ) { super ( c ); } QueueingFuture ( Runnable t , V r ) { super ( t , r ); } protected void done () { completionQueue . add ( this ); } } ( QueueingFuture class used by ExecutorCompletionService )","title":"CompletionService: Executor meets BlockingQueue"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#example-page-renderer-with-completionservice","text":"We can use a CompletionService to improve the performance of the page renderer in two ways: shorter total runtime and improved responsiveness. We can create a separate task for downloading each image and execute them in a thread pool, turning the sequential download into a parallel one: this reduces the amount of time to download all the images. And by fetching results from the CompletionService and rendering each image as soon as it is available, we can give the user a more dynamic and responsive user interface. public abstract class Renderer { private final ExecutorService executor ; Renderer ( ExecutorService executor ) { this . executor = executor ; } void renderPage ( CharSequence source ) { final List < ImageInfo > info = scanForImageInfo ( source ); CompletionService < ImageData > completionService = new ExecutorCompletionService < ImageData >( executor ); for ( final ImageInfo imageInfo : info ) completionService . submit ( new Callable < ImageData >() { public ImageData call () { return imageInfo . downloadImage (); } }); renderText ( source ); try { for ( int t = 0 , n = info . size (); t < n ; t ++) { Future < ImageData > f = completionService . take (); ImageData imageData = f . get (); renderImage ( imageData ); } } catch ( InterruptedException e ) { Thread . currentThread (). interrupt (); } catch ( ExecutionException e ) { throw launderThrowable ( e . getCause ()); } } } Multiple ExecutorCompletionServices can share a single Executor, so it is perfectly sensible to create an ExecutorCompletionService that is private to a particular computation while sharing a common Executor. When used in this way, a CompletionService acts as a handle for a batch of computations in much the same way that a Future acts as a handle for a single computation. By remembering how many tasks were submitted to the CompletionService and counting how many completed results are retrieved, you can know when all the results for a given batch have been retrieved, even if you use a shared Executor.","title":"Example: page renderer with CompletionService"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#placing-time-limits-on-tasks","text":"Sometimes, if an activity does not complete within a certain amount of time, the result is no longer needed and the activity can be abandoned. For example, a web application may fetch its advertisements from an external ad server, but if the ad is not available within two seconds, it instead displays a default advertisement so that ad unavailability does not undermine the site\u2019s responsiveness requirements. Similarly, a portal site may fetch data in parallel from multiple data sources, but may be willing to wait only a certain amount of time for data to be available before rendering the page without it. The primary challenge in executing tasks within a time budget is making sure that you don\u2019t wait longer than the time budget to get an answer or find out that one is not forthcoming. The timed version of Future.get supports this requirement: it returns as soon as the result is ready, but throws TimeoutException if the result is not ready within the timeout period. A secondary problem when using timed tasks is to stop them when they run out of time, so they do not waste computing resources by continuing to compute a result that will not be used. This can be accomplished by having the task strictly manage its own time budget and abort if it runs out of time, or by cancelling the task if the timeout expires. Again, Future can help; if a timed get completes with a TimeoutException , you can cancel the task through the Future . If the task is written to be cancellable, it can be terminated early so as not to consume excessive resources. Listing below shows a typical application of a timed Future.get . It generates a composite web page that contains the requested content plus an advertisement fetched from an ad server. It submits the ad-fetching task to an executor, computes the rest of the page content, and then waits for the ad until its time budget runs out. If the get times out 1 , it cancels 2 the ad-fetching task and uses a default advertisement instead. Page renderPageWithAd () throws InterruptedException { long endNanos = System . nanoTime () + TIME_BUDGET ; Future < Ad > f = exec . submit ( new FetchAdTask ()); // Render the page while waiting for the ad Page page = renderPageBody (); Ad ad ; try { // Only wait for the remaining time budget long timeLeft = endNanos - System . nanoTime (); ad = f . get ( timeLeft , NANOSECONDS ); } catch ( ExecutionException e ) { ad = DEFAULT_AD ; } catch ( TimeoutException e ) { ad = DEFAULT_AD ; f . cancel ( true ); } page . setAd ( ad ); return page ; }","title":"Placing time limits on tasks"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#example-a-travel-reservations-portal","text":"Consider a travel reservation portal: the user enters travel dates and requirements and the portal fetches and displays bids from a number of airlines, hotels or car rental companies. Depending on the company, fetching a bid might involve invoking a web service, consulting a database, performing an EDI transaction, or some other mechanism. Rather than have the response time for the page be driven by the slowest response, it may be preferable to present only the information available within a given time budget. For providers that do not respond in time, the page could either omit them completely or display a placeholder such as \u201cDid not hear from Air Java in time.\u201d Fetching a bid from one company is independent of fetching bids from another, so fetching a single bid is a sensible task boundary that allows bid retrieval to proceed concurrently. It would be easy enough to create n tasks, submit them to a thread pool, retain the Futures, and use a timed get to fetch each result sequentially via its Future, but there is an even easier way\u2014 invokeAll . Below code uses the timed version of invokeAll to submit multiple tasks to an ExecutorService and retrieve the results. The invokeAll method takes a collection of tasks and returns a collection of Futures. The two collections have identical structures; invokeAll adds the Futures to the returned collection in the order imposed by the task collection\u2019s iterator, thus allowing the caller to associate a Future with the Callable it represents. The timed version of invokeAll will return when all the tasks have completed, the calling thread is interrupted, or the timeout expires. Any tasks that are not complete when the timeout expires are cancelled. On return from invokeAll , each task will have either completed normally or been cancelled; the client code can call get or isCancelled to find out which. public class TimeBudget { private static ExecutorService exec = Executors . newCachedThreadPool (); public List < TravelQuote > getRankedTravelQuotes ( TravelInfo travelInfo , Set < TravelCompany > companies , Comparator < TravelQuote > ranking , long time , TimeUnit unit ) throws InterruptedException { List < QuoteTask > tasks = new ArrayList < QuoteTask >(); for ( TravelCompany company : companies ) tasks . add ( new QuoteTask ( company , travelInfo )); List < Future < TravelQuote >> futures = exec . invokeAll ( tasks , time , unit ); List < TravelQuote > quotes = new ArrayList < TravelQuote >( tasks . size ()); Iterator < QuoteTask > taskIter = tasks . iterator (); for ( Future < TravelQuote > f : futures ) { QuoteTask task = taskIter . next (); try { quotes . add ( f . get ()); } catch ( ExecutionException e ) { quotes . add ( task . getFailureQuote ( e . getCause ())); } catch ( CancellationException e ) { quotes . add ( task . getTimeoutQuote ( e )); } } Collections . sort ( quotes , ranking ); return quotes ; } } class QuoteTask implements Callable < TravelQuote > { private final TravelCompany company ; private final TravelInfo travelInfo ; // ... public TravelQuote call () throws Exception { return company . solicitQuote ( travelInfo ); } }","title":"Example: a travel reservations portal"},{"location":"java/concurrency/structuring-concurrent-applications/c06-task-execution/#summary","text":"Structuring applications around the execution of tasks can simplify development and facilitate concurrency. The Executor framework permits you to decouple task submission from execution policy and supports a rich variety of execution policies; whenever you find yourself creating threads to perform tasks, consider using an Executor instead. To maximize the benefit of decomposing an application into tasks, you must identify sensible task boundaries. In some applications, the obvious task boundaries work well, whereas in others some analysis may be required to uncover finer-grained exploitable parallelism. The timeout passed to get is computed by subtracting the current time from the deadline; this may in fact yield a negative number, but all the timed methods in java.util.concurrent treat negative timeouts as zero, so no extra code is needed to deal with this case.. \u21a9 The true parameter to Future.cancel means that the task thread can be interrupted if the task is currently running. \u21a9","title":"Summary"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/","text":"7. Cancellation and Shutdown Getting tasks and threads to stop safely, quickly, and reliably is not always easy. Java does not provide any mechanism for safely forcing a thread to stop what it is doing. Instead, it provides interruption, a cooperative mechanism that lets one thread ask another to stop what it is doing. The cooperative approach is required because we rarely want a task, thread, or service to stop immediately, since that could leave shared data structures in an inconsistent state. Instead, tasks and services can be coded so that, when requested, they clean up any work currently in progress and then terminate. This provides greater flexibility, since the task code itself is usually better able to assess the cleanup required than is the code requesting cancellation. End-of-lifecycle issues can complicate the design and implementation of tasks, services, and applications, and this important element of program design is too often ignored. Dealing well with failure, shutdown, and cancellation is one of the characteristics that distinguishes a well-behaved application from one that merely works. Task cancellation An activity is cancellable if external code can move it to completion before its normal completion. There are a number of reasons why you might want to cancel an activity: User-requested cancellation. Time-limited activities. Application events. Errors. Shutdown. There is no safe way to preemptively stop a thread in Java, and therefore no safe way to preemptively stop a task. There are only cooperative mechanisms, by which the task and the code requesting cancellation follow an agreed-upon protocol. A task that wants to be cancellable must have a cancellation policy that specifies the \u201chow\u201d, \u201cwhen\u201d, and \u201cwhat\u201d of cancellation\u2014how other code can request cancellation, when the task checks whether cancellation has been requested, and what actions the task takes in response to a cancellation request. Cancellation policy Banks have rules about how to submit a stop-payment request, what responsiveness guarantees it makes in processing such requests, and what procedures it follows when payment is actually stopped (such as notifying the other bank involved in the transaction and assessing a fee against the payor\u2019s account). Taken together, these procedures and guarantees comprise the cancellation policy for check payment. Interruption The producer thread generates primes and places them on a blocking queue. If the producer gets ahead of the consumer, the queue will fill up and put will block. What happens if the consumer tries to cancel the producer task while it is blocked in put ? It can call cancel which will set the cancelled flag\u2014but the producer will never check the flag because it will never emerge from the blocking put (because the consumer has stopped retrieving primes from the queue). Thread interruption is a cooperative mechanism for a thread to signal another thread that it should, at its convenience and if it feels like it, stop what it is doing and do something else. There is nothing in the API or language specification that ties interruption to any specific cancellation semantics, but in practice, using interruption for anything but cancellation is fragile and difficult to sustain in larger applications. class BrokenPrimeProducer extends Thread { private final BlockingQueue < BigInteger > queue ; private volatile boolean cancelled = false ; BrokenPrimeProducer ( BlockingQueue < BigInteger > queue ) { this . queue = queue ; } public void run () { try { BigInteger p = BigInteger . ONE ; while (! cancelled ) queue . put ( p = p . nextProbablePrime ()); } catch ( InterruptedException consumed ) { } } public void cancel () { cancelled = true ; } void consumePrimes () throws InterruptedException { BlockingQueue < BigInteger > primes = ...; BrokenPrimeProducer producer = new BrokenPrimeProducer ( primes ); producer . start (); try { while ( needMorePrimes ()) consume ( primes . take ()); } finally { producer . cancel (); } } } Each thread has a boolean interrupted status ; interrupting a thread sets its interrupted status to true. Thread contains methods for interrupting a thread and querying the interrupted status of a thread. The interrupt method interrupts the target thread, and isInterrupted returns the interrupted status of the target thread. The poorly named static interrupted method clears the interrupted status of the current thread and returns its previous value; this is the only way to clear the interrupted status. Blocking library methods like Thread.sleep and Object.wait try to detect when a thread has been interrupted and return early. They respond to interruption by clearing the interrupted status and throwing InterruptedException , indicating that the blocking operation completed early due to interruption. The JVM makes no guarantees on how quickly a blocking method will detect interruption, but in practice this happens reasonably quickly. If a thread is interrupted when it is not blocked, its interrupted status is set, and it is up to the activity being cancelled to poll the interrupted status to detect interruption. In this way interruption is \u201csticky\u201d\u2014if it doesn\u2019t trigger an InterruptedException , evidence of interruption persists until someone deliberately clears the interrupted status. Calling interrupt does not necessarily stop the target thread from doing what it is doing; it merely delivers the message that interruption has been requested. A good way to think about interruption is that it does not actually interrupt a running thread; it just requests that the thread interrupt itself at the next convenient opportunity. (These opportunities are called cancellation points.) Some methods, such as wait , sleep , and join , take such requests seriously, throwing an exception when they receive an interrupt request or encounter an already set interrupt status upon entry. Well behaved methods may totally ignore such requests so long as they leave the interruption request in place so that calling code can do something with it. Poorly behaved methods swallow the interrupt request, thus denying code further up the call stack the opportunity to act on it. The static interrupted method should be used with caution, because it clears the current thread\u2019s interrupted status. If you call interrupted and it returns true, unless you are planning to swallow the interruption, you should do something with it\u2014either throw InterruptedException or restore the interrupted status by calling interrupt again. BrokenPrimeProducer illustrates how custom cancellation mechanisms do not always interact well with blocking library methods. If you code your tasks to be responsive to interruption, you can use interruption as your cancellation mechanism and take advantage of the interruption support provided by many library classes. Interruption is usually the most sensible way to implement cancellation. BrokenPrimeProducer can be easily fixed (and simplified) by using interruption instead of a boolean flag to request cancellation. There are two points in each loop iteration where interruption may be detected: in the blocking put call, and by explicitly polling the interrupted status in the loop header. The explicit test is not strictly necessary here because of the blocking put call, but it makes PrimeProducer more responsive to interruption because it checks for interruption before starting the lengthy task of searching for a prime, rather than after. When calls to interruptible blocking methods are not frequent enough to deliver the desired responsiveness, explicitly testing the interrupted status can help. public class PrimeProducer extends Thread { private final BlockingQueue < BigInteger > queue ; PrimeProducer ( BlockingQueue < BigInteger > queue ) { this . queue = queue ; } public void run () { try { BigInteger p = BigInteger . ONE ; while (! Thread . currentThread (). isInterrupted ()) queue . put ( p = p . nextProbablePrime ()); } catch ( InterruptedException consumed ) { /* Allow thread to exit */ } } public void cancel () { interrupt (); } } Interruption policies Just as tasks should have a cancellation policy, threads should have an interruption policy. An interruption policy determines how a thread interprets an interruption request\u2014what it does (if anything) when one is detected, what units of work are considered atomic with respect to interruption, and how quickly it reacts to interruption. The most sensible interruption policy is some form of thread-level or service- level cancellation: exit as quickly as practical, cleaning up if necessary, and possibly notifying some owning entity that the thread is exiting. It is possible to establish other interruption policies, such as pausing or resuming a service, but threads or thread pools with nonstandard interruption policies may need to be restricted to tasks that have been written with an awareness of the policy. It is important to distinguish between how tasks and threads should react to interruption. A single interrupt request may have more than one desired recipient\u2014interrupting a worker thread in a thread pool can mean both \u201ccancel the current task\u201d and \u201cshut down the worker thread\u201d. Tasks do not execute in threads they own; they borrow threads owned by a service such as a thread pool. Code that doesn\u2019t own the thread (for a thread pool, any code outside of the thread pool implementation) should be careful to preserve the interrupted status so that the owning code can eventually act on it, even if the \u201cguest\u201d code acts on the interruption as well. (If you are house-sitting for someone, you don\u2019t throw out the mail that comes while they\u2019re away\u2014you save it and let them deal with it when they get back, even if you do read their magazines.) This is why most blocking library methods simply throw InterruptedException in response to an interrupt. They will never execute in a thread they own, so they implement the most reasonable cancellation policy for task or library code: get out of the way as quickly as possible and communicate the interruption back to the caller so that code higher up on the call stack can take further action. A task needn\u2019t necessarily drop everything when it detects an interruption request\u2014it can choose to postpone it until a more opportune time by remembering that it was interrupted, finishing the task it was performing, and then throwing InterruptedException or otherwise indicating interruption. This technique can protect data structures from corruption when an activity is interrupted in the middle of an update. A task should not assume anything about the interruption policy of its executing thread unless it is explicitly designed to run within a service that has a specific interruption policy. Whether a task interprets interruption as cancellation or takes some other action on interruption, it should take care to preserve the executing thread\u2019s interruption status. If it is not simply going to propagate InterruptedException to its caller, it should restore the interruption status after catching InterruptedException : Thread . currentThread (). interrupt (); Just as task code should not make assumptions about what interruption means to its executing thread, cancellation code should not make assumptions about the interruption policy of arbitrary threads. A thread should be interrupted only by its owner; the owner can encapsulate knowledge of the thread\u2019s interruption policy in an appropriate cancellation mechanism such as a shutdown method. Because each thread has its own interruption policy, you should not interrupt a thread unless you know what interruption means to that thread. Responding to interruption As mentioned, when you call an interruptible blocking method such as Thread.sleep or BlockingQueue.put , there are two practical strategies for handling InterruptedException : Propagate the exception (possibly after some task-specific cleanup), making your method an interruptible blocking method, too; or Restore the interruption status so that code higher up on the call stack can deal with it ( Thread.currentThread().interrupt() ). Propagating InterruptedException : BlockingQueue < Task > queue ; ... public Task getNextTask () throws InterruptedException { return queue . take (); } If you don\u2019t want to or cannot propagate InterruptedException (perhaps because your task is defined by a Runnable ), you need to find another way to preserve the interruption request. What you should not do is swallow the InterruptedException by catching it and doing nothing in the catch block, unless your code is actually implementing the interruption policy for a thread. PrimeProducer swallows the interrupt, but does so with the knowledge that the thread is about to terminate and that therefore there is no code higher up on the call stack that needs to know about the interruption. Most code does not know what thread it will run in and so should preserve the interrupted status. Only code that implements a thread\u2019s interruption policy may swallow an interruption request. General-purpose task and library code should never swallow interruption requests. Activities that do not support cancellation but still call interruptible blocking methods will have to call them in a loop, retrying when interruption is detected. In this case, they should save the interruption status locally and restore it just before returning, as shown in below code, rather than immediately upon catching InterruptedException . Setting the interrupted status too early could result in an infinite loop, because most interruptible blocking methods check the interrupted status on entry and throw InterruptedException immediately if it is set. (Interruptible methods usually poll for interruption before blocking or doing any significant work, so as to be as responsive to interruption as possible.) public class NonCancelableTask { public Task getNextTask ( BlockingQueue < Task > queue ) { boolean interrupted = false ; try { while ( true ) { try { return queue . take (); } catch ( InterruptedException e ) { interrupted = true ; // fall through and retry } } } finally { if ( interrupted ) Thread . currentThread (). interrupt (); } } } If your code does not call interruptible blocking methods, it can still be made responsive to interruption by polling the current thread\u2019s interrupted status throughout the task code. Cancellation can involve state other than the interruption status; interruption can be used to get the thread\u2019s attention, and information stored elsewhere by the interrupting thread can be used to provide further instructions for the interrupted thread. (Be sure to use synchronization when accessing that information.) For example, when a worker thread owned by a ThreadPoolExecutor detects interruption, it checks whether the pool is being shut down. Example: timed run Below code shows an attempt at running an arbitrary Runnable for a given amount of time. It runs the task in the calling thread and schedules a cancellation task to interrupt it after a given time interval. This addresses the problem of unchecked exceptions thrown from the task, since they can then be caught by the caller of timedRun . This is an appealingly simple approach, but it violates the rules: you should know a thread\u2019s interruption policy before interrupting it. Since timedRun can be called from an arbitrary thread, it cannot know the calling thread\u2019s interruption policy. private static final ScheduledExecutorService cancelExec = Executors . newScheduledThreadPool ( 1 ); public static void timedRun ( Runnable r , long timeout , TimeUnit unit ) { final Thread taskThread = Thread . currentThread (); cancelExec . schedule ( new Runnable () { public void run () { taskThread . interrupt (); } }, timeout , unit ); r . run (); } If the task completes before the timeout, the cancellation task that interrupts the thread in which timedRun was called could go off after timedRun has returned to its caller. We don\u2019t know what code will be running when that happens, but the result won\u2019t be good. Further, if the task is not responsive to interruption, timedRun will not return until the task finishes, which may be long after the desired timeout (or even not at all). In following example, the thread created to run the task can have its own execution policy, and even if the task doesn\u2019t respond to the interrupt, the timed run method can still return to its caller. After starting the task thread, timedRun executes a timed join with the newly created thread. After join returns, it checks if an exception was thrown from the task and if so, rethrows it in the thread calling timedRun. The saved Throwable is shared between the two threads, and so is declared volatile to safely publish it from the task thread to the timedRun thread. private static final ScheduledExecutorService cancelExec = Executors . newScheduledThreadPool ( 1 ); public static void timedRun ( final Runnable r , long timeout , TimeUnit unit ) throws InterruptedException { class RethrowableTask implements Runnable { private volatile Throwable t ; public void run () { try { r . run (); } catch ( Throwable t ) { this . t = t ; } } void rethrow () { if ( t != null ) throw launderThrowable ( t ); } } RethrowableTask task = new RethrowableTask (); final Thread taskThread = new Thread ( task ); taskThread . start (); cancelExec . schedule ( new Runnable () { public void run () { taskThread . interrupt (); } }, timeout , unit ); taskThread . join ( unit . toMillis ( timeout )); task . rethrow (); } This version addresses the problems in the previous examples, but because it relies on a timed join , it shares a deficiency with join : we don\u2019t know if control was returned because the thread exited normally or because the join timed out. Cancellation via Future We\u2019ve already used an abstraction for managing the lifecycle of a task, dealing with exceptions, and facilitating cancellation: Future . private static final ExecutorService taskExec = Executors . newCachedThreadPool (); public static void timedRun ( Runnable r , long timeout , TimeUnit unit ) throws InterruptedException { Future <?> task = taskExec . submit ( r ); try { task . get ( timeout , unit ); } catch ( TimeoutException e ) { // task will be cancelled below } catch ( ExecutionException e ) { // exception thrown in task; rethrow throw launderThrowable ( e . getCause ()); } finally { // Harmless if task already completed task . cancel ( true ); // interrupt if running } } ExecutorService.submit returns a Future describing the task. Future has a cancel method that takes a boolean argument ( mayInterruptIfRunning ), and returns a value indicating whether the cancellation attempt was successful. (This tells you only whether it was able to deliver the interruption, not whether the task detected and acted on it.) When mayInterruptIfRunning is true and the task is currently running in some thread, then that thread is interrupted. Setting this argument to false means \u201cdon\u2019t run this task if it hasn\u2019t started yet\u201d, and should be used for tasks that are not designed to handle interruption. The task execution threads created by the standard Executor implementations implement an interruption policy that lets tasks be cancelled using interruption, so it is safe to set mayInterruptIfRunning when cancelling tasks through their Future when they are running in a standard Executor. You should not interrupt a pool thread directly when attempting to cancel a task, because you won\u2019t know what task is running when the interrupt request is delivered\u2014do this only through the task\u2019s Future . Sample shows a version of timedRun that submits the task to an ExecutorService and retrieves the result with a timed Future.get . If get terminates with a TimeoutException , the task is cancelled via its Future. (To simplify coding, this version calls Future.cancel unconditionally in a finally block, taking advantage of the fact that cancelling a completed task has no effect. ) If the underlying computation throws an exception prior to cancellation, it is rethrown from timedRun , which is the most convenient way for the caller to deal with the exception. It also illustrates another good practice: cancelling tasks whose result is no longer needed. When Future.get throws InterruptedException or TimeoutException and you know that the result is no longer needed by the program, cancel the task with Future.cancel . Dealing with non-interruptible blocking Many blocking library methods respond to interruption by returning early and throwing InterruptedException , which makes it easier to build tasks that are responsive to cancellation. However, not all blocking methods or blocking mechanisms are responsive to interruption; if a thread is blocked performing synchronous socket I/O or waiting to acquire an intrinsic lock, interruption has no effect other than setting the thread\u2019s interrupted status. We can sometimes convince threads blocked in noninterruptible activities to stop by means similar to interruption, but this requires greater awareness of why the thread is blocked. ReaderThread shows a technique for encapsulating nonstandard cancellation. ReaderThread manages a single socket connection, reading synchronously from the socket and passing any data received to processBuffer. To facilitate terminating a user connection or shutting down the server, ReaderThread overrides interrupt to both deliver a standard interrupt and close the underlying socket; thus interrupting a ReaderThread makes it stop what it is doing whether it is blocked in read or in an interruptible blocking method. public class ReaderThread extends Thread { private static final int BUFSZ = 512 ; private final Socket socket ; private final InputStream in ; public ReaderThread ( Socket socket ) throws IOException { this . socket = socket ; this . in = socket . getInputStream (); } public void interrupt () { try { socket . close (); } catch ( IOException ignored ) { } finally { super . interrupt (); } } public void run () { try { byte [] buf = new byte [ BUFSZ ]; while ( true ) { int count = in . read ( buf ); if ( count < 0 ) break ; else if ( count > 0 ) processBuffer ( buf , count ); } } catch ( IOException e ) { /* Allow thread to exit */ } } public void processBuffer ( byte [] buf , int count ) { } } Encapsulating nonstandard cancellation with newTaskFor The technique used in ReaderThread to encapsulate nonstandard cancellation can be refined using the newTaskFor hook added to ThreadPoolExecutor in Java 6. When a Callable is submitted to an ExecutorService , submit returns a Future that can be used to cancel the task. The newTaskFor hook is a factory method that creates the Future representing the task. It returns a RunnableFuture , an interface that extends both Future and Runnable (and is implemented by FutureTask ). Customizing the task Future allows you to override Future.cancel . Custom cancellation code can perform logging or gather statistics on cancellation, and can also be used to cancel activities that are not responsive to interruption. ReaderThread encapsulates cancellation of socket-using threads by overriding interrupt; the same can be done for tasks by overriding Future.cancel . public abstract class SocketUsingTask < T > implements CancellableTask < T > { @GuardedBy ( \"this\" ) private Socket socket ; protected synchronized void setSocket ( Socket s ) { socket = s ; } public synchronized void cancel () { try { if ( socket != null ) socket . close (); } catch ( IOException ignored ) { } } public RunnableFuture < T > newTask () { return new FutureTask < T >( this ) { public boolean cancel ( boolean mayInterruptIfRunning ) { try { SocketUsingTask . this . cancel (); } finally { return super . cancel ( mayInterruptIfRunning ); } } }; } } interface CancellableTask < T > extends Callable < T > { void cancel (); RunnableFuture < T > newTask (); } @ThreadSafe class CancellingExecutor extends ThreadPoolExecutor { // ... protected < T > RunnableFuture < T > newTaskFor ( Callable < T > callable ) { if ( callable instanceof CancellableTask ) return (( CancellableTask < T >) callable ). newTask (); else return super . newTaskFor ( callable ); } } CancellableTask defines a CancellableTask interface that extends Callable and adds a cancel method and a newTask factory method for constructing a RunnableFuture . CancellingExecutor extends ThreadPoolExecutor , and overrides newTaskFor to let a CancellableTask create its own Future . SocketUsingTask implements CancellableTask and defines Future.cancel to close the socket as well as call super.cancel . If a SocketUsingTask is cancelled through its Future , the socket is closed and the executing thread is interrupted. This increases the task\u2019s responsiveness to cancellation: not only can it safely call interruptible blocking methods while remaining responsive to cancellation, but it can also call blocking socket I/O methods. Stopping a thread-based service Applications commonly create services that own threads, such as thread pools, and the lifetime of these services is usually longer than that of the method that creates them. If the application is to shut down gracefully, the threads owned by these services need to be terminated. Since there is no preemptive way to stop a thread, they must instead be persuaded to shut down on their own. Sensible encapsulation practices dictate that you should not manipulate a thread\u2014interrupt it, modify its priority, etc.\u2014unless you own it. The thread API has no formal concept of thread ownership: a thread is represented with a Thread object that can be freely shared like any other object. However, it makes sense to think of a thread as having an owner, and this is usually the class that created the thread. So a thread pool owns its worker threads, and if those threads need to be interrupted, the thread pool should take care of it. As with any other encapsulated object, thread ownership is not transitive: the application may own the service and the service may own the worker threads, but the application doesn\u2019t own the worker threads and therefore should not attempt to stop them directly. Instead, the service should provide lifecycle methods for shutting itself down that also shut down the owned threads; then the application can shut down the service, and the service can shut down the threads. ExecutorService provides the shutdown and shutdownNow methods; other thread-owning services should provide a similar shutdown mechanism. Provide lifecycle methods whenever a thread-owning service has a lifetime longer than that of the method that created it. Example: a logging service public class LogService { private final BlockingQueue < String > queue ; private final LoggerThread loggerThread ; private final PrintWriter writer ; @GuardedBy ( \"this\" ) private boolean isShutdown ; @GuardedBy ( \"this\" ) private int reservations ; public LogService ( Writer writer ) { this . queue = new LinkedBlockingQueue < String >(); this . loggerThread = new LoggerThread (); this . writer = new PrintWriter ( writer ); } public void start () { loggerThread . start (); } public void stop () { synchronized ( this ) { isShutdown = true ; } loggerThread . interrupt (); } public void log ( String msg ) throws InterruptedException { synchronized ( this ) { if ( isShutdown ) throw new IllegalStateException ( /*...*/ ); ++ reservations ; } queue . put ( msg ); } private class LoggerThread extends Thread { public void run () { try { while ( true ) { try { synchronized ( LogService . this ) { if ( isShutdown && reservations == 0 ) break ; } String msg = queue . take (); synchronized ( LogService . this ) { -- reservations ; } writer . println ( msg ); } catch ( InterruptedException e ) { /* retry */ } } } finally { writer . close (); } } } } ExecutorService shutdown ExecutorService offers two ways to shut down: graceful shutdown with shutdown , and abrupt shutdown with shutdownNow . In an abrupt shutdown, shutdownNow returns the list of tasks that had not yet started after attempting to cancel all actively executing tasks. The two different termination options offer a tradeoff between safety and responsiveness: abrupt termination is faster but riskier because tasks may be interrupted in the middle of execution, and normal termination is slower but safer because the ExecutorService does not shut down until all queued tasks are processed. Other thread-owning services should consider providing a similar choice of shutdown modes. Simple programs can get away with starting and shutting down a global ExecutorService from main. More sophisticated programs are likely to encapsulate an ExecutorService behind a higher-level service that provides its own lifecycle methods, such as the below sample, that delegates to an ExecutorService instead of managing its own threads. Encapsulating an ExecutorService extends the ownership chain from application to service to thread by adding another link; each member of the chain manages the lifecycle of the services or threads it owns. public class LogService { private final ExecutorService exec = newSingleThreadExecutor (); ... public void start () { } public void stop () throws InterruptedException { try { exec . shutdown (); exec . awaitTermination ( TIMEOUT , UNIT ); } finally { writer . close (); } } public void log ( String msg ) { try { exec . execute ( new WriteTask ( msg )); } catch ( RejectedExecutionException ignored ) { } } } Poison pills (tombstone) Another way to convince a producer-consumer service to shut down is with a poison pill: a recognizable object placed on the queue that means \u201cwhen you get this, stop.\u201d With a FIFO queue, poison pills ensure that consumers finish the work on their queue before shutting down, since any work submitted prior to submitting the poison pill will be retrieved before the pill; producers should not submit any work after putting a poison pill on the queue. Poison pills work only when the number of producers and consumers is known. Poison pills work reliably only with unbounded queues. Example: a one-shot execution service If a method needs to process a batch of tasks and does not return until all the tasks are finished, it can simplify service lifecycle management by using a private Executor whose lifetime is bounded by that method. The checkMail method in below sample checks for new mail in parallel on a number of hosts. It creates a private Executor and submits a task for each host: it then shuts down the executor and waits for termination, which occurs when all the mail-checking tasks have completed. public boolean checkMail ( Set < String > hosts , long timeout , TimeUnit unit ) throws InterruptedException { ExecutorService exec = Executors . newCachedThreadPool (); final AtomicBoolean hasNewMail = new AtomicBoolean ( false ); try { for ( final String host : hosts ) exec . execute ( new Runnable () { public void run () { if ( checkMail ( host )) hasNewMail . set ( true ); } }); } finally { exec . shutdown (); exec . awaitTermination ( timeout , unit ); } return hasNewMail . get (); } Using a private Executor whose lifetime is bounded by a method call Limitations of shutdownNow When an ExecutorService is shut down abruptly with shutdownNow , it attempts to cancel the tasks currently in progress and returns a list of tasks that were submitted but never started so that they can be logged or saved for later processing. However, there is no general way to find out which tasks started but did not complete. This means that there is no way of knowing the state of the tasks in progress at shutdown time unless the tasks themselves perform some sort of checkpointing. To know which tasks have not completed, you need to know not only which tasks didn\u2019t start, but also which tasks were in progress when the executor was shut down. public class TrackingExecutor extends AbstractExecutorService { private final ExecutorService exec ; private final Set < Runnable > tasksCancelledAtShutdown = Collections . synchronizedSet ( new HashSet < Runnable >()); // ... public List < Runnable > getCancelledTasks () { if (! exec . isTerminated ()) throw new IllegalStateException ( /*...*/ ); return new ArrayList < Runnable >( tasksCancelledAtShutdown ); } public void execute ( final Runnable runnable ) { exec . execute ( new Runnable () { public void run () { try { runnable . run (); } finally { if ( isShutdown () && Thread . currentThread (). isInterrupted ()) tasksCancelledAtShutdown . add ( runnable ); } } }); } } TrackingExecutor shows a technique for determining which tasks were in progress at shutdown time. By encapsulating an ExecutorService and instrumenting execute (and similarly submit, not shown) to remember which tasks were cancelled after shutdown TrackingExecutor can identify which tasks started but did not complete normally. After the executor terminates, getCancelledTasks returns the list of cancelled tasks. In order for this technique to work, the tasks must preserve the thread\u2019s interrupted status when they return, which well behaved tasks will do anyway. public abstract class WebCrawler { private volatile TrackingExecutor exec ; @GuardedBy ( \"this\" ) private final Set < URL > urlsToCrawl = new HashSet < URL >(); private final ConcurrentMap < URL , Boolean > seen = new ConcurrentHashMap < URL , Boolean >(); private static final long TIMEOUT = 500 ; private static final TimeUnit UNIT = MILLISECONDS ; public WebCrawler ( URL startUrl ) { urlsToCrawl . add ( startUrl ); } public synchronized void start () { exec = new TrackingExecutor ( Executors . newCachedThreadPool ()); for ( URL url : urlsToCrawl ) submitCrawlTask ( url ); urlsToCrawl . clear (); } public synchronized void stop () throws InterruptedException { try { saveUncrawled ( exec . shutdownNow ()); if ( exec . awaitTermination ( TIMEOUT , UNIT )) saveUncrawled ( exec . getCancelledTasks ()); } finally { exec = null ; } } protected abstract List < URL > processPage ( URL url ); private void saveUncrawled ( List < Runnable > uncrawled ) { for ( Runnable task : uncrawled ) urlsToCrawl . add ((( CrawlTask ) task ). getPage ()); } private void submitCrawlTask ( URL u ) { exec . execute ( new CrawlTask ( u )); } // ... } WebCrawler shows an application of TrackingExecutor . When the crawler is shut down, both the tasks that did not start and those that were cancelled are scanned and their URLs recorded, so that page-crawling tasks for those URLs can be added to the queue when the crawler restarts. Handling abnormal thread termination Failure of a thread in a concurrent application is not always so obvious. The stack trace may be printed on the console, but no one may be watching the console. Also, when a thread fails, the application may appear to continue to work, so its failure could go unnoticed. Fortunately, there are means of both detecting and preventing threads from \u201cleaking\u201d from an application. The leading cause of premature thread death is RuntimeException. Because these exceptions indicate a programming error or other unrecoverable problem, they are generally not caught. Instead they propagate all the way up the stack, at which point the default behavior is to print a stack trace on the console and let the thread terminate. The consequences of abnormal thread death range from benign to disastrous, depending on the thread\u2019s role in the application. Losing a thread from a thread pool can have performance consequences, but an application that runs well with a 50-thread pool will probably run fine with a 49-thread pool too. But losing the event dispatch thread in a GUI application would be quite noticeable\u2014the application would stop processing events and the GUI would freeze. Task-processing threads such as the worker threads in a thread pool or the Swing event dispatch thread spend their whole life calling unknown code through an abstraction barrier like Runnable , and these threads should be very skeptical that the code they call will be well behaved. It would be very bad if a service like the Swing event thread failed just because some poorly written event handler threw a NullPointerException . Accordingly, these facilities should call tasks within a try-catch block that catches unchecked exceptions, or within a try-finally block to ensure that if the thread exits abnormally the framework is informed of this and can take corrective action. This is one of the few times when you might want to consider catching RuntimeException \u2014when you are calling unknown, untrusted code through an abstraction such as Runnable 1 . Below code illustrates a way to structure a worker thread within a thread pool. If a task throws an unchecked exception, it allows the thread to die, but not before notifying the framework that the thread has died. The framework may then replace the worker thread with a new thread, or may choose not to because the thread pool is being shut down or there are already enough worker threads to meet current demand. public void run () { Throwable thrown = null ; try { while (! isInterrupted ()) runTask ( getTaskFromWorkQueue ()); } catch ( Throwable e ) { thrown = e ; } finally { threadExited ( this , thrown ); } } ThreadPoolExecutor and Swing use this technique to ensure that a poorly behaved task doesn\u2019t prevent subsequent tasks from executing. If you are writing a worker thread class that executes submitted tasks, or calling untrusted external code (such as dynamically loaded plugins), use one of these approaches to prevent a poorly written task or plugin from taking down the thread that happens to call it. Uncaught exception handlers The previous section offered a proactive approach to the problem of unchecked exceptions. The Thread API also provides the UncaughtExceptionHandler facility, which lets you detect when a thread dies due to an uncaught exception. The two approaches are complementary: taken together, they provide defense-in-depth against thread leakage. When a thread exits due to an uncaught exception, the JVM reports this event to an application-provided UncaughtExceptionHandler ; if no handler exists, the default behavior is to print the stack trace to System.err . public interface UncaughtExceptionHandler { void uncaughtException ( Thread t , Throwable e ); } What the handler should do with an uncaught exception depends on your quality-of-service requirements. The most common response is to write an error message and stack trace to the application log, or take more direct action trying to restart the thread, shutting down the application, paging an operator, other corrective or diagnostic action. In long-running applications, always use uncaught exception handlers for all threads that at least log the exception. To set an UncaughtExceptionHandler for pool threads, provide a ThreadFactory to the ThreadPoolExecutor constructor. The standard thread pools allow an uncaught task exception to terminate the pool thread, but use a try-finally block to be notified when this happens so the thread can be replaced. Without an uncaught exception handler or other failure notification mechanism, tasks can appear to fail silently, which can be very confusing. If you want to be notified when a task fails due to an exception so that you can take some task-specific recovery action, either wrap the task with a Runnable or Callable that catches the exception or override the afterExecute hook in ThreadPoolExecutor . JVM shutdown In an orderly shutdown, the JVM first starts all registered shutdown hooks. Shutdown hooks are unstarted threads that are registered with Runtime.addShutdownHook . The JVM makes no guarantees on the order in which shutdown hooks are started. If any application threads (daemon or nondaemon) are still running at shutdown time, they continue to run concurrently with the shutdown process. When all shutdown hooks have completed, the JVM may choose to run finalizers if runFinalizersOnExit is true , and then halts. The JVM makes no attempt to stop or interrupt any application threads that are still running at shutdown time; they are abruptly terminated when the JVM eventually halts If the shutdown hooks or finalizers don\u2019t complete, then the orderly shutdown process \u201changs\u201d and the JVM must be shut down abruptly. In an abrupt shutdown, the JVM is not required to do anything other than halt the JVM; shutdown hooks will not run. Shutdown hooks should be thread-safe: they must use synchronization when accessing shared data and should be careful to avoid deadlock, just like any other concurrent code. Further, they should not make assumptions about the state of the application (such as whether other services have shut down already or all normal threads have completed) or about why the JVM is shutting down, and must therefore be coded extremely defensively. Finally, they should exit as quickly as possible, since their existence delays JVM termination at a time when the user may be expecting the JVM to terminate quickly. Shutdown hooks can be used for service or application cleanup, such as deleting temporary files or cleaning up resources that are not automatically cleaned up by the OS. Below code shows how LogService could register a shutdown hook from its start method to ensure the log file is closed on exit. public void start () { Runtime . getRuntime (). addShutdownHook ( new Thread () { public void run () { try { LogService . this . stop (); } catch ( InterruptedException ignored ) {} } }); } Because shutdown hooks all run concurrently, closing the log file could cause trouble for other shutdown hooks who want to use the logger. To avoid this problem, shutdown hooks should not rely on services that can be shut down by the application or other shutdown hooks. One way to accomplish this is to use a single shutdown hook for all services, rather than one for each service, and have it call a series of shutdown actions. This ensures that shutdown actions execute sequentially in a single thread, thus avoiding the possibility of race conditions or deadlock between shutdown actions. This technique can be used whether or not you use shutdown hooks; executing shutdown actions sequentially rather than concurrently eliminates many potential sources of failure. In applications that maintain explicit dependency information among services, this technique can also ensure that shutdown actions are performed in the right order. Daemon threads Sometimes you want to create a thread that performs some helper function but you don\u2019t want the existence of this thread to prevent the JVM from shutting down. This is what daemon threads are for. Threads are divided into two types: normal threads and daemon threads. When the JVM starts up, all the threads it creates (such as garbage collector and other housekeeping threads) are daemon threads, except the main thread. When a new thread is created, it inherits the daemon status of the thread that created it, so by default any threads created by the main thread are also normal threads. Normal threads and daemon threads differ only in what happens when they exit. When a thread exits, the JVM performs an inventory of running threads, and if the only threads that are left are daemon threads, it initiates an orderly shutdown. When the JVM halts, any remaining daemon threads are abandoned\u2014finally blocks are not executed, stacks are not unwound\u2014the JVM just exits. Daemon threads should be used sparingly\u2014few processing activities can be safely abandoned at any time with no cleanup. In particular, it is dangerous to use daemon threads for tasks that might perform any sort of I/O. Daemon threads are best saved for \u201chousekeeping\u201d tasks, such as a background thread that periodically removes expired entries from an in-memory cache. Daemon threads are not a good substitute for properly managing the life-cycle of services within an application. Finalizers The garbage collector does a good job of reclaiming memory resources when they are no longer needed, but some resources, such as file or socket handles, must be explicitly returned to the operating system when no longer needed. To assist in this, the garbage collector treats objects that have a nontrivial finalize method specially: after they are reclaimed by the collector, finalize is called so that persistent resources can be released. Since finalizers can run in a thread managed by the JVM, any state accessed by a finalizer will be accessed by more than one thread and therefore must be accessed with synchronization. Finalizers offer no guarantees on when or even if they run, and they impose a significant performance cost on objects with nontrivial finalizers. They are also extremely difficult to write correctly. In most cases, the combination of finally blocks and explicit close methods does a better job of resource management than finalizers; the sole exception is when you need to manage objects that hold resources acquired by native methods. For these reasons and others, work hard to avoid writing or using classes with finalizers (other than the platform library classes.) Avoid finalizers. Sumary End-of-lifecycle issues for tasks, threads, services, and applications can add complexity to their design and implementation. Java does not provide a preemptive mechanism for cancelling activities or terminating threads. Instead, it provides a cooperative interruption mechanism that can be used to facilitate cancellation, but it is up to you to construct protocols for cancellation and use them consistently. Using FutureTask and the Executor framework simplifies building cancellable tasks and services. There is some controversy over the safety of this technique; when a thread throws an unchecked exception, the entire application may possibly be compromised. But the alternative\u2014shutting down the entire application\u2014is usually not practical. \u21a9","title":"Cancellation and Shutdown"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#7-cancellation-and-shutdown","text":"Getting tasks and threads to stop safely, quickly, and reliably is not always easy. Java does not provide any mechanism for safely forcing a thread to stop what it is doing. Instead, it provides interruption, a cooperative mechanism that lets one thread ask another to stop what it is doing. The cooperative approach is required because we rarely want a task, thread, or service to stop immediately, since that could leave shared data structures in an inconsistent state. Instead, tasks and services can be coded so that, when requested, they clean up any work currently in progress and then terminate. This provides greater flexibility, since the task code itself is usually better able to assess the cleanup required than is the code requesting cancellation. End-of-lifecycle issues can complicate the design and implementation of tasks, services, and applications, and this important element of program design is too often ignored. Dealing well with failure, shutdown, and cancellation is one of the characteristics that distinguishes a well-behaved application from one that merely works.","title":"7. Cancellation and Shutdown"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#task-cancellation","text":"An activity is cancellable if external code can move it to completion before its normal completion. There are a number of reasons why you might want to cancel an activity: User-requested cancellation. Time-limited activities. Application events. Errors. Shutdown. There is no safe way to preemptively stop a thread in Java, and therefore no safe way to preemptively stop a task. There are only cooperative mechanisms, by which the task and the code requesting cancellation follow an agreed-upon protocol. A task that wants to be cancellable must have a cancellation policy that specifies the \u201chow\u201d, \u201cwhen\u201d, and \u201cwhat\u201d of cancellation\u2014how other code can request cancellation, when the task checks whether cancellation has been requested, and what actions the task takes in response to a cancellation request.","title":"Task cancellation"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#cancellation-policy","text":"Banks have rules about how to submit a stop-payment request, what responsiveness guarantees it makes in processing such requests, and what procedures it follows when payment is actually stopped (such as notifying the other bank involved in the transaction and assessing a fee against the payor\u2019s account). Taken together, these procedures and guarantees comprise the cancellation policy for check payment.","title":"Cancellation policy"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#interruption","text":"The producer thread generates primes and places them on a blocking queue. If the producer gets ahead of the consumer, the queue will fill up and put will block. What happens if the consumer tries to cancel the producer task while it is blocked in put ? It can call cancel which will set the cancelled flag\u2014but the producer will never check the flag because it will never emerge from the blocking put (because the consumer has stopped retrieving primes from the queue). Thread interruption is a cooperative mechanism for a thread to signal another thread that it should, at its convenience and if it feels like it, stop what it is doing and do something else. There is nothing in the API or language specification that ties interruption to any specific cancellation semantics, but in practice, using interruption for anything but cancellation is fragile and difficult to sustain in larger applications. class BrokenPrimeProducer extends Thread { private final BlockingQueue < BigInteger > queue ; private volatile boolean cancelled = false ; BrokenPrimeProducer ( BlockingQueue < BigInteger > queue ) { this . queue = queue ; } public void run () { try { BigInteger p = BigInteger . ONE ; while (! cancelled ) queue . put ( p = p . nextProbablePrime ()); } catch ( InterruptedException consumed ) { } } public void cancel () { cancelled = true ; } void consumePrimes () throws InterruptedException { BlockingQueue < BigInteger > primes = ...; BrokenPrimeProducer producer = new BrokenPrimeProducer ( primes ); producer . start (); try { while ( needMorePrimes ()) consume ( primes . take ()); } finally { producer . cancel (); } } } Each thread has a boolean interrupted status ; interrupting a thread sets its interrupted status to true. Thread contains methods for interrupting a thread and querying the interrupted status of a thread. The interrupt method interrupts the target thread, and isInterrupted returns the interrupted status of the target thread. The poorly named static interrupted method clears the interrupted status of the current thread and returns its previous value; this is the only way to clear the interrupted status. Blocking library methods like Thread.sleep and Object.wait try to detect when a thread has been interrupted and return early. They respond to interruption by clearing the interrupted status and throwing InterruptedException , indicating that the blocking operation completed early due to interruption. The JVM makes no guarantees on how quickly a blocking method will detect interruption, but in practice this happens reasonably quickly. If a thread is interrupted when it is not blocked, its interrupted status is set, and it is up to the activity being cancelled to poll the interrupted status to detect interruption. In this way interruption is \u201csticky\u201d\u2014if it doesn\u2019t trigger an InterruptedException , evidence of interruption persists until someone deliberately clears the interrupted status. Calling interrupt does not necessarily stop the target thread from doing what it is doing; it merely delivers the message that interruption has been requested. A good way to think about interruption is that it does not actually interrupt a running thread; it just requests that the thread interrupt itself at the next convenient opportunity. (These opportunities are called cancellation points.) Some methods, such as wait , sleep , and join , take such requests seriously, throwing an exception when they receive an interrupt request or encounter an already set interrupt status upon entry. Well behaved methods may totally ignore such requests so long as they leave the interruption request in place so that calling code can do something with it. Poorly behaved methods swallow the interrupt request, thus denying code further up the call stack the opportunity to act on it. The static interrupted method should be used with caution, because it clears the current thread\u2019s interrupted status. If you call interrupted and it returns true, unless you are planning to swallow the interruption, you should do something with it\u2014either throw InterruptedException or restore the interrupted status by calling interrupt again. BrokenPrimeProducer illustrates how custom cancellation mechanisms do not always interact well with blocking library methods. If you code your tasks to be responsive to interruption, you can use interruption as your cancellation mechanism and take advantage of the interruption support provided by many library classes. Interruption is usually the most sensible way to implement cancellation. BrokenPrimeProducer can be easily fixed (and simplified) by using interruption instead of a boolean flag to request cancellation. There are two points in each loop iteration where interruption may be detected: in the blocking put call, and by explicitly polling the interrupted status in the loop header. The explicit test is not strictly necessary here because of the blocking put call, but it makes PrimeProducer more responsive to interruption because it checks for interruption before starting the lengthy task of searching for a prime, rather than after. When calls to interruptible blocking methods are not frequent enough to deliver the desired responsiveness, explicitly testing the interrupted status can help. public class PrimeProducer extends Thread { private final BlockingQueue < BigInteger > queue ; PrimeProducer ( BlockingQueue < BigInteger > queue ) { this . queue = queue ; } public void run () { try { BigInteger p = BigInteger . ONE ; while (! Thread . currentThread (). isInterrupted ()) queue . put ( p = p . nextProbablePrime ()); } catch ( InterruptedException consumed ) { /* Allow thread to exit */ } } public void cancel () { interrupt (); } }","title":"Interruption"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#interruption-policies","text":"Just as tasks should have a cancellation policy, threads should have an interruption policy. An interruption policy determines how a thread interprets an interruption request\u2014what it does (if anything) when one is detected, what units of work are considered atomic with respect to interruption, and how quickly it reacts to interruption. The most sensible interruption policy is some form of thread-level or service- level cancellation: exit as quickly as practical, cleaning up if necessary, and possibly notifying some owning entity that the thread is exiting. It is possible to establish other interruption policies, such as pausing or resuming a service, but threads or thread pools with nonstandard interruption policies may need to be restricted to tasks that have been written with an awareness of the policy. It is important to distinguish between how tasks and threads should react to interruption. A single interrupt request may have more than one desired recipient\u2014interrupting a worker thread in a thread pool can mean both \u201ccancel the current task\u201d and \u201cshut down the worker thread\u201d. Tasks do not execute in threads they own; they borrow threads owned by a service such as a thread pool. Code that doesn\u2019t own the thread (for a thread pool, any code outside of the thread pool implementation) should be careful to preserve the interrupted status so that the owning code can eventually act on it, even if the \u201cguest\u201d code acts on the interruption as well. (If you are house-sitting for someone, you don\u2019t throw out the mail that comes while they\u2019re away\u2014you save it and let them deal with it when they get back, even if you do read their magazines.) This is why most blocking library methods simply throw InterruptedException in response to an interrupt. They will never execute in a thread they own, so they implement the most reasonable cancellation policy for task or library code: get out of the way as quickly as possible and communicate the interruption back to the caller so that code higher up on the call stack can take further action. A task needn\u2019t necessarily drop everything when it detects an interruption request\u2014it can choose to postpone it until a more opportune time by remembering that it was interrupted, finishing the task it was performing, and then throwing InterruptedException or otherwise indicating interruption. This technique can protect data structures from corruption when an activity is interrupted in the middle of an update. A task should not assume anything about the interruption policy of its executing thread unless it is explicitly designed to run within a service that has a specific interruption policy. Whether a task interprets interruption as cancellation or takes some other action on interruption, it should take care to preserve the executing thread\u2019s interruption status. If it is not simply going to propagate InterruptedException to its caller, it should restore the interruption status after catching InterruptedException : Thread . currentThread (). interrupt (); Just as task code should not make assumptions about what interruption means to its executing thread, cancellation code should not make assumptions about the interruption policy of arbitrary threads. A thread should be interrupted only by its owner; the owner can encapsulate knowledge of the thread\u2019s interruption policy in an appropriate cancellation mechanism such as a shutdown method. Because each thread has its own interruption policy, you should not interrupt a thread unless you know what interruption means to that thread.","title":"Interruption policies"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#responding-to-interruption","text":"As mentioned, when you call an interruptible blocking method such as Thread.sleep or BlockingQueue.put , there are two practical strategies for handling InterruptedException : Propagate the exception (possibly after some task-specific cleanup), making your method an interruptible blocking method, too; or Restore the interruption status so that code higher up on the call stack can deal with it ( Thread.currentThread().interrupt() ). Propagating InterruptedException : BlockingQueue < Task > queue ; ... public Task getNextTask () throws InterruptedException { return queue . take (); } If you don\u2019t want to or cannot propagate InterruptedException (perhaps because your task is defined by a Runnable ), you need to find another way to preserve the interruption request. What you should not do is swallow the InterruptedException by catching it and doing nothing in the catch block, unless your code is actually implementing the interruption policy for a thread. PrimeProducer swallows the interrupt, but does so with the knowledge that the thread is about to terminate and that therefore there is no code higher up on the call stack that needs to know about the interruption. Most code does not know what thread it will run in and so should preserve the interrupted status. Only code that implements a thread\u2019s interruption policy may swallow an interruption request. General-purpose task and library code should never swallow interruption requests. Activities that do not support cancellation but still call interruptible blocking methods will have to call them in a loop, retrying when interruption is detected. In this case, they should save the interruption status locally and restore it just before returning, as shown in below code, rather than immediately upon catching InterruptedException . Setting the interrupted status too early could result in an infinite loop, because most interruptible blocking methods check the interrupted status on entry and throw InterruptedException immediately if it is set. (Interruptible methods usually poll for interruption before blocking or doing any significant work, so as to be as responsive to interruption as possible.) public class NonCancelableTask { public Task getNextTask ( BlockingQueue < Task > queue ) { boolean interrupted = false ; try { while ( true ) { try { return queue . take (); } catch ( InterruptedException e ) { interrupted = true ; // fall through and retry } } } finally { if ( interrupted ) Thread . currentThread (). interrupt (); } } } If your code does not call interruptible blocking methods, it can still be made responsive to interruption by polling the current thread\u2019s interrupted status throughout the task code. Cancellation can involve state other than the interruption status; interruption can be used to get the thread\u2019s attention, and information stored elsewhere by the interrupting thread can be used to provide further instructions for the interrupted thread. (Be sure to use synchronization when accessing that information.) For example, when a worker thread owned by a ThreadPoolExecutor detects interruption, it checks whether the pool is being shut down.","title":"Responding to interruption"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#example-timed-run","text":"Below code shows an attempt at running an arbitrary Runnable for a given amount of time. It runs the task in the calling thread and schedules a cancellation task to interrupt it after a given time interval. This addresses the problem of unchecked exceptions thrown from the task, since they can then be caught by the caller of timedRun . This is an appealingly simple approach, but it violates the rules: you should know a thread\u2019s interruption policy before interrupting it. Since timedRun can be called from an arbitrary thread, it cannot know the calling thread\u2019s interruption policy. private static final ScheduledExecutorService cancelExec = Executors . newScheduledThreadPool ( 1 ); public static void timedRun ( Runnable r , long timeout , TimeUnit unit ) { final Thread taskThread = Thread . currentThread (); cancelExec . schedule ( new Runnable () { public void run () { taskThread . interrupt (); } }, timeout , unit ); r . run (); } If the task completes before the timeout, the cancellation task that interrupts the thread in which timedRun was called could go off after timedRun has returned to its caller. We don\u2019t know what code will be running when that happens, but the result won\u2019t be good. Further, if the task is not responsive to interruption, timedRun will not return until the task finishes, which may be long after the desired timeout (or even not at all). In following example, the thread created to run the task can have its own execution policy, and even if the task doesn\u2019t respond to the interrupt, the timed run method can still return to its caller. After starting the task thread, timedRun executes a timed join with the newly created thread. After join returns, it checks if an exception was thrown from the task and if so, rethrows it in the thread calling timedRun. The saved Throwable is shared between the two threads, and so is declared volatile to safely publish it from the task thread to the timedRun thread. private static final ScheduledExecutorService cancelExec = Executors . newScheduledThreadPool ( 1 ); public static void timedRun ( final Runnable r , long timeout , TimeUnit unit ) throws InterruptedException { class RethrowableTask implements Runnable { private volatile Throwable t ; public void run () { try { r . run (); } catch ( Throwable t ) { this . t = t ; } } void rethrow () { if ( t != null ) throw launderThrowable ( t ); } } RethrowableTask task = new RethrowableTask (); final Thread taskThread = new Thread ( task ); taskThread . start (); cancelExec . schedule ( new Runnable () { public void run () { taskThread . interrupt (); } }, timeout , unit ); taskThread . join ( unit . toMillis ( timeout )); task . rethrow (); } This version addresses the problems in the previous examples, but because it relies on a timed join , it shares a deficiency with join : we don\u2019t know if control was returned because the thread exited normally or because the join timed out.","title":"Example: timed run"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#cancellation-via-future","text":"We\u2019ve already used an abstraction for managing the lifecycle of a task, dealing with exceptions, and facilitating cancellation: Future . private static final ExecutorService taskExec = Executors . newCachedThreadPool (); public static void timedRun ( Runnable r , long timeout , TimeUnit unit ) throws InterruptedException { Future <?> task = taskExec . submit ( r ); try { task . get ( timeout , unit ); } catch ( TimeoutException e ) { // task will be cancelled below } catch ( ExecutionException e ) { // exception thrown in task; rethrow throw launderThrowable ( e . getCause ()); } finally { // Harmless if task already completed task . cancel ( true ); // interrupt if running } } ExecutorService.submit returns a Future describing the task. Future has a cancel method that takes a boolean argument ( mayInterruptIfRunning ), and returns a value indicating whether the cancellation attempt was successful. (This tells you only whether it was able to deliver the interruption, not whether the task detected and acted on it.) When mayInterruptIfRunning is true and the task is currently running in some thread, then that thread is interrupted. Setting this argument to false means \u201cdon\u2019t run this task if it hasn\u2019t started yet\u201d, and should be used for tasks that are not designed to handle interruption. The task execution threads created by the standard Executor implementations implement an interruption policy that lets tasks be cancelled using interruption, so it is safe to set mayInterruptIfRunning when cancelling tasks through their Future when they are running in a standard Executor. You should not interrupt a pool thread directly when attempting to cancel a task, because you won\u2019t know what task is running when the interrupt request is delivered\u2014do this only through the task\u2019s Future . Sample shows a version of timedRun that submits the task to an ExecutorService and retrieves the result with a timed Future.get . If get terminates with a TimeoutException , the task is cancelled via its Future. (To simplify coding, this version calls Future.cancel unconditionally in a finally block, taking advantage of the fact that cancelling a completed task has no effect. ) If the underlying computation throws an exception prior to cancellation, it is rethrown from timedRun , which is the most convenient way for the caller to deal with the exception. It also illustrates another good practice: cancelling tasks whose result is no longer needed. When Future.get throws InterruptedException or TimeoutException and you know that the result is no longer needed by the program, cancel the task with Future.cancel .","title":"Cancellation via Future"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#dealing-with-non-interruptible-blocking","text":"Many blocking library methods respond to interruption by returning early and throwing InterruptedException , which makes it easier to build tasks that are responsive to cancellation. However, not all blocking methods or blocking mechanisms are responsive to interruption; if a thread is blocked performing synchronous socket I/O or waiting to acquire an intrinsic lock, interruption has no effect other than setting the thread\u2019s interrupted status. We can sometimes convince threads blocked in noninterruptible activities to stop by means similar to interruption, but this requires greater awareness of why the thread is blocked. ReaderThread shows a technique for encapsulating nonstandard cancellation. ReaderThread manages a single socket connection, reading synchronously from the socket and passing any data received to processBuffer. To facilitate terminating a user connection or shutting down the server, ReaderThread overrides interrupt to both deliver a standard interrupt and close the underlying socket; thus interrupting a ReaderThread makes it stop what it is doing whether it is blocked in read or in an interruptible blocking method. public class ReaderThread extends Thread { private static final int BUFSZ = 512 ; private final Socket socket ; private final InputStream in ; public ReaderThread ( Socket socket ) throws IOException { this . socket = socket ; this . in = socket . getInputStream (); } public void interrupt () { try { socket . close (); } catch ( IOException ignored ) { } finally { super . interrupt (); } } public void run () { try { byte [] buf = new byte [ BUFSZ ]; while ( true ) { int count = in . read ( buf ); if ( count < 0 ) break ; else if ( count > 0 ) processBuffer ( buf , count ); } } catch ( IOException e ) { /* Allow thread to exit */ } } public void processBuffer ( byte [] buf , int count ) { } }","title":"Dealing with non-interruptible blocking"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#encapsulating-nonstandard-cancellation-with-newtaskfor","text":"The technique used in ReaderThread to encapsulate nonstandard cancellation can be refined using the newTaskFor hook added to ThreadPoolExecutor in Java 6. When a Callable is submitted to an ExecutorService , submit returns a Future that can be used to cancel the task. The newTaskFor hook is a factory method that creates the Future representing the task. It returns a RunnableFuture , an interface that extends both Future and Runnable (and is implemented by FutureTask ). Customizing the task Future allows you to override Future.cancel . Custom cancellation code can perform logging or gather statistics on cancellation, and can also be used to cancel activities that are not responsive to interruption. ReaderThread encapsulates cancellation of socket-using threads by overriding interrupt; the same can be done for tasks by overriding Future.cancel . public abstract class SocketUsingTask < T > implements CancellableTask < T > { @GuardedBy ( \"this\" ) private Socket socket ; protected synchronized void setSocket ( Socket s ) { socket = s ; } public synchronized void cancel () { try { if ( socket != null ) socket . close (); } catch ( IOException ignored ) { } } public RunnableFuture < T > newTask () { return new FutureTask < T >( this ) { public boolean cancel ( boolean mayInterruptIfRunning ) { try { SocketUsingTask . this . cancel (); } finally { return super . cancel ( mayInterruptIfRunning ); } } }; } } interface CancellableTask < T > extends Callable < T > { void cancel (); RunnableFuture < T > newTask (); } @ThreadSafe class CancellingExecutor extends ThreadPoolExecutor { // ... protected < T > RunnableFuture < T > newTaskFor ( Callable < T > callable ) { if ( callable instanceof CancellableTask ) return (( CancellableTask < T >) callable ). newTask (); else return super . newTaskFor ( callable ); } } CancellableTask defines a CancellableTask interface that extends Callable and adds a cancel method and a newTask factory method for constructing a RunnableFuture . CancellingExecutor extends ThreadPoolExecutor , and overrides newTaskFor to let a CancellableTask create its own Future . SocketUsingTask implements CancellableTask and defines Future.cancel to close the socket as well as call super.cancel . If a SocketUsingTask is cancelled through its Future , the socket is closed and the executing thread is interrupted. This increases the task\u2019s responsiveness to cancellation: not only can it safely call interruptible blocking methods while remaining responsive to cancellation, but it can also call blocking socket I/O methods.","title":"Encapsulating nonstandard cancellation with newTaskFor"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#stopping-a-thread-based-service","text":"Applications commonly create services that own threads, such as thread pools, and the lifetime of these services is usually longer than that of the method that creates them. If the application is to shut down gracefully, the threads owned by these services need to be terminated. Since there is no preemptive way to stop a thread, they must instead be persuaded to shut down on their own. Sensible encapsulation practices dictate that you should not manipulate a thread\u2014interrupt it, modify its priority, etc.\u2014unless you own it. The thread API has no formal concept of thread ownership: a thread is represented with a Thread object that can be freely shared like any other object. However, it makes sense to think of a thread as having an owner, and this is usually the class that created the thread. So a thread pool owns its worker threads, and if those threads need to be interrupted, the thread pool should take care of it. As with any other encapsulated object, thread ownership is not transitive: the application may own the service and the service may own the worker threads, but the application doesn\u2019t own the worker threads and therefore should not attempt to stop them directly. Instead, the service should provide lifecycle methods for shutting itself down that also shut down the owned threads; then the application can shut down the service, and the service can shut down the threads. ExecutorService provides the shutdown and shutdownNow methods; other thread-owning services should provide a similar shutdown mechanism. Provide lifecycle methods whenever a thread-owning service has a lifetime longer than that of the method that created it.","title":"Stopping a thread-based service"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#example-a-logging-service","text":"public class LogService { private final BlockingQueue < String > queue ; private final LoggerThread loggerThread ; private final PrintWriter writer ; @GuardedBy ( \"this\" ) private boolean isShutdown ; @GuardedBy ( \"this\" ) private int reservations ; public LogService ( Writer writer ) { this . queue = new LinkedBlockingQueue < String >(); this . loggerThread = new LoggerThread (); this . writer = new PrintWriter ( writer ); } public void start () { loggerThread . start (); } public void stop () { synchronized ( this ) { isShutdown = true ; } loggerThread . interrupt (); } public void log ( String msg ) throws InterruptedException { synchronized ( this ) { if ( isShutdown ) throw new IllegalStateException ( /*...*/ ); ++ reservations ; } queue . put ( msg ); } private class LoggerThread extends Thread { public void run () { try { while ( true ) { try { synchronized ( LogService . this ) { if ( isShutdown && reservations == 0 ) break ; } String msg = queue . take (); synchronized ( LogService . this ) { -- reservations ; } writer . println ( msg ); } catch ( InterruptedException e ) { /* retry */ } } } finally { writer . close (); } } } }","title":"Example: a logging service"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#executorservice-shutdown","text":"ExecutorService offers two ways to shut down: graceful shutdown with shutdown , and abrupt shutdown with shutdownNow . In an abrupt shutdown, shutdownNow returns the list of tasks that had not yet started after attempting to cancel all actively executing tasks. The two different termination options offer a tradeoff between safety and responsiveness: abrupt termination is faster but riskier because tasks may be interrupted in the middle of execution, and normal termination is slower but safer because the ExecutorService does not shut down until all queued tasks are processed. Other thread-owning services should consider providing a similar choice of shutdown modes. Simple programs can get away with starting and shutting down a global ExecutorService from main. More sophisticated programs are likely to encapsulate an ExecutorService behind a higher-level service that provides its own lifecycle methods, such as the below sample, that delegates to an ExecutorService instead of managing its own threads. Encapsulating an ExecutorService extends the ownership chain from application to service to thread by adding another link; each member of the chain manages the lifecycle of the services or threads it owns. public class LogService { private final ExecutorService exec = newSingleThreadExecutor (); ... public void start () { } public void stop () throws InterruptedException { try { exec . shutdown (); exec . awaitTermination ( TIMEOUT , UNIT ); } finally { writer . close (); } } public void log ( String msg ) { try { exec . execute ( new WriteTask ( msg )); } catch ( RejectedExecutionException ignored ) { } } }","title":"ExecutorService shutdown"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#poison-pills-tombstone","text":"Another way to convince a producer-consumer service to shut down is with a poison pill: a recognizable object placed on the queue that means \u201cwhen you get this, stop.\u201d With a FIFO queue, poison pills ensure that consumers finish the work on their queue before shutting down, since any work submitted prior to submitting the poison pill will be retrieved before the pill; producers should not submit any work after putting a poison pill on the queue. Poison pills work only when the number of producers and consumers is known. Poison pills work reliably only with unbounded queues.","title":"Poison pills (tombstone)"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#example-a-one-shot-execution-service","text":"If a method needs to process a batch of tasks and does not return until all the tasks are finished, it can simplify service lifecycle management by using a private Executor whose lifetime is bounded by that method. The checkMail method in below sample checks for new mail in parallel on a number of hosts. It creates a private Executor and submits a task for each host: it then shuts down the executor and waits for termination, which occurs when all the mail-checking tasks have completed. public boolean checkMail ( Set < String > hosts , long timeout , TimeUnit unit ) throws InterruptedException { ExecutorService exec = Executors . newCachedThreadPool (); final AtomicBoolean hasNewMail = new AtomicBoolean ( false ); try { for ( final String host : hosts ) exec . execute ( new Runnable () { public void run () { if ( checkMail ( host )) hasNewMail . set ( true ); } }); } finally { exec . shutdown (); exec . awaitTermination ( timeout , unit ); } return hasNewMail . get (); } Using a private Executor whose lifetime is bounded by a method call","title":"Example: a one-shot execution service"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#limitations-of-shutdownnow","text":"When an ExecutorService is shut down abruptly with shutdownNow , it attempts to cancel the tasks currently in progress and returns a list of tasks that were submitted but never started so that they can be logged or saved for later processing. However, there is no general way to find out which tasks started but did not complete. This means that there is no way of knowing the state of the tasks in progress at shutdown time unless the tasks themselves perform some sort of checkpointing. To know which tasks have not completed, you need to know not only which tasks didn\u2019t start, but also which tasks were in progress when the executor was shut down. public class TrackingExecutor extends AbstractExecutorService { private final ExecutorService exec ; private final Set < Runnable > tasksCancelledAtShutdown = Collections . synchronizedSet ( new HashSet < Runnable >()); // ... public List < Runnable > getCancelledTasks () { if (! exec . isTerminated ()) throw new IllegalStateException ( /*...*/ ); return new ArrayList < Runnable >( tasksCancelledAtShutdown ); } public void execute ( final Runnable runnable ) { exec . execute ( new Runnable () { public void run () { try { runnable . run (); } finally { if ( isShutdown () && Thread . currentThread (). isInterrupted ()) tasksCancelledAtShutdown . add ( runnable ); } } }); } } TrackingExecutor shows a technique for determining which tasks were in progress at shutdown time. By encapsulating an ExecutorService and instrumenting execute (and similarly submit, not shown) to remember which tasks were cancelled after shutdown TrackingExecutor can identify which tasks started but did not complete normally. After the executor terminates, getCancelledTasks returns the list of cancelled tasks. In order for this technique to work, the tasks must preserve the thread\u2019s interrupted status when they return, which well behaved tasks will do anyway. public abstract class WebCrawler { private volatile TrackingExecutor exec ; @GuardedBy ( \"this\" ) private final Set < URL > urlsToCrawl = new HashSet < URL >(); private final ConcurrentMap < URL , Boolean > seen = new ConcurrentHashMap < URL , Boolean >(); private static final long TIMEOUT = 500 ; private static final TimeUnit UNIT = MILLISECONDS ; public WebCrawler ( URL startUrl ) { urlsToCrawl . add ( startUrl ); } public synchronized void start () { exec = new TrackingExecutor ( Executors . newCachedThreadPool ()); for ( URL url : urlsToCrawl ) submitCrawlTask ( url ); urlsToCrawl . clear (); } public synchronized void stop () throws InterruptedException { try { saveUncrawled ( exec . shutdownNow ()); if ( exec . awaitTermination ( TIMEOUT , UNIT )) saveUncrawled ( exec . getCancelledTasks ()); } finally { exec = null ; } } protected abstract List < URL > processPage ( URL url ); private void saveUncrawled ( List < Runnable > uncrawled ) { for ( Runnable task : uncrawled ) urlsToCrawl . add ((( CrawlTask ) task ). getPage ()); } private void submitCrawlTask ( URL u ) { exec . execute ( new CrawlTask ( u )); } // ... } WebCrawler shows an application of TrackingExecutor . When the crawler is shut down, both the tasks that did not start and those that were cancelled are scanned and their URLs recorded, so that page-crawling tasks for those URLs can be added to the queue when the crawler restarts.","title":"Limitations of shutdownNow"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#handling-abnormal-thread-termination","text":"Failure of a thread in a concurrent application is not always so obvious. The stack trace may be printed on the console, but no one may be watching the console. Also, when a thread fails, the application may appear to continue to work, so its failure could go unnoticed. Fortunately, there are means of both detecting and preventing threads from \u201cleaking\u201d from an application. The leading cause of premature thread death is RuntimeException. Because these exceptions indicate a programming error or other unrecoverable problem, they are generally not caught. Instead they propagate all the way up the stack, at which point the default behavior is to print a stack trace on the console and let the thread terminate. The consequences of abnormal thread death range from benign to disastrous, depending on the thread\u2019s role in the application. Losing a thread from a thread pool can have performance consequences, but an application that runs well with a 50-thread pool will probably run fine with a 49-thread pool too. But losing the event dispatch thread in a GUI application would be quite noticeable\u2014the application would stop processing events and the GUI would freeze. Task-processing threads such as the worker threads in a thread pool or the Swing event dispatch thread spend their whole life calling unknown code through an abstraction barrier like Runnable , and these threads should be very skeptical that the code they call will be well behaved. It would be very bad if a service like the Swing event thread failed just because some poorly written event handler threw a NullPointerException . Accordingly, these facilities should call tasks within a try-catch block that catches unchecked exceptions, or within a try-finally block to ensure that if the thread exits abnormally the framework is informed of this and can take corrective action. This is one of the few times when you might want to consider catching RuntimeException \u2014when you are calling unknown, untrusted code through an abstraction such as Runnable 1 . Below code illustrates a way to structure a worker thread within a thread pool. If a task throws an unchecked exception, it allows the thread to die, but not before notifying the framework that the thread has died. The framework may then replace the worker thread with a new thread, or may choose not to because the thread pool is being shut down or there are already enough worker threads to meet current demand. public void run () { Throwable thrown = null ; try { while (! isInterrupted ()) runTask ( getTaskFromWorkQueue ()); } catch ( Throwable e ) { thrown = e ; } finally { threadExited ( this , thrown ); } } ThreadPoolExecutor and Swing use this technique to ensure that a poorly behaved task doesn\u2019t prevent subsequent tasks from executing. If you are writing a worker thread class that executes submitted tasks, or calling untrusted external code (such as dynamically loaded plugins), use one of these approaches to prevent a poorly written task or plugin from taking down the thread that happens to call it.","title":"Handling abnormal thread termination"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#uncaught-exception-handlers","text":"The previous section offered a proactive approach to the problem of unchecked exceptions. The Thread API also provides the UncaughtExceptionHandler facility, which lets you detect when a thread dies due to an uncaught exception. The two approaches are complementary: taken together, they provide defense-in-depth against thread leakage. When a thread exits due to an uncaught exception, the JVM reports this event to an application-provided UncaughtExceptionHandler ; if no handler exists, the default behavior is to print the stack trace to System.err . public interface UncaughtExceptionHandler { void uncaughtException ( Thread t , Throwable e ); } What the handler should do with an uncaught exception depends on your quality-of-service requirements. The most common response is to write an error message and stack trace to the application log, or take more direct action trying to restart the thread, shutting down the application, paging an operator, other corrective or diagnostic action. In long-running applications, always use uncaught exception handlers for all threads that at least log the exception. To set an UncaughtExceptionHandler for pool threads, provide a ThreadFactory to the ThreadPoolExecutor constructor. The standard thread pools allow an uncaught task exception to terminate the pool thread, but use a try-finally block to be notified when this happens so the thread can be replaced. Without an uncaught exception handler or other failure notification mechanism, tasks can appear to fail silently, which can be very confusing. If you want to be notified when a task fails due to an exception so that you can take some task-specific recovery action, either wrap the task with a Runnable or Callable that catches the exception or override the afterExecute hook in ThreadPoolExecutor .","title":"Uncaught exception handlers"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#jvm-shutdown","text":"In an orderly shutdown, the JVM first starts all registered shutdown hooks. Shutdown hooks are unstarted threads that are registered with Runtime.addShutdownHook . The JVM makes no guarantees on the order in which shutdown hooks are started. If any application threads (daemon or nondaemon) are still running at shutdown time, they continue to run concurrently with the shutdown process. When all shutdown hooks have completed, the JVM may choose to run finalizers if runFinalizersOnExit is true , and then halts. The JVM makes no attempt to stop or interrupt any application threads that are still running at shutdown time; they are abruptly terminated when the JVM eventually halts If the shutdown hooks or finalizers don\u2019t complete, then the orderly shutdown process \u201changs\u201d and the JVM must be shut down abruptly. In an abrupt shutdown, the JVM is not required to do anything other than halt the JVM; shutdown hooks will not run. Shutdown hooks should be thread-safe: they must use synchronization when accessing shared data and should be careful to avoid deadlock, just like any other concurrent code. Further, they should not make assumptions about the state of the application (such as whether other services have shut down already or all normal threads have completed) or about why the JVM is shutting down, and must therefore be coded extremely defensively. Finally, they should exit as quickly as possible, since their existence delays JVM termination at a time when the user may be expecting the JVM to terminate quickly. Shutdown hooks can be used for service or application cleanup, such as deleting temporary files or cleaning up resources that are not automatically cleaned up by the OS. Below code shows how LogService could register a shutdown hook from its start method to ensure the log file is closed on exit. public void start () { Runtime . getRuntime (). addShutdownHook ( new Thread () { public void run () { try { LogService . this . stop (); } catch ( InterruptedException ignored ) {} } }); } Because shutdown hooks all run concurrently, closing the log file could cause trouble for other shutdown hooks who want to use the logger. To avoid this problem, shutdown hooks should not rely on services that can be shut down by the application or other shutdown hooks. One way to accomplish this is to use a single shutdown hook for all services, rather than one for each service, and have it call a series of shutdown actions. This ensures that shutdown actions execute sequentially in a single thread, thus avoiding the possibility of race conditions or deadlock between shutdown actions. This technique can be used whether or not you use shutdown hooks; executing shutdown actions sequentially rather than concurrently eliminates many potential sources of failure. In applications that maintain explicit dependency information among services, this technique can also ensure that shutdown actions are performed in the right order.","title":"JVM shutdown"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#daemon-threads","text":"Sometimes you want to create a thread that performs some helper function but you don\u2019t want the existence of this thread to prevent the JVM from shutting down. This is what daemon threads are for. Threads are divided into two types: normal threads and daemon threads. When the JVM starts up, all the threads it creates (such as garbage collector and other housekeeping threads) are daemon threads, except the main thread. When a new thread is created, it inherits the daemon status of the thread that created it, so by default any threads created by the main thread are also normal threads. Normal threads and daemon threads differ only in what happens when they exit. When a thread exits, the JVM performs an inventory of running threads, and if the only threads that are left are daemon threads, it initiates an orderly shutdown. When the JVM halts, any remaining daemon threads are abandoned\u2014finally blocks are not executed, stacks are not unwound\u2014the JVM just exits. Daemon threads should be used sparingly\u2014few processing activities can be safely abandoned at any time with no cleanup. In particular, it is dangerous to use daemon threads for tasks that might perform any sort of I/O. Daemon threads are best saved for \u201chousekeeping\u201d tasks, such as a background thread that periodically removes expired entries from an in-memory cache. Daemon threads are not a good substitute for properly managing the life-cycle of services within an application.","title":"Daemon threads"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#finalizers","text":"The garbage collector does a good job of reclaiming memory resources when they are no longer needed, but some resources, such as file or socket handles, must be explicitly returned to the operating system when no longer needed. To assist in this, the garbage collector treats objects that have a nontrivial finalize method specially: after they are reclaimed by the collector, finalize is called so that persistent resources can be released. Since finalizers can run in a thread managed by the JVM, any state accessed by a finalizer will be accessed by more than one thread and therefore must be accessed with synchronization. Finalizers offer no guarantees on when or even if they run, and they impose a significant performance cost on objects with nontrivial finalizers. They are also extremely difficult to write correctly. In most cases, the combination of finally blocks and explicit close methods does a better job of resource management than finalizers; the sole exception is when you need to manage objects that hold resources acquired by native methods. For these reasons and others, work hard to avoid writing or using classes with finalizers (other than the platform library classes.) Avoid finalizers.","title":"Finalizers"},{"location":"java/concurrency/structuring-concurrent-applications/c07-cancellation-and-shutdown/#sumary","text":"End-of-lifecycle issues for tasks, threads, services, and applications can add complexity to their design and implementation. Java does not provide a preemptive mechanism for cancelling activities or terminating threads. Instead, it provides a cooperative interruption mechanism that can be used to facilitate cancellation, but it is up to you to construct protocols for cancellation and use them consistently. Using FutureTask and the Executor framework simplifies building cancellable tasks and services. There is some controversy over the safety of this technique; when a thread throws an unchecked exception, the entire application may possibly be compromised. But the alternative\u2014shutting down the entire application\u2014is usually not practical. \u21a9","title":"Sumary"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/","text":"8. Applying Thread Pools Implicit couplings between tasks and execution policies While the Executor framework offers substantial flexibility in specifying and modifying execution policies, not all tasks are compatible with all execution policies. Types of tasks that require specific execution policies include: Dependent tasks. The most well behaved tasks are independent: those that do not depend on the timing, results, or side effects of other tasks. When executing independent tasks in a thread pool, you can freely vary the pool size and configuration without affecting anything but performance. On the other hand, when you submit tasks that depend on other tasks to a thread pool, you implicitly create constraints on the execution policy that must be carefully managed to avoid liveness problems. Tasks that exploit thread confinement. Single-threaded executors make stronger promises about concurrency than do arbitrary thread pools. They guarantee that tasks are not executed concurrently, which allows you to relax the thread safety of task code. This forms an implicit coupling between the task and the execution policy\u2014the tasks require their executor to be single-threaded. In this case, if you changed the Executor from a single-threaded one to a thread pool, thread safety could be lost. Response-time-sensitive tasks. GUI applications are sensitive to response time: users are annoyed at long delays between a button click and the corresponding visual feedback. Submitting a long-running task to a single-threaded executor, or submitting several long-running tasks to a thread pool with a small number of threads, may impair the responsiveness of the service managed by that Executor. Tasks that use ThreadLocal . ThreadLocal allows each thread to have its own private \u201cversion\u201d of a variable. However, executors are free to reuse threads as they see fit. The standard Executor implementations may reap idle threads when demand is low and add new ones when demand is high, and also replace a worker thread with a fresh one if an unchecked exception is thrown from a task. ThreadLocal makes sense to use in pool threads only if the thread-local value has a lifetime that is bounded by that of a task; ThreadLocal should not be used in pool threads to communicate values between tasks. Thread pools work best when tasks are homogeneous and independent. Mixing long-running and short-running tasks risks \u201cclogging\u201d the pool unless it is very large; submitting tasks that depend on other tasks risks deadlock unless the pool is unbounded. Fortunately, requests in typical network-based server applications\u2014web servers, mail servers, file servers\u2014usually meet these guidelines. Some tasks have characteristics that require or preclude a specific execution policy. Tasks that depend on other tasks require that the thread pool be large enough that tasks are never queued or rejected; tasks that exploit thread confinement require sequential execution. Document these requirements so that future maintainers do not undermine safety or liveness by substituting an incompatible execution policy. Thread starvation deadlock If tasks that depend on other tasks execute in a thread pool, they can deadlock. In a single-threaded executor, a task that submits another task to the same executor and waits for its result will always deadlock. The second task sits on the work queue until the first task completes, but the first will not complete because it is waiting for the result of the second task. The same thing can happen in larger thread pools if all threads are executing tasks that are blocked waiting for other tasks still on the work queue. This is called thread starvation deadlock , and can occur whenever a pool task initiates an unbounded blocking wait for some resource or condition that can succeed only through the action of another pool task, such as waiting for the return value or side effect of another task, unless you can guarantee that the pool is large enough. ThreadDeadlock illustrates thread starvation deadlock. RenderPageTask submits two additional tasks to the Executor to fetch the page header and footer, renders the page body, waits for the results of the header and footer tasks, and then combines the header, body, and footer into the finished page. With a single-threaded executor, ThreadDeadlock will always deadlock. Similarly, tasks coordinating amongst themselves with a barrier could also cause thread starvation deadlock if the pool is not big enough. Whenever you submit to an Executor tasks that are not independent, be aware of the possibility of thread starvation deadlock, and document any pool sizing or configuration constraints in the code or configuration file where the Executor is configured. // Task that deadlocks in a single-threaded Executor. Don\u2019t do this. public class ThreadDeadlock { ExecutorService exec = Executors . newSingleThreadExecutor (); public class LoadFileTask implements Callable < String > { private final String fileName ; public LoadFileTask ( String fileName ) { this . fileName = fileName ; } public String call () throws Exception { // Here's where we would actually read the file return \"\" ; } } public class RenderPageTask implements Callable < String > { public String call () throws Exception { Future < String > header , footer ; header = exec . submit ( new LoadFileTask ( \"header.html\" )); footer = exec . submit ( new LoadFileTask ( \"footer.html\" )); String page = renderBody (); // Will deadlock -- task waiting for result of subtask return header . get () + page + footer . get (); } private String renderBody () { // Here's where we would actually render the page return \"\" ; } } } Long-running tasks Thread pools can have responsiveness problems if tasks can block for extended periods of time, even if deadlock is not a possibility. A thread pool can become clogged with long-running tasks, increasing the service time even for short tasks. If the pool size is too small relative to the expected steady-state number of long-running tasks, eventually all the pool threads will be running long-running tasks and responsiveness will suffer. One technique that can mitigate the ill effects of long-running tasks is for tasks to use timed resource waits instead of unbounded waits. Most blocking methods in the plaform libraries come in both untimed and timed versions, such as Thread.join , BlockingQueue.put , CountDownLatch.await , and Selector.select . If the wait times out, you can mark the task as failed and abort it or requeue it for execution later. This guarantees that each task eventually makes progress towards either successful or failed completion, freeing up threads for tasks that might complete more quickly. If a thread pool is frequently full of blocked tasks, this may also be a sign that the pool is too small. Sizing thread pools The ideal size for a thread pool depends on the types of tasks that will be submitted and the characteristics of the deployment system. Thread pool sizes should rarely be hard-coded; instead pool sizes should be provided by a configuration mechanism or computed dynamically by consulting Runtime.availableProcessors . Sizing thread pools is not an exact science, but fortunately you need only avoid the extremes of \u201ctoo big\u201d and \u201ctoo small\u201d. If a thread pool is too big, then threads compete for scarce CPU and memory resources, resulting in higher memory usage and possible resource exhaustion. If it is too small, throughput suffers as processors go unused despite available work. To size a thread pool properly, you need to understand your computing environment, your resource budget, and the nature of your tasks. How many processors does the deployment system have? How much memory? Do tasks perform mostly computation, I/O, or some combination? Do they require a scarce resource, such as a JDBC connection? If you have different categories of tasks with very different behaviors, consider using multiple thread pools so each can be tuned according to its workload. For compute-intensive tasks, an N_{cpu} N_{cpu} \u2014 processor system usually achieves optimum utilization with a thread pool of N_{cpu} + 1 N_{cpu} + 1 threads (Even compute-intensive threads occasionally take a page fault or pause for some other reason, so an \u201cextra\u201d runnable thread prevents CPU cycles from going unused when this happens.) For tasks that also include I/O or other blocking operations, you want a larger pool, since not all of the threads will be schedulable at all times. In order to size the pool properly, you must estimate the ratio of waiting time to compute time for your tasks; this estimate need not be precise and can be obtained through profiling or instrumentation. Alternatively, the size of the thread pool can be tuned by running the application using several different pool sizes under a benchmark load and observing the level of CPU utilization. Given these definitions: \\begin{align} N_{cpu} & = \\text{number of CPUs} \\\\ U_{cpu} & = \\text{target CPU utilization, }{0\\leq U_{cpu}\\leq 1} \\\\ \\frac{W}{C} & = \\text{ratio of wait time to compute time} \\\\ \\end{align} \\begin{align} N_{cpu} & = \\text{number of CPUs} \\\\ U_{cpu} & = \\text{target CPU utilization, }{0\\leq U_{cpu}\\leq 1} \\\\ \\frac{W}{C} & = \\text{ratio of wait time to compute time} \\\\ \\end{align} The optimal pool size for keeping the processors at the desired utilization is: N_{threads} = N_{cpu}*U_{cpu}*\\left(\\frac{W}{C}\\right) N_{threads} = N_{cpu}*U_{cpu}*\\left(\\frac{W}{C}\\right) Of course, CPU cycles are not the only resource you might want to manage using thread pools. Other resources that can contribute to sizing constraints are memory, file handles, socket handles, and database connections. Calculating pool size constraints for these types of resources is easier: just add up how much of that resource each task requires and divide that into the total quantity available. The result will be an upper bound on the pool size. When tasks require a pooled resource such as database connections, thread pool size and resource pool size affect each other. If each task requires a connection, the effective size of the thread pool is limited by the connection pool size. Similarly, when the only consumers of connections are pool tasks, the effective size of the connection pool is limited by the thread pool size. Configuring ThreadPoolExecutor ThreadPoolExecutor provides the base implementation for the executors returned by the newCachedThreadPool , newFixedThreadPool , and newScheduledThreadExecutor factories in Executors. ThreadPoolExecutor is a flexible, robust pool implementation that allows a variety of customizations. If the default execution policy does not meet your needs, you can instantiate a ThreadPoolExecutor through its constructor and customize it as you see fit; you can consult the source code for Executors to see the execution policies for the default configurations and use them as a starting point. ThreadPoolExecutor has several constructors. Thread creation and teardown The core pool size, maximum pool size, and keep-alive time govern thread creation and teardown. The core size is the target size; the implementation attempts to maintain the pool at this size even when there are no tasks to execute 1 , and will not create more threads than this unless the work queue is full 2 . The maximum pool size is the upper bound on how many pool threads can be active at once. A thread that has been idle for longer than the keep-alive time becomes a candidate for reaping and can be terminated if the current pool size exceeds the core size. By tuning the core pool size and keep-alive times, you can encourage the pool to reclaim resources used by otherwise idle threads, making them available for more useful work. (Like everything else, this is a tradeoff: reaping idle threads incurs additional latency due to thread creation if threads must later be created when demand increases.) The newFixedThreadPool factory sets both the core pool size and the maximum pool size to the requested pool size, creating the effect of infinite timeout; the newCachedThreadPool factory sets the maximum pool size to Integer.MAX_VALUE and the core pool size to zero with a timeout of one minute, creating the effect of an infinitely expandable thread pool that will contract again when demand decreases. Other combinations are possible using the explicit ThreadPoolExecutor constructor. Managing queued tasks Bounded thread pools limit the number of tasks that can be executed concurrently. (The single-threaded executors are a notable special case: they guarantee that no tasks will execute concurrently, offering the possibility of achieving thread safety through thread confinement.) If the arrival rate for new requests exceeds the rate at which they can be handled, requests will still queue up. With a thread pool, they wait in a queue of Runnables managed by the Executor instead of queueing up as threads contending for the CPU. Representing a waiting task with a Runnable and a list node is certainly a lot cheaper than with a thread, but the risk of resource exhaustion still remains if clients can throw requests at the server faster than it can handle them. Queues can help smooth out transient bursts of tasks, but if tasks continue to arrive too quickly you will eventually have to throttle the arrival rate to avoid running out of memory 3 . Even before you run out of memory, response time will get progressively worse as the task queue grows. ThreadPoolExecutor allows you to supply a BlockingQueue to hold tasks awaiting execution. There are three basic approaches to task queueing: Unbounded queue (The default for newFixedThreadPool and newSingleThreadExecutor is to use an unbounded LinkedBlockingQueue .) Tasks will queue up if all worker threads are busy, but the queue could grow without bound if the tasks keep arriving faster than they can be executed. Bounded queue ( ArrayBlockingQueue or a bounded LinkedBlockingQueue or PriorityBlockingQueue ) It is a more stable resource management strategy. Bounded queues help prevent resource exhaustion but introduce the question of what to do with new tasks when the queue is full. (There are a number of possible saturation policies for addressing this problem); with a bounded work queue, the queue size and pool size must be tuned together. A large queue coupled with a small pool can help reduce memory usage, CPU usage, and context switching, at the cost of potentially constraining throughput. Synchronous handoff ( SynchronousQueue used by newCachedThreadPool ) For very large or unbounded pools, you can also bypass queueing entirely and instead hand off tasks directly from producers to worker threads. A SynchronousQueue is not really a queue at all, but a mechanism for managing handoffs between threads. In order to put an element on a SynchronousQueue , another thread must already be waiting to accept the handoff. If no thread is waiting but the current pool size is less than the maximum, ThreadPoolExecutor creates a new thread; otherwise the task is rejected according to the saturation policy. Using a direct handoff is more efficient because the task can be handed right to the thread that will execute it, rather than first placing it on a queue and then having the worker thread fetch it from the queue. SynchronousQueue is a practical choice only if the pool is unbounded or if rejecting excess tasks is acceptable. The choice of queue interacts with other configuration parameters such as pool size. Using a FIFO queue like LinkedBlockingQueue or ArrayBlockingQueue causes tasks to be started in the order in which they arrived. For more control over task execution order, you can use a PriorityBlockingQueue , which orders tasks according to priority. Priority can be defined by natural order (if tasks implement Comparable ) or by a Comparator . The newCachedThreadPool factory is a good default choice for an Executor , providing better queuing performance than a fixed thread pool 4 . A fixed size thread pool is a good choice when you need to limit the number of concurrent tasks for resource-management purposes, as in a server application that accepts requests from network clients and would otherwise be vulnerable to overload. Bounding either the thread pool or the work queue is suitable only when tasks are independent. With tasks that depend on other tasks, bounded thread pools or queues can cause thread starvation deadlock; instead, use an unbounded pool configuration like newCachedThreadPool 5 . Saturation policies When a bounded work queue fills up, the saturation policy comes into play. The saturation policy for a ThreadPoolExecutor can be modified by calling setRejectedExecutionHandler . (The saturation policy is also used when a task is submitted to an Executor that has been shut down.) Several implementations of RejectedExecutionHandler are provided, each implementing a different saturation policy: AbortPolicy , CallerRunsPolicy , DiscardPolicy , and DiscardOldestPolicy . Abort . It is the default, and causes execute to throw the unchecked RejectedExecutionException ; the caller can catch this exception and implement its own overflow handling as it sees fit. Discard . Silently discards the newly submitted task if it cannot be queued for execution. Discard-Oldest . Discards the task that would otherwise be executed next and tries to resubmit the new task. (If the work queue is a priority queue, this discards the highest-priority element, so the combination of a discard-oldest saturation policy and a priority queue is not a good one.) Caller-Runs . Implements a form of throttling that neither discards tasks nor throws an exception, but instead tries to slow down the flow of new tasks by pushing some of the work back to the caller. It executes the newly submitted task not in a pool thread, but in the thread that calls execute . If we modified our WebServer example to use a bounded queue and the caller-runs policy, after all the pool threads were occupied and the work queue filled up the next task would be executed in the main thread during the call to execute. Since this would probably take some time, the main thread cannot submit any more tasks for at least a little while, giving the worker threads some time to catch up on the backlog. The main thread would also not be calling accept during this time, so incoming requests will queue up in the TCP layer instead of in the application. If the overload persisted, eventually the TCP layer would decide it has queued enough connection requests and begin discarding connection requests as well. As the server becomes overloaded, the overload is gradually pushed outward\u2014from the pool threads to the work queue to the application to the TCP layer, and eventually to the client\u2014enabling more graceful degradation under load. Choosing a saturation policy or making other changes to the execution policy can be done when the Executor is created. There is no predefined saturation policy to make execute block when the work queue is full. However, the same effect can be accomplished by using a Semaphore to bound the task injection rate as shown in BoundedExecutor . In such an approach, use an unbounded queue (there\u2019s no reason to bound both the queue size and the injection rate) and set the bound on the semaphore to be equal to the pool size plus the number of queued tasks you want to allow, since the semaphore is bounding the number of tasks both currently executing and awaiting execution. @ThreadSafe public class BoundedExecutor { private final Executor exec ; private final Semaphore semaphore ; public BoundedExecutor ( Executor exec , int bound ) { this . exec = exec ; this . semaphore = new Semaphore ( bound ); } public void submitTask ( final Runnable command ) throws InterruptedException { semaphore . acquire (); try { exec . execute ( new Runnable () { public void run () { try { command . run (); } finally { semaphore . release (); } } }); } catch ( RejectedExecutionException e ) { semaphore . release (); } } } Thread factories Whenever a thread pool needs to create a thread, it does so through a thread factory . The default thread factory creates a new, nondaemon thread with no special configuration. Specifying a thread factory allows you to customize the configuration of pool threads. ThreadFactory has a single method, newThread , that is called whenever a thread pool needs to create a new thread. There are a number of reasons to use a custom thread factory. You might want to specify an UncaughtExceptionHandler for pool threads, or instantiate an instance of a custom Thread class, such as one that performs debug logging. You might want to modify the priority (generally not a very good idea) or set the daemon status (again, not all that good an idea) of pool threads. Or maybe you just want to give pool threads more meaningful names to simplify interpreting thread dumps and error logs. MyThreadFactory illustrates a custom thread factory. It instantiates a new MyAppThread , passing a pool-specific name to the constructor so that threads from each pool can be distinguished in thread dumps and error logs. MyAppThread can also be used elsewhere in the application so that all threads can take advantage of its debugging features. public class MyThreadFactory implements ThreadFactory { private final String poolName ; public MyThreadFactory ( String poolName ) { this . poolName = poolName ; } public Thread newThread ( Runnable runnable ) { return new MyAppThread ( runnable , poolName ); } } The interesting customization takes place in MyAppThread , which lets you provide a thread name, sets a custom UncaughtExceptionHandler that writes a message to a Logger, maintains statistics on how many threads have been created and destroyed, and optionally writes a debug message to the log when a thread is created or terminates. public class MyAppThread extends Thread { public static final String DEFAULT_NAME = \"MyAppThread\" ; private static volatile boolean debugLifecycle = false ; private static final AtomicInteger created = new AtomicInteger (); private static final AtomicInteger alive = new AtomicInteger (); private static final Logger log = Logger . getAnonymousLogger (); public MyAppThread ( Runnable r ) { this ( r , DEFAULT_NAME ); } public MyAppThread ( Runnable runnable , String name ) { super ( runnable , name + \"-\" + created . incrementAndGet ()); setUncaughtExceptionHandler ( new Thread . UncaughtExceptionHandler () { public void uncaughtException ( Thread t , Throwable e ) { log . log ( Level . SEVERE , \"UNCAUGHT in thread \" + t . getName (), e ); } }); } public void run () { // Copy debug flag to ensure consistent value throughout. boolean debug = debugLifecycle ; if ( debug ) log . log ( Level . FINE , \"Created \" + getName ()); try { alive . incrementAndGet (); super . run (); } finally { alive . decrementAndGet (); if ( debug ) log . log ( Level . FINE , \"Exiting \" + getName ()); } } public static int getThreadsCreated () { return created . get (); } public static int getThreadsAlive () { return alive . get (); } public static boolean getDebug () { return debugLifecycle ; } public static void setDebug ( boolean b ) { debugLifecycle = b ; } } If your application takes advantage of security policies to grant permissions to particular codebases, you may want to use the privilegedThreadFactory factory method in Executors to construct your thread factory. It creates pool threads that have the same permissions, AccessControlContext , and contextClassLoader as the thread creating the privilegedThreadFactory . Otherwise, threads created by the thread pool inherit permissions from whatever client happens to be calling execute or submit at the time a new thread is needed, which could cause confusing security-related exceptions. Customizing ThreadPoolExecutor after construction Most of the options passed to the ThreadPoolExecutor constructors can also be modified after construction via setters (such as the core thread pool size, maximum thread pool size, keep-alive time, thread factory, and rejected execution handler). If the Executor is created through one of the factory methods in Executors (except newSingleThreadExecutor ), you can cast the result to ThreadPoolExecutor to access the setters. Executors includes a factory method, unconfigurableExecutorService , which takes an existing ExecutorService and wraps it with one exposing only the methods of ExecutorService so it cannot be further configured. Unlike the pooled implementations, newSingleThreadExecutor returns an ExecutorService wrapped in this manner, rather than a raw ThreadPoolExecutor . While a single-threaded executor is actually implemented as a thread pool with one thread, it also promises not to execute tasks concurrently. If some misguided code were to increase the pool size on a single-threaded executor, it would undermine the intended execution semantics. You can use this technique with your own executors to prevent the execution policy from being modified. If you will be exposing an ExecutorService to code you don\u2019t trust not to modify it, you can wrap it with an unconfigurableExecutorService . Extending ThreadPoolExecutor ThreadPoolExecutor was designed for extension, providing several \u201chooks\u201d for subclasses to override\u2014 beforeExecute , afterExecute , and terminated \u2014that can be used to extend the behavior of ThreadPoolExecutor . The beforeExecute and afterExecute hooks are called in the thread that executes the task, and can be used for adding logging, timing, monitoring, or statistics gathering. The afterExecute hook is called whether the task completes by returning normally from run or by throwing an Exception . (If the task completes with an Error , afterExecute is not called.) If beforeExecute throws a RuntimeException , the task is not executed and afterExecute is not called. The terminated hook is called when the thread pool completes the shutdown process, after all tasks have finished and all worker threads have shut down. It can be used to release resources allocated by the Executor during its lifecycle, perform notification or logging, or finalize statistics gathering. Example: adding statistics to a thread pool TimingThreadPool shows a custom thread pool that uses beforeExecute , afterExecute , and terminated to add logging and statistics gathering. To measure a task\u2019s runtime, beforeExecute must record the start time and store it somewhere afterExecute can find it. Because execution hooks are called in the thread that executes the task, a value placed in a ThreadLocal by beforeExecute can be retrieved by afterExecute . TimingThreadPool uses a pair of AtomicLongs to keep track of the total number of tasks processed and the total processing time, and uses the terminated hook to print a log message showing the average task time. public class TimingThreadPool extends ThreadPoolExecutor { public TimingThreadPool () { super ( 1 , 1 , 0 L , TimeUnit . SECONDS , null ); } private final ThreadLocal < Long > startTime = new ThreadLocal < Long >(); private final Logger log = Logger . getLogger ( \"TimingThreadPool\" ); private final AtomicLong numTasks = new AtomicLong (); private final AtomicLong totalTime = new AtomicLong (); protected void beforeExecute ( Thread t , Runnable r ) { super . beforeExecute ( t , r ); log . fine ( String . format ( \"Thread %s: start %s\" , t , r )); startTime . set ( System . nanoTime ()); } protected void afterExecute ( Runnable r , Throwable t ) { try { long endTime = System . nanoTime (); long taskTime = endTime - startTime . get (); numTasks . incrementAndGet (); totalTime . addAndGet ( taskTime ); log . fine ( String . format ( \"Thread %s: end %s, time=%dns\" , t , r , taskTime )); } finally { super . afterExecute ( r , t ); } } protected void terminated () { try { log . info ( String . format ( \"Terminated: avg time=%dns\" , totalTime . get () / numTasks . get ())); } finally { super . terminated (); } } } Summary The Executor framework is a powerful and flexible framework for concurrently executing tasks. It offers a number of tuning options, such as policies for creating and tearing down threads, handling queued tasks, and what to do with excess tasks, and provides several hooks for extending its behavior. As in most powerful frameworks, however, there are combinations of settings that do not work well together; some types of tasks require specific execution policies, and some combinations of tuning parameters may produce strange results. When a ThreadPoolExecutor is initially created, the core threads are not started immediately but instead as tasks are submitted, unless you call prestartAllCoreThreads . \u21a9 Developers are sometimes tempted to set the core size to zero so that the worker threads will eventually be torn down and therefore won\u2019t prevent the JVM from exiting, but this can cause some strange-seeming behavior in thread pools that don\u2019t use a SynchronousQueue for their work queue (as newCachedThreadPool does). If the pool is already at the core size, ThreadPoolExecutor creates a new thread only if the work queue is full. So tasks submitted to a thread pool with a work queue that has any capacity and a core size of zero will not execute until the queue fills up, which is usually not what is desired. In Java 6, allowCoreThreadTimeOut allows you to request that all pool threads be able to time out; enable this feature with a core size of zero if you want a bounded thread pool with a bounded work queue but still have all the threads torn down when there is no work to do. \u21a9 This is analogous to flow control in communications networks: you may be willing to buffer a certain amount of data, but eventually you need to find a way to get the other side to stop sending you data, or throw the excess data on the floor and hope the sender retransmits it when you\u2019re not so busy. \u21a9 This performance difference comes from the use of SynchronousQueue instead of LinkedBlockingQueue . SynchronousQueue was replaced in Java 6 with a new nonblocking algorithm that improved throughput in Executor benchmarks by a factor of three over the Java 5.0 SynchronousQueue implementation. \u21a9 An alternative configuration for tasks that submit other tasks and wait for their results is to use a bounded thread pool, a SynchronousQueue as the work queue, and the caller-runs saturation policy. \u21a9","title":"Applying Thread Pools"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#8-applying-thread-pools","text":"","title":"8. Applying Thread Pools"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#implicit-couplings-between-tasks-and-execution-policies","text":"While the Executor framework offers substantial flexibility in specifying and modifying execution policies, not all tasks are compatible with all execution policies. Types of tasks that require specific execution policies include: Dependent tasks. The most well behaved tasks are independent: those that do not depend on the timing, results, or side effects of other tasks. When executing independent tasks in a thread pool, you can freely vary the pool size and configuration without affecting anything but performance. On the other hand, when you submit tasks that depend on other tasks to a thread pool, you implicitly create constraints on the execution policy that must be carefully managed to avoid liveness problems. Tasks that exploit thread confinement. Single-threaded executors make stronger promises about concurrency than do arbitrary thread pools. They guarantee that tasks are not executed concurrently, which allows you to relax the thread safety of task code. This forms an implicit coupling between the task and the execution policy\u2014the tasks require their executor to be single-threaded. In this case, if you changed the Executor from a single-threaded one to a thread pool, thread safety could be lost. Response-time-sensitive tasks. GUI applications are sensitive to response time: users are annoyed at long delays between a button click and the corresponding visual feedback. Submitting a long-running task to a single-threaded executor, or submitting several long-running tasks to a thread pool with a small number of threads, may impair the responsiveness of the service managed by that Executor. Tasks that use ThreadLocal . ThreadLocal allows each thread to have its own private \u201cversion\u201d of a variable. However, executors are free to reuse threads as they see fit. The standard Executor implementations may reap idle threads when demand is low and add new ones when demand is high, and also replace a worker thread with a fresh one if an unchecked exception is thrown from a task. ThreadLocal makes sense to use in pool threads only if the thread-local value has a lifetime that is bounded by that of a task; ThreadLocal should not be used in pool threads to communicate values between tasks. Thread pools work best when tasks are homogeneous and independent. Mixing long-running and short-running tasks risks \u201cclogging\u201d the pool unless it is very large; submitting tasks that depend on other tasks risks deadlock unless the pool is unbounded. Fortunately, requests in typical network-based server applications\u2014web servers, mail servers, file servers\u2014usually meet these guidelines. Some tasks have characteristics that require or preclude a specific execution policy. Tasks that depend on other tasks require that the thread pool be large enough that tasks are never queued or rejected; tasks that exploit thread confinement require sequential execution. Document these requirements so that future maintainers do not undermine safety or liveness by substituting an incompatible execution policy.","title":"Implicit couplings between tasks and execution policies"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#thread-starvation-deadlock","text":"If tasks that depend on other tasks execute in a thread pool, they can deadlock. In a single-threaded executor, a task that submits another task to the same executor and waits for its result will always deadlock. The second task sits on the work queue until the first task completes, but the first will not complete because it is waiting for the result of the second task. The same thing can happen in larger thread pools if all threads are executing tasks that are blocked waiting for other tasks still on the work queue. This is called thread starvation deadlock , and can occur whenever a pool task initiates an unbounded blocking wait for some resource or condition that can succeed only through the action of another pool task, such as waiting for the return value or side effect of another task, unless you can guarantee that the pool is large enough. ThreadDeadlock illustrates thread starvation deadlock. RenderPageTask submits two additional tasks to the Executor to fetch the page header and footer, renders the page body, waits for the results of the header and footer tasks, and then combines the header, body, and footer into the finished page. With a single-threaded executor, ThreadDeadlock will always deadlock. Similarly, tasks coordinating amongst themselves with a barrier could also cause thread starvation deadlock if the pool is not big enough. Whenever you submit to an Executor tasks that are not independent, be aware of the possibility of thread starvation deadlock, and document any pool sizing or configuration constraints in the code or configuration file where the Executor is configured. // Task that deadlocks in a single-threaded Executor. Don\u2019t do this. public class ThreadDeadlock { ExecutorService exec = Executors . newSingleThreadExecutor (); public class LoadFileTask implements Callable < String > { private final String fileName ; public LoadFileTask ( String fileName ) { this . fileName = fileName ; } public String call () throws Exception { // Here's where we would actually read the file return \"\" ; } } public class RenderPageTask implements Callable < String > { public String call () throws Exception { Future < String > header , footer ; header = exec . submit ( new LoadFileTask ( \"header.html\" )); footer = exec . submit ( new LoadFileTask ( \"footer.html\" )); String page = renderBody (); // Will deadlock -- task waiting for result of subtask return header . get () + page + footer . get (); } private String renderBody () { // Here's where we would actually render the page return \"\" ; } } }","title":"Thread starvation deadlock"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#long-running-tasks","text":"Thread pools can have responsiveness problems if tasks can block for extended periods of time, even if deadlock is not a possibility. A thread pool can become clogged with long-running tasks, increasing the service time even for short tasks. If the pool size is too small relative to the expected steady-state number of long-running tasks, eventually all the pool threads will be running long-running tasks and responsiveness will suffer. One technique that can mitigate the ill effects of long-running tasks is for tasks to use timed resource waits instead of unbounded waits. Most blocking methods in the plaform libraries come in both untimed and timed versions, such as Thread.join , BlockingQueue.put , CountDownLatch.await , and Selector.select . If the wait times out, you can mark the task as failed and abort it or requeue it for execution later. This guarantees that each task eventually makes progress towards either successful or failed completion, freeing up threads for tasks that might complete more quickly. If a thread pool is frequently full of blocked tasks, this may also be a sign that the pool is too small.","title":"Long-running tasks"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#sizing-thread-pools","text":"The ideal size for a thread pool depends on the types of tasks that will be submitted and the characteristics of the deployment system. Thread pool sizes should rarely be hard-coded; instead pool sizes should be provided by a configuration mechanism or computed dynamically by consulting Runtime.availableProcessors . Sizing thread pools is not an exact science, but fortunately you need only avoid the extremes of \u201ctoo big\u201d and \u201ctoo small\u201d. If a thread pool is too big, then threads compete for scarce CPU and memory resources, resulting in higher memory usage and possible resource exhaustion. If it is too small, throughput suffers as processors go unused despite available work. To size a thread pool properly, you need to understand your computing environment, your resource budget, and the nature of your tasks. How many processors does the deployment system have? How much memory? Do tasks perform mostly computation, I/O, or some combination? Do they require a scarce resource, such as a JDBC connection? If you have different categories of tasks with very different behaviors, consider using multiple thread pools so each can be tuned according to its workload. For compute-intensive tasks, an N_{cpu} N_{cpu} \u2014 processor system usually achieves optimum utilization with a thread pool of N_{cpu} + 1 N_{cpu} + 1 threads (Even compute-intensive threads occasionally take a page fault or pause for some other reason, so an \u201cextra\u201d runnable thread prevents CPU cycles from going unused when this happens.) For tasks that also include I/O or other blocking operations, you want a larger pool, since not all of the threads will be schedulable at all times. In order to size the pool properly, you must estimate the ratio of waiting time to compute time for your tasks; this estimate need not be precise and can be obtained through profiling or instrumentation. Alternatively, the size of the thread pool can be tuned by running the application using several different pool sizes under a benchmark load and observing the level of CPU utilization. Given these definitions: \\begin{align} N_{cpu} & = \\text{number of CPUs} \\\\ U_{cpu} & = \\text{target CPU utilization, }{0\\leq U_{cpu}\\leq 1} \\\\ \\frac{W}{C} & = \\text{ratio of wait time to compute time} \\\\ \\end{align} \\begin{align} N_{cpu} & = \\text{number of CPUs} \\\\ U_{cpu} & = \\text{target CPU utilization, }{0\\leq U_{cpu}\\leq 1} \\\\ \\frac{W}{C} & = \\text{ratio of wait time to compute time} \\\\ \\end{align} The optimal pool size for keeping the processors at the desired utilization is: N_{threads} = N_{cpu}*U_{cpu}*\\left(\\frac{W}{C}\\right) N_{threads} = N_{cpu}*U_{cpu}*\\left(\\frac{W}{C}\\right) Of course, CPU cycles are not the only resource you might want to manage using thread pools. Other resources that can contribute to sizing constraints are memory, file handles, socket handles, and database connections. Calculating pool size constraints for these types of resources is easier: just add up how much of that resource each task requires and divide that into the total quantity available. The result will be an upper bound on the pool size. When tasks require a pooled resource such as database connections, thread pool size and resource pool size affect each other. If each task requires a connection, the effective size of the thread pool is limited by the connection pool size. Similarly, when the only consumers of connections are pool tasks, the effective size of the connection pool is limited by the thread pool size.","title":"Sizing thread pools"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#configuring-threadpoolexecutor","text":"ThreadPoolExecutor provides the base implementation for the executors returned by the newCachedThreadPool , newFixedThreadPool , and newScheduledThreadExecutor factories in Executors. ThreadPoolExecutor is a flexible, robust pool implementation that allows a variety of customizations. If the default execution policy does not meet your needs, you can instantiate a ThreadPoolExecutor through its constructor and customize it as you see fit; you can consult the source code for Executors to see the execution policies for the default configurations and use them as a starting point. ThreadPoolExecutor has several constructors.","title":"Configuring ThreadPoolExecutor"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#thread-creation-and-teardown","text":"The core pool size, maximum pool size, and keep-alive time govern thread creation and teardown. The core size is the target size; the implementation attempts to maintain the pool at this size even when there are no tasks to execute 1 , and will not create more threads than this unless the work queue is full 2 . The maximum pool size is the upper bound on how many pool threads can be active at once. A thread that has been idle for longer than the keep-alive time becomes a candidate for reaping and can be terminated if the current pool size exceeds the core size. By tuning the core pool size and keep-alive times, you can encourage the pool to reclaim resources used by otherwise idle threads, making them available for more useful work. (Like everything else, this is a tradeoff: reaping idle threads incurs additional latency due to thread creation if threads must later be created when demand increases.) The newFixedThreadPool factory sets both the core pool size and the maximum pool size to the requested pool size, creating the effect of infinite timeout; the newCachedThreadPool factory sets the maximum pool size to Integer.MAX_VALUE and the core pool size to zero with a timeout of one minute, creating the effect of an infinitely expandable thread pool that will contract again when demand decreases. Other combinations are possible using the explicit ThreadPoolExecutor constructor.","title":"Thread creation and teardown"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#managing-queued-tasks","text":"Bounded thread pools limit the number of tasks that can be executed concurrently. (The single-threaded executors are a notable special case: they guarantee that no tasks will execute concurrently, offering the possibility of achieving thread safety through thread confinement.) If the arrival rate for new requests exceeds the rate at which they can be handled, requests will still queue up. With a thread pool, they wait in a queue of Runnables managed by the Executor instead of queueing up as threads contending for the CPU. Representing a waiting task with a Runnable and a list node is certainly a lot cheaper than with a thread, but the risk of resource exhaustion still remains if clients can throw requests at the server faster than it can handle them. Queues can help smooth out transient bursts of tasks, but if tasks continue to arrive too quickly you will eventually have to throttle the arrival rate to avoid running out of memory 3 . Even before you run out of memory, response time will get progressively worse as the task queue grows. ThreadPoolExecutor allows you to supply a BlockingQueue to hold tasks awaiting execution. There are three basic approaches to task queueing: Unbounded queue (The default for newFixedThreadPool and newSingleThreadExecutor is to use an unbounded LinkedBlockingQueue .) Tasks will queue up if all worker threads are busy, but the queue could grow without bound if the tasks keep arriving faster than they can be executed. Bounded queue ( ArrayBlockingQueue or a bounded LinkedBlockingQueue or PriorityBlockingQueue ) It is a more stable resource management strategy. Bounded queues help prevent resource exhaustion but introduce the question of what to do with new tasks when the queue is full. (There are a number of possible saturation policies for addressing this problem); with a bounded work queue, the queue size and pool size must be tuned together. A large queue coupled with a small pool can help reduce memory usage, CPU usage, and context switching, at the cost of potentially constraining throughput. Synchronous handoff ( SynchronousQueue used by newCachedThreadPool ) For very large or unbounded pools, you can also bypass queueing entirely and instead hand off tasks directly from producers to worker threads. A SynchronousQueue is not really a queue at all, but a mechanism for managing handoffs between threads. In order to put an element on a SynchronousQueue , another thread must already be waiting to accept the handoff. If no thread is waiting but the current pool size is less than the maximum, ThreadPoolExecutor creates a new thread; otherwise the task is rejected according to the saturation policy. Using a direct handoff is more efficient because the task can be handed right to the thread that will execute it, rather than first placing it on a queue and then having the worker thread fetch it from the queue. SynchronousQueue is a practical choice only if the pool is unbounded or if rejecting excess tasks is acceptable. The choice of queue interacts with other configuration parameters such as pool size. Using a FIFO queue like LinkedBlockingQueue or ArrayBlockingQueue causes tasks to be started in the order in which they arrived. For more control over task execution order, you can use a PriorityBlockingQueue , which orders tasks according to priority. Priority can be defined by natural order (if tasks implement Comparable ) or by a Comparator . The newCachedThreadPool factory is a good default choice for an Executor , providing better queuing performance than a fixed thread pool 4 . A fixed size thread pool is a good choice when you need to limit the number of concurrent tasks for resource-management purposes, as in a server application that accepts requests from network clients and would otherwise be vulnerable to overload. Bounding either the thread pool or the work queue is suitable only when tasks are independent. With tasks that depend on other tasks, bounded thread pools or queues can cause thread starvation deadlock; instead, use an unbounded pool configuration like newCachedThreadPool 5 .","title":"Managing queued tasks"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#saturation-policies","text":"When a bounded work queue fills up, the saturation policy comes into play. The saturation policy for a ThreadPoolExecutor can be modified by calling setRejectedExecutionHandler . (The saturation policy is also used when a task is submitted to an Executor that has been shut down.) Several implementations of RejectedExecutionHandler are provided, each implementing a different saturation policy: AbortPolicy , CallerRunsPolicy , DiscardPolicy , and DiscardOldestPolicy . Abort . It is the default, and causes execute to throw the unchecked RejectedExecutionException ; the caller can catch this exception and implement its own overflow handling as it sees fit. Discard . Silently discards the newly submitted task if it cannot be queued for execution. Discard-Oldest . Discards the task that would otherwise be executed next and tries to resubmit the new task. (If the work queue is a priority queue, this discards the highest-priority element, so the combination of a discard-oldest saturation policy and a priority queue is not a good one.) Caller-Runs . Implements a form of throttling that neither discards tasks nor throws an exception, but instead tries to slow down the flow of new tasks by pushing some of the work back to the caller. It executes the newly submitted task not in a pool thread, but in the thread that calls execute . If we modified our WebServer example to use a bounded queue and the caller-runs policy, after all the pool threads were occupied and the work queue filled up the next task would be executed in the main thread during the call to execute. Since this would probably take some time, the main thread cannot submit any more tasks for at least a little while, giving the worker threads some time to catch up on the backlog. The main thread would also not be calling accept during this time, so incoming requests will queue up in the TCP layer instead of in the application. If the overload persisted, eventually the TCP layer would decide it has queued enough connection requests and begin discarding connection requests as well. As the server becomes overloaded, the overload is gradually pushed outward\u2014from the pool threads to the work queue to the application to the TCP layer, and eventually to the client\u2014enabling more graceful degradation under load. Choosing a saturation policy or making other changes to the execution policy can be done when the Executor is created. There is no predefined saturation policy to make execute block when the work queue is full. However, the same effect can be accomplished by using a Semaphore to bound the task injection rate as shown in BoundedExecutor . In such an approach, use an unbounded queue (there\u2019s no reason to bound both the queue size and the injection rate) and set the bound on the semaphore to be equal to the pool size plus the number of queued tasks you want to allow, since the semaphore is bounding the number of tasks both currently executing and awaiting execution. @ThreadSafe public class BoundedExecutor { private final Executor exec ; private final Semaphore semaphore ; public BoundedExecutor ( Executor exec , int bound ) { this . exec = exec ; this . semaphore = new Semaphore ( bound ); } public void submitTask ( final Runnable command ) throws InterruptedException { semaphore . acquire (); try { exec . execute ( new Runnable () { public void run () { try { command . run (); } finally { semaphore . release (); } } }); } catch ( RejectedExecutionException e ) { semaphore . release (); } } }","title":"Saturation policies"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#thread-factories","text":"Whenever a thread pool needs to create a thread, it does so through a thread factory . The default thread factory creates a new, nondaemon thread with no special configuration. Specifying a thread factory allows you to customize the configuration of pool threads. ThreadFactory has a single method, newThread , that is called whenever a thread pool needs to create a new thread. There are a number of reasons to use a custom thread factory. You might want to specify an UncaughtExceptionHandler for pool threads, or instantiate an instance of a custom Thread class, such as one that performs debug logging. You might want to modify the priority (generally not a very good idea) or set the daemon status (again, not all that good an idea) of pool threads. Or maybe you just want to give pool threads more meaningful names to simplify interpreting thread dumps and error logs. MyThreadFactory illustrates a custom thread factory. It instantiates a new MyAppThread , passing a pool-specific name to the constructor so that threads from each pool can be distinguished in thread dumps and error logs. MyAppThread can also be used elsewhere in the application so that all threads can take advantage of its debugging features. public class MyThreadFactory implements ThreadFactory { private final String poolName ; public MyThreadFactory ( String poolName ) { this . poolName = poolName ; } public Thread newThread ( Runnable runnable ) { return new MyAppThread ( runnable , poolName ); } } The interesting customization takes place in MyAppThread , which lets you provide a thread name, sets a custom UncaughtExceptionHandler that writes a message to a Logger, maintains statistics on how many threads have been created and destroyed, and optionally writes a debug message to the log when a thread is created or terminates. public class MyAppThread extends Thread { public static final String DEFAULT_NAME = \"MyAppThread\" ; private static volatile boolean debugLifecycle = false ; private static final AtomicInteger created = new AtomicInteger (); private static final AtomicInteger alive = new AtomicInteger (); private static final Logger log = Logger . getAnonymousLogger (); public MyAppThread ( Runnable r ) { this ( r , DEFAULT_NAME ); } public MyAppThread ( Runnable runnable , String name ) { super ( runnable , name + \"-\" + created . incrementAndGet ()); setUncaughtExceptionHandler ( new Thread . UncaughtExceptionHandler () { public void uncaughtException ( Thread t , Throwable e ) { log . log ( Level . SEVERE , \"UNCAUGHT in thread \" + t . getName (), e ); } }); } public void run () { // Copy debug flag to ensure consistent value throughout. boolean debug = debugLifecycle ; if ( debug ) log . log ( Level . FINE , \"Created \" + getName ()); try { alive . incrementAndGet (); super . run (); } finally { alive . decrementAndGet (); if ( debug ) log . log ( Level . FINE , \"Exiting \" + getName ()); } } public static int getThreadsCreated () { return created . get (); } public static int getThreadsAlive () { return alive . get (); } public static boolean getDebug () { return debugLifecycle ; } public static void setDebug ( boolean b ) { debugLifecycle = b ; } } If your application takes advantage of security policies to grant permissions to particular codebases, you may want to use the privilegedThreadFactory factory method in Executors to construct your thread factory. It creates pool threads that have the same permissions, AccessControlContext , and contextClassLoader as the thread creating the privilegedThreadFactory . Otherwise, threads created by the thread pool inherit permissions from whatever client happens to be calling execute or submit at the time a new thread is needed, which could cause confusing security-related exceptions.","title":"Thread factories"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#customizing-threadpoolexecutor-after-construction","text":"Most of the options passed to the ThreadPoolExecutor constructors can also be modified after construction via setters (such as the core thread pool size, maximum thread pool size, keep-alive time, thread factory, and rejected execution handler). If the Executor is created through one of the factory methods in Executors (except newSingleThreadExecutor ), you can cast the result to ThreadPoolExecutor to access the setters. Executors includes a factory method, unconfigurableExecutorService , which takes an existing ExecutorService and wraps it with one exposing only the methods of ExecutorService so it cannot be further configured. Unlike the pooled implementations, newSingleThreadExecutor returns an ExecutorService wrapped in this manner, rather than a raw ThreadPoolExecutor . While a single-threaded executor is actually implemented as a thread pool with one thread, it also promises not to execute tasks concurrently. If some misguided code were to increase the pool size on a single-threaded executor, it would undermine the intended execution semantics. You can use this technique with your own executors to prevent the execution policy from being modified. If you will be exposing an ExecutorService to code you don\u2019t trust not to modify it, you can wrap it with an unconfigurableExecutorService .","title":"Customizing ThreadPoolExecutor after construction"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#extending-threadpoolexecutor","text":"ThreadPoolExecutor was designed for extension, providing several \u201chooks\u201d for subclasses to override\u2014 beforeExecute , afterExecute , and terminated \u2014that can be used to extend the behavior of ThreadPoolExecutor . The beforeExecute and afterExecute hooks are called in the thread that executes the task, and can be used for adding logging, timing, monitoring, or statistics gathering. The afterExecute hook is called whether the task completes by returning normally from run or by throwing an Exception . (If the task completes with an Error , afterExecute is not called.) If beforeExecute throws a RuntimeException , the task is not executed and afterExecute is not called. The terminated hook is called when the thread pool completes the shutdown process, after all tasks have finished and all worker threads have shut down. It can be used to release resources allocated by the Executor during its lifecycle, perform notification or logging, or finalize statistics gathering.","title":"Extending ThreadPoolExecutor"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#example-adding-statistics-to-a-thread-pool","text":"TimingThreadPool shows a custom thread pool that uses beforeExecute , afterExecute , and terminated to add logging and statistics gathering. To measure a task\u2019s runtime, beforeExecute must record the start time and store it somewhere afterExecute can find it. Because execution hooks are called in the thread that executes the task, a value placed in a ThreadLocal by beforeExecute can be retrieved by afterExecute . TimingThreadPool uses a pair of AtomicLongs to keep track of the total number of tasks processed and the total processing time, and uses the terminated hook to print a log message showing the average task time. public class TimingThreadPool extends ThreadPoolExecutor { public TimingThreadPool () { super ( 1 , 1 , 0 L , TimeUnit . SECONDS , null ); } private final ThreadLocal < Long > startTime = new ThreadLocal < Long >(); private final Logger log = Logger . getLogger ( \"TimingThreadPool\" ); private final AtomicLong numTasks = new AtomicLong (); private final AtomicLong totalTime = new AtomicLong (); protected void beforeExecute ( Thread t , Runnable r ) { super . beforeExecute ( t , r ); log . fine ( String . format ( \"Thread %s: start %s\" , t , r )); startTime . set ( System . nanoTime ()); } protected void afterExecute ( Runnable r , Throwable t ) { try { long endTime = System . nanoTime (); long taskTime = endTime - startTime . get (); numTasks . incrementAndGet (); totalTime . addAndGet ( taskTime ); log . fine ( String . format ( \"Thread %s: end %s, time=%dns\" , t , r , taskTime )); } finally { super . afterExecute ( r , t ); } } protected void terminated () { try { log . info ( String . format ( \"Terminated: avg time=%dns\" , totalTime . get () / numTasks . get ())); } finally { super . terminated (); } } }","title":"Example: adding statistics to a thread pool"},{"location":"java/concurrency/structuring-concurrent-applications/c08-applying-thread-pools/#summary","text":"The Executor framework is a powerful and flexible framework for concurrently executing tasks. It offers a number of tuning options, such as policies for creating and tearing down threads, handling queued tasks, and what to do with excess tasks, and provides several hooks for extending its behavior. As in most powerful frameworks, however, there are combinations of settings that do not work well together; some types of tasks require specific execution policies, and some combinations of tuning parameters may produce strange results. When a ThreadPoolExecutor is initially created, the core threads are not started immediately but instead as tasks are submitted, unless you call prestartAllCoreThreads . \u21a9 Developers are sometimes tempted to set the core size to zero so that the worker threads will eventually be torn down and therefore won\u2019t prevent the JVM from exiting, but this can cause some strange-seeming behavior in thread pools that don\u2019t use a SynchronousQueue for their work queue (as newCachedThreadPool does). If the pool is already at the core size, ThreadPoolExecutor creates a new thread only if the work queue is full. So tasks submitted to a thread pool with a work queue that has any capacity and a core size of zero will not execute until the queue fills up, which is usually not what is desired. In Java 6, allowCoreThreadTimeOut allows you to request that all pool threads be able to time out; enable this feature with a core size of zero if you want a bounded thread pool with a bounded work queue but still have all the threads torn down when there is no work to do. \u21a9 This is analogous to flow control in communications networks: you may be willing to buffer a certain amount of data, but eventually you need to find a way to get the other side to stop sending you data, or throw the excess data on the floor and hope the sender retransmits it when you\u2019re not so busy. \u21a9 This performance difference comes from the use of SynchronousQueue instead of LinkedBlockingQueue . SynchronousQueue was replaced in Java 6 with a new nonblocking algorithm that improved throughput in Executor benchmarks by a factor of three over the Java 5.0 SynchronousQueue implementation. \u21a9 An alternative configuration for tasks that submit other tasks and wait for their results is to use a bounded thread pool, a SynchronousQueue as the work queue, and the caller-runs saturation policy. \u21a9","title":"Summary"},{"location":"python/introduction/","text":"Python","title":"Python"},{"location":"python/introduction/#python","text":"","title":"Python"}]}